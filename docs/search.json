[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modal Examples",
    "section": "",
    "text": "0.1 Clone repo",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "index.html#clone-repo",
    "href": "index.html#clone-repo",
    "title": "Modal Examples",
    "section": "",
    "text": "git clone https://github.com/modal-labs/modal-examples.git",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "index.html#modal-setup",
    "href": "index.html#modal-setup",
    "title": "Modal Examples",
    "section": "0.2 Modal setup",
    "text": "0.2 Modal setup\n$ modal setup\n\nThe web browser should have opened for you to authenticate and get an API token.\nIf it didn't, please copy this URL into your web browser manually:\n\nhttps://modal.com/token-flow/tf-xxxxxxxxxxx\n\nWeb authentication finished successfully!\nToken is connected to the charlotte-llm workspace.\nVerifying token against https://api.modal.com\nToken verified successfully!\nToken written to /Users/ryan/.modal.toml in profile charlotte-llm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "index.html#running-locally-remotely-and-in-parallel",
    "href": "index.html#running-locally-remotely-and-in-parallel",
    "title": "Modal Examples",
    "section": "1.1 Running locally, remotely, and in parallel",
    "text": "1.1 Running locally, remotely, and in parallel\nLetâ€™s focus in the main() part of the script.\nIt calls our function, f, in three different ways:\n\nAs a regular local call on your computer, with f.local\nAs a remote call that runs in the cloud, with f.remote\nBy map ping many copies of f in the cloud over many inputs, with f.map\n\nWe can execute this script by running modal run hello_world.py:\n$ cd 01_getting_started\n$ modal run hello_world.py\nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/01_getting_started/hello_world.py\nâ””â”€â”€ ğŸ”¨ Created function f.\nhello 1000\n1000000\n1000000\nhello 1000\nhello 0\nworld 1\nhello 2\nworld 3\nhello 4\nworld 5\nhello 6\nworld 7\nhello 8\nworld 9\nhello 10\nworld 11\nhello 12\nworld 13\nhello 14\nworld 15\nhello 16\nworld 17\nhello 18\nworld 19\n2470\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\n\n1.1.1 What just happened?\nWhen we called .remote on f, the function was executed in the cloud, on Modalâ€™s infrastructure, not locally on our computer.\nIn short, we took the function f, put it inside a container, sent it the inputs, and streamed back the logs and outputs.\n\n\n1.1.2 But why does this matter?\nTry doing one of these things next to start seeing the full power of Modal!\n\n\n1.1.3 Change the code\nIâ€™ll change the print to â€œspamâ€ and â€œeggsâ€:\n\n\nhello_world_spam.py\n\nimport sys\nimport modal\n\napp = modal.App(\"example-hello-world\")\n\n@app.function()\ndef f(i):\n    if i % 2 == 0:\n        print(\"spam\", i)\n    else:\n        print(\"eggs\", i, file=sys.stderr)\n\n    return i * i\n\n@app.local_entrypoint()\ndef main():\n    # run the function locally\n    print(f.local(1000))\n\n    # run the function remotely on Modal\n    print(f.remote(1000))\n\n    # run the function in parallel and remotely on Modal\n    total = 0\n    for ret in f.map(range(20)):\n        total += ret\n\n    print(total)\n\nThen run:\n$ modal run hello_world_spam.py\nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/01_getting_started/hello_world_spam.py\nâ””â”€â”€ ğŸ”¨ Created function f.\nspam 1000\n1000000\nspam 1000\n1000000\nspam 0\neggs 1\nspam 2\neggs 3\nspam 4\neggs 5\nspam 6\neggs 7\nspam 8\neggs 9\nspam 10\neggs 11\nspam 12\neggs 13\nspam 14\neggs 15\nspam 16\neggs 17\nspam 18\neggs 19\n2470\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\nI can view the output via browser:\n\nThis example is obviously very simple, but there are many other things you can do with modal like:\n\nRunning language model inference or fine-tuning\nManipulating audio or images\nCollecting financial data to backtest a trading algorithm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "index.html#decorators",
    "href": "index.html#decorators",
    "title": "Modal Examples",
    "section": "2.1 Decorators",
    "text": "2.1 Decorators\nNotice the two different app decorators: @app.function() and @app.local_entrypoint().\n\n\n\n\n\n\nFrom the docs, local_entrypoint:\n\n\n\n\n\n&gt; def local_entrypoint(\n    self, _warn_parentheses_missing=None, *, name: Optional[str] = None\n) -&gt; Callable[[Callable[..., Any]], None]:\nDecorate a function to be used as a CLI entrypoint for a Modal App.\nThese functions can be used to define code that runs locally to set up the app, and act as an entrypoint to start Modal functions from. Note that regular Modal functions can also be used as CLI entrypoints, but unlike local_entrypoint, those functions are executed remotely directly.\n@app.local_entrypoint()\ndef main():\n    some_modal_function.remote()\nYou can call the function using modal run directly from the CLI:\nmodal run app_module.py\nNote that an explicit app.run() is not needed, as an app is automatically created for you.\n\n\n\nWe can run:\n$ modal run get_started.py     \nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/01_getting_started/get_started.py\nâ””â”€â”€ ğŸ”¨ Created function square.\nthe square is 1764\nThis code is running on a remote worker!\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\nNow I wonder what happens if I create a similar new file:\n\n\nget_started_local.py\n\nimport modal\n\napp = modal.App(\"example-get-started-local\")\n\n\n@app.function()\ndef square(x):\n    print(\"This code is running on a local worker!\")\n    return x**2\n\n\n@app.local_entrypoint()\ndef main():\n    print(\"the square is\", square.local(42))\n\nAnd then run:\n$ modal run get_started_local.py\nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/01_getting_started/get_started_local.py\nâ””â”€â”€ ğŸ”¨ Created function square.\nThis code is running on a local worker!\nthe square is 1764\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nVery similar. What happens when we look at the logs:",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02_building_containers.html",
    "href": "02_building_containers.html",
    "title": "2Â  Building Containers",
    "section": "",
    "text": "2.1 import_sklearn.py",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Building Containers</span>"
    ]
  },
  {
    "objectID": "02_building_containers.html#import_sklearn.py",
    "href": "02_building_containers.html#import_sklearn.py",
    "title": "2Â  Building Containers",
    "section": "",
    "text": "2.1.1 Install scikit-learn in a custom image\nThis builds a custom image which installs the sklearn (scikit-learn) Python package in it. Itâ€™s an example of how you can use packages, even if you donâ€™t have them installed locally.\nFirst, the imports:\nimport time\nimport modal\nNext, weâ€™ll define an app, with a custom image that installs sklearn.\napp = modal.App(\n    \"import-sklearn\",\n    image=modal.Image.debian_slim()\n    .apt_install(\"libgomp1\")\n    .pip_install(\"scikit-learn\"),\n)\n\n\n\n\n\n\nChaining\n\n\n\nA nice design in modal is the idea of method chaining, where the image is built by layers.\n\n\nThe app.image.imports() lets us conditionally import in the global scope. This is needed because we might not have sklearn and numpy installed locally, but we know they are installed inside the custom image.\nwith app.image.imports():\n    import numpy as np\n    from sklearn import datasets, linear_model\nNow, letâ€™s define a function that uses one of scikit-learnâ€™s built-in datasets and fits a very simple model (linear regression) to it.\n@app.function()\ndef fit():\n    print(\"Inside run!\")\n    t0 = time.time()\n    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n    diabetes_X = diabetes_X[:, np.newaxis, 2]\n    regr = linear_model.LinearRegression()\n    regr.fit(diabetes_X, diabetes_y)\n    return time.time() - t0\nFinally, weâ€™d trigger the run locally, which we time with time.time() - t0.\nObserve that the first time we run this snippet, it will build the image. This might take 1-2 minutes.\nBut when we run this subsequent times, the image is already build, and it will run much faster.\nif __name__ == \"__main__\":\n    t0 = time.time()\n    with app.run():\n        t = fit.remote()\n        print(\"Function time spent:\", t)\n    print(\"Full time spent:\", time.time() - t0)\nSo letâ€™s run it:\n$ modal run import_sklearn.py \nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nBuilding image im-m9EoOtS0dmWsGUat8WCWFc\n\n=&gt; Step 0: FROM base\n\n=&gt; Step 1: RUN apt-get update\nGet:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\nGet:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]\nGet:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\nGet:4 http://deb.debian.org/debian bullseye/main amd64 Packages [8068 kB]\nGet:5 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [275 kB]\nGet:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages.diff/Index [26.3 kB]\nGet:7 http://deb.debian.org/debian bullseye-updates/main amd64 Packages T-2023-12-29-1403.39-F-2023-07-31-2005.11.pdiff [6053 B]\nGet:7 http://deb.debian.org/debian bullseye-updates/main amd64 Packages T-2023-12-29-1403.39-F-2023-07-31-2005.11.pdiff [6053 B]\nGet:8 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [18.8 kB]\nFetched 8602 kB in 4s (2239 kB/s)\nReading package lists...\n\n=&gt; Step 2: RUN apt-get install -y libgomp1\nReading package lists...\nBuilding dependency tree...\nReading state information...\nlibgomp1 is already the newest version (10.2.1-6).\nlibgomp1 set to manually installed.\n0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\nCreating image snapshot...\nFinished snapshot; took 1.14s\n\nBuilt image im-m9EoOtS0dmWsGUat8WCWFc in 8.53s\nBuilding image im-Kndkz3TpRhPEMy6UcNR7YR\n\n=&gt; Step 0: FROM base\n\n=&gt; Step 1: RUN python -m pip install scikit-learn \nLooking in indexes: http://pypi-mirror.modal.local:5555/simple\nCollecting scikit-learn\n  Downloading http://pypi-mirror.modal.local:5555/simple/scikit-learn/scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.3/13.3 MB 169.9 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.19.5 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.25.0)\nCollecting scipy&gt;=1.6.0 (from scikit-learn)\n  Downloading http://pypi-mirror.modal.local:5555/simple/scipy/scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 38.6/38.6 MB 233.1 MB/s eta 0:00:00\nCollecting joblib&gt;=1.2.0 (from scikit-learn)\n  Downloading http://pypi-mirror.modal.local:5555/simple/joblib/joblib-1.4.2-py3-none-any.whl (301 kB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 301.8/301.8 kB 252.9 MB/s eta 0:00:00\nCollecting threadpoolctl&gt;=3.1.0 (from scikit-learn)\n  Downloading http://pypi-mirror.modal.local:5555/simple/threadpoolctl/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\nSuccessfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n\n[notice] A new release of pip is available: 23.1.2 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\nCreating image snapshot...\nFinished snapshot; took 2.27s\n\nBuilt image im-Kndkz3TpRhPEMy6UcNR7YR in 13.14s\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/02_building_containers/import_sklearn.py\nâ””â”€â”€ ğŸ”¨ Created function fit.\nInside run!\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nThat took 8.53s to build the first image, 2.27s to create the snapshot and 13.14s to build the second image.\nBut if we run this again, itâ€™ll be much faster than before as weâ€™ve already.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Building Containers</span>"
    ]
  },
  {
    "objectID": "02_building_containers.html#import_sklearn_r2.py",
    "href": "02_building_containers.html#import_sklearn_r2.py",
    "title": "2Â  Building Containers",
    "section": "2.2 import_sklearn_r2.py",
    "text": "2.2 import_sklearn_r2.py\nJust for fun, letâ€™s modify this script to now output the R^2 value on the test data.\n\n\nimport_sklearn_r2.py\n\nimport time\nimport modal\n\napp = modal.App(\n    \"import-sklearn\",\n    image=modal.Image.debian_slim()\n    .apt_install(\"libgomp1\")\n    .pip_install(\"scikit-learn\"),\n)\n\nwith app.image.imports():\n    import numpy as np\n    from sklearn import datasets, linear_model\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import r2_score\n\n@app.function()\ndef fit():\n    print(\"Inside run!\")\n    X, y = datasets.load_diabetes(return_X_y=True)\n    X = X[:, np.newaxis, 2]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    regr = linear_model.LinearRegression()\n    regr.fit(X_train, y_train)\n    predict = regr.predict(X_test)\n    \n    return r2_score(predict, y_test)\n\n\nif __name__ == \"__main__\":\n    t0 = time.time()\n    with app.run():\n        t = fit.remote()\n        print(\"R Squared is:\", t)\n    print(\"Full time spent:\", time.time() - t0)\n\nRunning this, we get:\n$ modal run import_sklearn_r2.py\nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/02_building_containers/import_sklearn_r2.py\nâ””â”€â”€ ğŸ”¨ Created function fit.\nInside run!\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nThis result somewhat surprised me.\nFirst, I didnâ€™t see the output R^2. I was expecting this perhaps the first time running, but didnâ€™t see it.\nSecond, after running, unlike the previous example that shut down immediately, this container was running ephemerally:\n\nSo letâ€™s rerun, but this time renaming our function from fit to fit_r2:\n$ modal run import_sklearn_r2.py\nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/02_building_containers/import_sklearn_r2.py\nâ””â”€â”€ ğŸ”¨ Created function fit_r2.\nInside run!\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nThis avoided the issue of perpetually running but didnâ€™t print the R^2 to console:\n\nInstead, I modified to put the print within the function such that:\n@app.function()\ndef fit_r2():\n    print(\"Inside run!\")\n    X, y = datasets.load_diabetes(return_X_y=True)\n    X = X[:, np.newaxis, 2]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    regr = linear_model.LinearRegression()\n    regr.fit(X_train, y_train)\n    predict = regr.predict(X_test)\n    r2 = r2_score(predict, y_test) \n    print(\"R squared is:\", r2) # added this\n    return r2\nWhen doing this, I now get the result I want:\n$ modal run 02_building_containers/import_sklearn_r2.py \nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /Users/ryan/modal-examples/02_building_containers/import_sklearn_r2.py\nâ””â”€â”€ ğŸ”¨ Created function fit_r2.\nInside run!\nR squared is: -0.8503156043967386\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nItâ€™s important to mindful of scope of local versus remote when using Modal. This will be an extended discussion weâ€™ll see in later examples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Building Containers</span>"
    ]
  },
  {
    "objectID": "02_building_containers.html#install_cuda.py",
    "href": "02_building_containers.html#install_cuda.py",
    "title": "2Â  Building Containers",
    "section": "2.3 install_cuda.py",
    "text": "2.3 install_cuda.py\nThe next examples shows how to use the Nvidia CUDA base image from DockerHub.\nHereâ€™s a list of the different CUDA images available.\nWe need to add Python 3 and pip with the add_python option because the image doesnâ€™t have these by default.\n\n\ninstall_cuda.py\n\nfrom modal import App, Image\n\nimage = Image.from_registry(\n    \"nvidia/cuda:12.2.0-devel-ubuntu22.04\", add_python=\"3.11\"\n)\napp = App(image=image)\n\n@app.function(gpu=\"T4\")\ndef f():\n    import subprocess\n\n    subprocess.run([\"nvidia-smi\"])\n\n\n\n\n\n\n\nUse an existing container image with .from_registry\n\n\n\n\n\nYou donâ€™t always need to start from scratch! Public registries like Docker Hub have many pre-built container images for common software packages.\nYou can use any public image in your function using Image.from_registry, so long as:\n\nPython 3.8 or above is present, and is available as python\npip is installed correctly\nThe image is built for the linux/amd64 platform\nThe image has a valid ENTRYPOINT\n\nfrom modal import Image\n\nsklearn_image = Image.from_registry(\"huanjason/scikit-learn\")\n\n@app.function(image=sklearn_image)\ndef fit_knn():\n    from sklearn.neighbors import KNeighborsClassifier\n    ...\nIf an existing image does not have either python or pip set up properly, you can still use it. Just provide a version number as the add_python argument to install a reproducible, standalone build of Python:\nfrom modal import Image\n\nimage1 = Image.from_registry(\"ubuntu:22.04\", add_python=\"3.11\")\nimage2 = Image.from_registry(\"gisops/valhalla:latest\", add_python=\"3.11\")\nThe from_registry method can load images from all public registries, such as Nvidiaâ€™s nvcr.io, AWS ECR, and GitHubâ€™s ghcr.io.\nModal also supports access to private AWS ECR and GCP Artifact Registry images.\n\n\n\nRunning it provides:\n$ modal run install_cuda.py     \nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nBuilding image im-NAV0762Ag7PgTCJY8XyAqb\n\n=&gt; Step 0: FROM nvidia/cuda:12.2.0-devel-ubuntu22.04\nGetting image source signatures\nCopying blob sha256:9a9dd462fc4c5ca1dd29994385be60a5bb359843fc93447331b8c97dfec99bf9\nCopying blob sha256:9fe5ccccae45d6811769206667e494085cb511666be47b8e659087c249083c3f\nCopying blob sha256:aece8493d3972efa43bfd4ee3cdba659c0f787f8f59c82fb3e48c87cbb22a12e\nCopying blob sha256:bdddd5cb92f6b4613055bcbcd3226df9821c7facd5af9a998ba12dae080ef134\nCopying blob sha256:8054e9d6e8d6718cc3461aa4172ad048564cdf9f552c8f9820bd127859aa007c\nCopying blob sha256:5324914b447286e0e6512290373af079a25f94499a379e642774245376e60885\nCopying blob sha256:95eef45e00fabd2bce97586bfe26be456b0e4b3ef3d88d07a8b334ee05cc603c\nCopying blob sha256:e2554c2d377e1176c0b8687b17aa7cbe2c48746857acc11686281a4adee35a0a\nCopying blob sha256:4640d022dbb8eb47da53ccc2de59f8f5e780ea046289ba3cffdf0a5bd8d19810\nCopying blob sha256:aa750c79a4cc745750c40a37cad738f9bcea14abb96b0c5a811a9b53f185b9c9\nCopying blob sha256:9e2de25be969afa4e73937f8283a1100f4d964fc0876c2f2184fda25ad23eeda\nCopying config sha256:fead46ae620f9febc59f92a8f1f277f502ef6dca8111ce459c154d236ee84eee\nWriting manifest to image destination\nUnpacking OCI image\n   â€¢ unpacking rootfs ...\n   â€¢ ... done\n   â€¢ unpacked image rootfs: /tmp/.tmpDUhHRA\n\n=&gt; Step 1: COPY /python/. /usr/local\n\n=&gt; Step 2: RUN ln -s /usr/local/bin/python3 /usr/local/bin/python\n\n=&gt; Step 3: ENV TERMINFO_DIRS=/etc/terminfo:/lib/terminfo:/usr/share/terminfo:/usr/lib/terminfo\n\n=&gt; Step 4: COPY /modal_requirements.txt /modal_requirements.txt\n\n=&gt; Step 5: RUN python -m pip install --upgrade pip\nLooking in indexes: http://pypi-mirror.modal.local:5555/simple\nRequirement already satisfied: pip in /usr/local/lib/python3.11/site-packages (23.2.1)\nCollecting pip\n  Obtaining dependency information for pip from http://pypi-mirror.modal.local:5555/simple/pip/pip-24.0-py3-none-any.whl.metadata\n  Downloading http://pypi-mirror.modal.local:5555/simple/pip/pip-24.0-py3-none-any.whl.metadata (3.6 kB)\nDownloading http://pypi-mirror.modal.local:5555/simple/pip/pip-24.0-py3-none-any.whl (2.1 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.1/2.1 MB 216.4 MB/s eta 0:00:00\nInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.2.1\n    Uninstalling pip-23.2.1:\n      Successfully uninstalled pip-23.2.1\nSuccessfully installed pip-24.0\n\n=&gt; Step 6: RUN python -m pip install -r /modal_requirements.txt\nLooking in indexes: http://pypi-mirror.modal.local:5555/simple\nIgnoring cloudpickle: markers 'python_version &lt; \"3.11\"' don't match your environment\nIgnoring ddtrace: markers 'python_version &lt; \"3.11\"' don't match your environment\nCollecting aiohttp==3.8.3 (from -r /modal_requirements.txt (line 2))\n\n...\n\nCreating image snapshot...\nFinished snapshot; took 6.10s\n\nBuilt image im-NAV0762Ag7PgTCJY8XyAqb in 136.43s\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/02_building_containers/install_cuda.py\nâ””â”€â”€ ğŸ”¨ Created function f.\n\n==========\n== CUDA ==\n==========\n\nCUDA Version 12.2.0\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nThu Jun 13 23:08:03 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       On  |   00000000:36:00.0 Off |                 ERR! |\n| N/A   32C ERR!               9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nWhile this process did take a few minutes, itâ€™s very easy and should be very quick if rerunning.\nItâ€™s also helpful to note how to run a subprocess as we would in Python anyways:\nimport subprocess\n\nsubprocess.run([\"nvidia-smi\"])",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Building Containers</span>"
    ]
  },
  {
    "objectID": "02_building_containers.html#screenshot.py",
    "href": "02_building_containers.html#screenshot.py",
    "title": "2Â  Building Containers",
    "section": "2.4 screenshot.py",
    "text": "2.4 screenshot.py\nIn this example, we use Modal functions and the playwright package to take screenshots of websites from a list of URLs in parallel.\nYou can run this example on the command line with:\nmodal run 02_building_containers/screenshot.py --url 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'\n\n\n\n\n\n\nBe sure to install playwright locally\n\n\n\nWhen I first ran screenshot.py, I received an error like:\nStopping app - uncaught exception raised locally: ExecutionError('Could not deserialize remote exception due to local error:\\nDeserialization failed because the \\'playwright\\' module is not available in the local environment.\\nThis can happen if your local environment does not have the remote exception definitions.\nIt was fixed after I installed playwright into my active local Python environment.\n\n\nThis should take a few seconds then create a /tmp/screenshots/screenshot.png file, shown below.\n\n\n\nscreenshot\n\n\n\n2.4.1 Setup\nFirst we import the Modal client library.\nimport pathlib\n\nimport modal\n\napp = modal.App(\"example-screenshot\")\n\n\n2.4.2 Define a custom image\nWe need an image with the playwright Python package as well as its chromium plugin pre-installed.\nThis requires intalling a few Debian packages, as well as setting up a new Debian repository. Modal lets you run arbitrary commands, just like in Docker:\nimage = modal.Image.debian_slim().run_commands(\n    \"apt-get update\",\n    \"apt-get install -y software-properties-common\",\n    \"apt-add-repository non-free\",\n    \"apt-add-repository contrib\",\n    \"pip install playwright==1.42.0\",\n    \"playwright install-deps chromium\",\n    \"playwright install chromium\",\n)\n\n\n2.4.3 The screenshot function\nNext, the scraping function which runs headless Chromium, goes to a website, and takes a screenshot.\nThis is a Modal function which runs inside the remote container.\n@app.function(image=image)\nasync def screenshot(url):\n    from playwright.async_api import async_playwright\n\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        await page.goto(url, wait_until=\"networkidle\")\n        await page.screenshot(path=\"screenshot.png\")\n        await browser.close()\n        data = open(\"screenshot.png\", \"rb\").read()\n        print(\"Screenshot of size %d bytes\" % len(data))\n        return data\n\n\n2.4.4 Entrypoint code\nLetâ€™s kick it off by reading a bunch of URLs from a txt file and scrape some of those.\n@app.local_entrypoint()\ndef main(url: str = \"https://modal.com\"):\n    filename = pathlib.Path(\"/tmp/screenshots/screenshot.png\")\n    data = screenshot.remote(url)\n    filename.parent.mkdir(exist_ok=True)\n    with open(filename, \"wb\") as f:\n        f.write(data)\n    print(f\"wrote {len(data)} bytes to {filename}\")\nAnd weâ€™re done! Modalâ€™s introductory guide also has another example of a web scraper, with more in-depth logic.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Building Containers</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html",
    "href": "03_scaling_out.html",
    "title": "3Â  Scaling out",
    "section": "",
    "text": "4 basic_grid_search.py\nThis example showcases a simple grid search in one dimension, where we try different parameters for a model and pick the one with the best results on a holdout set.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#defining-the-image",
    "href": "03_scaling_out.html#defining-the-image",
    "title": "3Â  Scaling out",
    "section": "4.1 Defining the image",
    "text": "4.1 Defining the image\nFirst, letâ€™s build a custom image and install scikit-learn in it.\nimport modal\n\napp = modal.App(\n    \"example-basic-grid-search\",\n    image=modal.Image.debian_slim().pip_install(\"scikit-learn~=1.2.2\"),\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#the-modal-function",
    "href": "03_scaling_out.html#the-modal-function",
    "title": "3Â  Scaling out",
    "section": "4.2 The Modal function",
    "text": "4.2 The Modal function\nNext, define the function. Note that we use the custom image with scikit-learn in it. We also take the hyperparameter k, which is how many nearest neighbors we use.\n@app.function()\ndef fit_knn(k):\n    from sklearn.datasets import load_digits\n    from sklearn.model_selection import train_test_split\n    from sklearn.neighbors import KNeighborsClassifier\n\n    X, y = load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    clf = KNeighborsClassifier(k)\n    clf.fit(X_train, y_train)\n    score = float(clf.score(X_test, y_test))\n    print(\"k = %3d, score = %.4f\" % (k, score))\n    return score, k",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#parallel-search",
    "href": "03_scaling_out.html#parallel-search",
    "title": "3Â  Scaling out",
    "section": "4.3 Parallel search",
    "text": "4.3 Parallel search\nTo do a hyperparameter search, letâ€™s map over this function with different values for k, and then select for the best score on the holdout set:\n@app.local_entrypoint()\ndef main():\n    # Do a basic hyperparameter search\n    best_score, best_k = max(fit_knn.map(range(1, 100)))\n    print(\"Best k = %3d, score = %.4f\" % (best_k, best_score))\nNotice the map() function, which is a parallel map over a set of inputs.\nIt takes one iterator argument per argument in the function being mapped over.\nPutting all of this together, we can run it:\n$ modal run basic_grid_search.py \nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nBuilding image im-F71KgZSeUtGyLnboXtpDAA\n\n=&gt; Step 0: FROM base\n\n=&gt; Step 1: RUN python -m pip install 'scikit-learn~=1.2.2' \nLooking in indexes: http://pypi-mirror.modal.local:5555/simple\nCollecting scikit-learn~=1.2.2\n  Downloading http://pypi-mirror.modal.local:5555/simple/scikit-learn/scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9.6/9.6 MB 204.2 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17.3 in /usr/local/lib/python3.10/site-packages (from scikit-learn~=1.2.2) (1.25.0)\nCollecting scipy&gt;=1.3.2 (from scikit-learn~=1.2.2)\n  Downloading http://pypi-mirror.modal.local:5555/simple/scipy/scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 38.6/38.6 MB 230.3 MB/s eta 0:00:00\nCollecting joblib&gt;=1.1.1 (from scikit-learn~=1.2.2)\n  Downloading http://pypi-mirror.modal.local:5555/simple/joblib/joblib-1.4.2-py3-none-any.whl (301 kB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 301.8/301.8 kB 237.9 MB/s eta 0:00:00\nCollecting threadpoolctl&gt;=2.0.0 (from scikit-learn~=1.2.2)\n  Downloading http://pypi-mirror.modal.local:5555/simple/threadpoolctl/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\nSuccessfully installed joblib-1.4.2 scikit-learn-1.2.2 scipy-1.13.1 threadpoolctl-3.5.0\n\n[notice] A new release of pip is available: 23.1.2 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\nCreating image snapshot...\nFinished snapshot; took 2.85s\n\nBuilt image im-F71KgZSeUtGyLnboXtpDAA in 15.78s\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/03_scaling_out/basic_grid_search.py\nâ””â”€â”€ ğŸ”¨ Created function fit_knn.\nk =   1, score = 0.9822\nk =  16, score = 0.9778\nk =  17, score = 0.9800\n\n...\n\nk =  93, score = 0.9156\nk =  28, score = 0.9711\nk =  22, score = 0.9800\nBest k =   6, score = 0.9956\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#setup",
    "href": "03_scaling_out.html#setup",
    "title": "3Â  Scaling out",
    "section": "5.1 Setup",
    "text": "5.1 Setup\nFor this image, we need:\n\nhttpx and beautifulsoup4 to fetch a list of ETFs from a HTML page\nyfinance to fetch stock prices from the Yahoo Finance API\nmatplotlib to plot the result\n\nimport io\nimport os\n\nimport modal\n\napp = modal.App(\n    \"example-fetch-stock-prices\",\n    image=modal.Image.debian_slim().pip_install(\n        \"httpx~=0.24.0\",\n        \"yfinance~=0.2.31\",\n        \"beautifulsoup4~=4.12.2\",\n        \"matplotlib~=3.7.1\",\n    ),\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#fetch-a-list-of-tickers",
    "href": "03_scaling_out.html#fetch-a-list-of-tickers",
    "title": "3Â  Scaling out",
    "section": "5.2 Fetch a list of tickers",
    "text": "5.2 Fetch a list of tickers\nThe yfinance package does not have a way to download a list of stocks. To get a list of stocks, we parse the HTML from Yahoo Finance using Beautiful Soup and ask for the top 100 ETFs.\n@app.function()\ndef get_stocks():\n    import bs4\n    import httpx\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n        \"referer\": \"https://finance.yahoo.com/\",\n    }\n    url = \"https://finance.yahoo.com/etfs?count=100&offset=0\"\n    res = httpx.get(url, headers=headers)\n    res.raise_for_status()\n    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n    for td in soup.find_all(\"td\", {\"aria-label\": \"Symbol\"}):\n        for link in td.find_all(\"a\", {\"data-test\": \"quoteLink\"}):\n            symbol = str(link.next)\n            print(f\"Found symbol {symbol}\")\n            yield symbol",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#fetch-stock-prices",
    "href": "03_scaling_out.html#fetch-stock-prices",
    "title": "3Â  Scaling out",
    "section": "5.3 Fetch stock prices",
    "text": "5.3 Fetch stock prices\nNow, letâ€™s fetch the stock data. This is the function that we will parallelize.\nItâ€™s fairly simple and just uses the yfinance package.\n@app.function()\ndef get_prices(symbol):\n    import yfinance\n\n    print(f\"Fetching symbol {symbol}...\")\n    ticker = yfinance.Ticker(symbol)\n    data = ticker.history(period=\"1Y\")[\"Close\"]\n    print(f\"Done fetching symbol {symbol}!\")\n    return symbol, data.to_dict()",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#plot-the-result",
    "href": "03_scaling_out.html#plot-the-result",
    "title": "3Â  Scaling out",
    "section": "5.4 Plot the result",
    "text": "5.4 Plot the result\nHere is our code for our plot using matplotlib.\nWe run this in Modal, although you could also run it locally.\nNote that the code calls the other two functions. Since we plot the data in the cloud, we canâ€™t display it, so we generate a PNG and return the binary content from the function.\n@app.function()\ndef plot_stocks():\n    from matplotlib import pyplot, ticker\n\n    # Setup\n    pyplot.style.use(\"ggplot\")\n    fig, ax = pyplot.subplots(figsize=(8, 5))\n\n    # Get data\n    tickers = list(get_stocks.remote_gen())\n    if not tickers:\n        raise RuntimeError(\"Retrieved zero stock tickers!\")\n    data = list(get_prices.map(tickers))\n    first_date = min((min(prices.keys()) for symbol, prices in data if prices))\n    last_date = max((max(prices.keys()) for symbol, prices in data if prices))\n\n    # Plot every symbol\n    for symbol, prices in data:\n        if len(prices) == 0:\n            continue\n        dates = list(sorted(prices.keys()))\n        prices = list(prices[date] for date in dates)\n        changes = [\n            100.0 * (price / prices[0] - 1) for price in prices\n        ]  # Normalize to initial price\n        if changes[-1] &gt; 20:\n            # Highlight this line\n            p = ax.plot(dates, changes, alpha=0.7)\n            ax.annotate(\n                symbol,\n                (last_date, changes[-1]),\n                ha=\"left\",\n                va=\"center\",\n                color=p[0].get_color(),\n                alpha=0.7,\n            )\n        else:\n            ax.plot(dates, changes, color=\"gray\", alpha=0.2)\n\n    # Configure axes and title\n    ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n    ax.set_title(f\"Best ETFs {first_date.date()} - {last_date.date()}\")\n    ax.set_ylabel(f\"% change, {first_date.date()} = 0%\")\n\n    # Dump the chart to .png and return the bytes\n    with io.BytesIO() as buf:\n        pyplot.savefig(buf, format=\"png\", dpi=300)\n        return buf.getvalue()",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#entrypoint",
    "href": "03_scaling_out.html#entrypoint",
    "title": "3Â  Scaling out",
    "section": "5.5 Entrypoint",
    "text": "5.5 Entrypoint\nThe entrypoint locally runs the app, gets the chart back as a PNG file, and saves it to disk.\nOUTPUT_DIR = \"/tmp/\"\n\n\n@app.local_entrypoint()\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    data = plot_stocks.remote()\n    filename = os.path.join(OUTPUT_DIR, \"stock_prices.png\")\n    print(f\"saving data to {filename}\")\n    with open(filename, \"wb\") as f:\n        f.write(data)\nLetâ€™s see if we can combine what weâ€™ve learned by creating a new example.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#initialize-the-app",
    "href": "03_scaling_out.html#initialize-the-app",
    "title": "3Â  Scaling out",
    "section": "6.1 Initialize the App",
    "text": "6.1 Initialize the App\nLetâ€™s first name the app and create the initial image.\nimport io\nimport os\n\nimport modal\n\napp = modal.App(\n    \"example-boosting-regularization\",\n    image=modal.Image.debian_slim()\n    .pip_install(\"scikit-learn~=1.2.2\")\n    .pip_install(\"matplotlib~=3.9.0\"),\n)\nWe needed to install matplotlib since weâ€™re calling it in our function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "03_scaling_out.html#define-function",
    "href": "03_scaling_out.html#define-function",
    "title": "3Â  Scaling out",
    "section": "6.2 Define function",
    "text": "6.2 Define function\nFor our function, weâ€™ll use:\n@app.function()\ndef fit_boosting(n):\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    from sklearn import datasets, ensemble\n    from sklearn.metrics import log_loss\n    from sklearn.model_selection import train_test_split\n\n    X, y = datasets.make_hastie_10_2(n_samples=n, random_state=1)\n\n    # map labels from {-1, 1} to {0, 1}\n    labels, y = np.unique(y, return_inverse=True)\n\n    # note change from 0.8 to 0.2 test dataset\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    original_params = {\n        \"n_estimators\": 500,\n        \"max_leaf_nodes\": 4,\n        \"max_depth\": None,\n        \"random_state\": 2,\n        \"min_samples_split\": 5,\n    }\n\n    plt.figure()\n\n    for label, color, setting in [\n        (\"No shrinkage\", \"orange\", {\"learning_rate\": 1.0, \"subsample\": 1.0}),\n        (\"learning_rate=0.2\", \"turquoise\", {\"learning_rate\": 0.2, \"subsample\": 1.0}),\n        (\"subsample=0.5\", \"blue\", {\"learning_rate\": 1.0, \"subsample\": 0.5}),\n        (\n            \"learning_rate=0.2, subsample=0.5\",\n            \"gray\",\n            {\"learning_rate\": 0.2, \"subsample\": 0.5},\n        ),\n        (\n            \"learning_rate=0.2, max_features=2\",\n            \"magenta\",\n            {\"learning_rate\": 0.2, \"max_features\": 2},\n        ),\n    ]:\n        params = dict(original_params)\n        params.update(setting)\n\n        clf = ensemble.GradientBoostingClassifier(**params)\n        clf.fit(X_train, y_train)\n\n        # compute test set deviance\n        test_deviance = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n\n        for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n            test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])\n\n        plt.plot(\n            (np.arange(test_deviance.shape[0]) + 1)[::5],\n            test_deviance[::5],\n            \"-\",\n            color=color,\n            label=label,\n        )\n\n    plt.legend(loc=\"upper right\")\n    plt.xlabel(\"Boosting Iterations\")\n    plt.ylabel(\"Test Set Deviance\")\n\n        # Dump the chart to .png and return the bytes\n    with io.BytesIO() as buf:\n        plt.savefig(buf, format=\"png\", dpi=300)\n        return buf.getvalue()\nThis is primarily the scikit-learn demo but a few modifications like:\n\nwe modified the test_size from 0.8 to 0.2\nwe parameterized the sample size n, which weâ€™ll loop through\nweâ€™ll return the chart, similarly from fetch_stock_prices.py\nincreased the number of boosting iterations from 400 to 500\n\nLast, weâ€™ll define the local_entrypoint as:\nOUTPUT_DIR = \"/tmp/modal\"\n\n\n@app.local_entrypoint()\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    for n in [1000,5000,10000,20000,50000]:\n        plot = fit_boosting.remote(n)\n        filename = os.path.join(OUTPUT_DIR, f\"boosting_{n}.png\")\n        print(f\"saving data to {filename}\")\n        with open(filename, \"wb\") as f:\n            f.write(plot)\nThis will end with us saving each of the images into a folder /tmp/modal.\nSo letâ€™s now run this:\n$ modal run boosting_regularization.py\nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /modal-examples/03_scaling_out/boosting_regularization.py\nâ””â”€â”€ ğŸ”¨ Created function fit_boosting.\nsaving data to /tmp/modal/boosting_1000.png\nsaving data to /tmp/modal/boosting_5000.png\nsaving data to /tmp/modal/boosting_10000.png\nsaving data to /tmp/modal/boosting_20000.png\nsaving data to /tmp/modal/boosting_50000.png\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nWe can view a few of the images. For example, this is n = 5000:\n\nThis is particularly interesting due to the subsample = 0.5, which generally follows No shrinkage but then jumps up. Itâ€™s not clear why but a curious case.\nAlternatively, letâ€™s look at n = 10000:\n\nNow we see a result consistent with Vincentâ€™s video as all curves smooth out, none shrinkage learns quickly and then levels out very quickly. Even after 500 iterations no shrinkage has a lower deviance, which indicates a better out-of-sample fit.\nLetâ€™s last look at n = 50000:\n\nVery similar curves again, but this time the gains of no shrinkage is even magnified more as up to 500 iterations thereâ€™s a larger gap between no shrinkage and shrinkage.\nWhatâ€™s nice about Modal is we can also view the remote logs such that:\n\nNot surprising, our last (n = 50000) execution took the longest, taking about 4 minutes and 23 seconds. This is helpful for us to keep in mind and use these logs more as we begin to run more computationally intensive examples moving forward.\nBut thereâ€™s a problem with this setup: weâ€™re missing out on speed gains we could get by parallelizing. Letâ€™s instead use the .map() instead of .remote() when we call our main function:\n\n\nboosting_regularization_parallel.py\n\n...\n\n@app.local_entrypoint()\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    val = [1000,5000,10000,20000,50000]\n    plots = fit_boosting.map(val)\n    for n, plot in zip(val, plots):\n        filename = os.path.join(OUTPUT_DIR, f\"boosting_{n}.png\")\n        print(f\"saving data to {filename}\")\n        with open(filename, \"wb\") as f:\n            f.write(plot)\n\nSo now when we run, all five of the calls run in parallel, yielding a much faster run time.\nWe can confirm this by looking at the logs:",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Scaling out</span>"
    ]
  },
  {
    "objectID": "04_secrets.html",
    "href": "04_secrets.html",
    "title": "4Â  Secrets",
    "section": "",
    "text": "Letâ€™s explore secrets in Modal.\n\n\n\n\n\n\nModal docs on secrets\n\n\n\n\n\nSecrets provide a dictionary of environment variables for images.\nSecrets are a secure way to add credentials and other sensitive information to the containers your functions run in. You can create and edit secrets on the dashboard, or programmatically from Python code.\nTo inject Secrets into the container running your function, you add the secrets=[â€¦] argument to your app.function annotation. For deployed Secrets (typically created via the Modal dashboard) you can refer to those using Secret.from_name(secret_name).\nFor example, if you have a Secret called secret-keys containing the key MY_PASSWORD:\nimport os\nimport modal\n\napp = modal.App()\n\n\n@app.function(secrets=[modal.Secret.from_name(\"secret-keys\")])\ndef some_function():\n    secret_key = os.environ[\"MY_PASSWORD\"]\n    ...\nEach Secret can contain multiple keys and values but you can also inject multiple Secrets, allowing you to separate Secrets into smaller reusable units:\n@app.function(secrets=[\n    modal.Secret.from_name(\"my-secret-name\"),\n    modal.Secret.from_name(\"other-secret\"),\n])\ndef other_function():\n    ...\nThe Secrets are applied in order, so key-values from later modal.Secret objects in the list will overwrite earlier key-values in the case of a clash. For example, if both modal.Secret objects above contained the key FOO, then the value from â€œother-secretâ€ would always be present in os.environ[\"FOO\"].\n\n\n\n\n5 04_secrets.py\nTBD",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Secrets</span>"
    ]
  },
  {
    "objectID": "05_scheduling.html",
    "href": "05_scheduling.html",
    "title": "5Â  Scheduling",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Scheduling</span>"
    ]
  },
  {
    "objectID": "06_gpu_and_ml.html",
    "href": "06_gpu_and_ml.html",
    "title": "6Â  GPU and ML",
    "section": "",
    "text": "6.1 import_torch.py",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>GPU and ML</span>"
    ]
  },
  {
    "objectID": "06_gpu_and_ml.html#import_torch.py",
    "href": "06_gpu_and_ml.html#import_torch.py",
    "title": "6Â  GPU and ML",
    "section": "",
    "text": "6.1.1 PyTorch with CUDA GPU support\nThis example shows how you can use CUDA GPUs in Modal, with a minimal PyTorch image. You can specify GPU requirements in the app.function decorator.\n\n\nimport_torch.py\n\nimport time\n\nimport modal\n\napp = modal.App(\n    \"example-import-torch\",\n    image=modal.Image.debian_slim().pip_install(\n        \"torch\", find_links=\"https://download.pytorch.org/whl/cu116\"\n    ),\n)\n\n\n@app.function(gpu=\"any\")\ndef gpu_function():\n    import subprocess\n\n    import torch\n\n    subprocess.run([\"nvidia-smi\"])\n    print(\"Torch version:\", torch.__version__)\n    print(\"CUDA available:\", torch.cuda.is_available())\n    print(\"CUDA device count:\", torch.cuda.device_count())\n\n\nif __name__ == \"__main__\":\n    t0 = time.time()\n    with app.run():\n        gpu_function.remote()\n    print(\"Full time spent:\", time.time() - t0)\n\nLetâ€™s run it:\n$ modal run 06_gpu_and_ml/import_torch.py \nâœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx\nBuilding image im-q9v0dExl8NyFXzmsp0RKxA\n\n=&gt; Step 0: FROM base\n\n=&gt; Step 1: RUN python -m pip install torch --find-links https://download.pytorch.org/whl/cu116\nLooking in indexes: http://pypi-mirror.modal.local:5555/simple\nLooking in links: https://download.pytorch.org/whl/cu116\nCollecting torch\n  Downloading http://pypi-mirror.modal.local:5555/simple/torch/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 779.1/779.1 MB 249.1 MB/s eta 0:00:00\n\n...\n\nInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.7.0\n    Uninstalling typing_extensions-4.7.0:\n      Successfully uninstalled typing_extensions-4.7.0\nSuccessfully installed MarkupSafe-2.1.5 filelock-3.15.1 fsspec-2024.6.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sympy-1.12.1 torch-2.3.1 triton-2.3.1 typing-extensions-4.12.2\n\n[notice] A new release of pip is available: 23.1.2 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\nCreating image snapshot...\nFinished snapshot; took 11.22s\n\nBuilt image im-q9v0dExl8NyFXzmsp0RKxA in 106.04s\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /Users/ryan/modal-examples/06_gpu_and_ml/import_torch.py\nâ””â”€â”€ ğŸ”¨ Created function gpu_function.\nFri Jun 14 20:37:32 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10                     On  |   00000000:CA:00.0 Off |                 ERR! |\n|  0%   30C ERR!              15W /  150W |       0MiB /  23028MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nTorch version: 2.3.1+cu121\nCUDA available: True\nCUDA device count: 1\nStopping app - local entrypoint completed.\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>GPU and ML</span>"
    ]
  },
  {
    "objectID": "06_gpu_and_ml.html#stable-diffusion-via-hf",
    "href": "06_gpu_and_ml.html#stable-diffusion-via-hf",
    "title": "6Â  GPU and ML",
    "section": "6.2 Stable diffusion via HF",
    "text": "6.2 Stable diffusion via HF\nFor this, youâ€™ll need to create a secret via Modal for Huggingface.\n\n\nstable_diffusion.py\n\nimport io\nimport os\n\nimport modal\n\napp = modal.App()\n\n\n@app.function(\n    image=modal.Image.debian_slim().pip_install(\"torch\", \"diffusers[torch]\", \"transformers\", \"ftfy\"),\n    secrets=[modal.Secret.from_name(\"huggingface\")],\n    gpu=\"any\",\n)\nasync def run_stable_diffusion(prompt: str):\n    from diffusers import StableDiffusionPipeline\n\n    pipe = StableDiffusionPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        use_auth_token=os.environ[\"HF_TOKEN\"],\n    ).to(\"cuda\")\n\n    image = pipe(prompt, num_inference_steps=10).images[0]\n\n    buf = io.BytesIO()\n    image.save(buf, format=\"PNG\")\n    img_bytes = buf.getvalue()\n\n    return img_bytes\n\n\n@app.local_entrypoint()\ndef main():\n    img_bytes = run_stable_diffusion.remote(\"Tri-color beagle riding a bike in Paris, wearing a black beret, and a baguette in a bag in the bike's front basket.\")\n    with open(\"/tmp/parisian-beagle.png\", \"wb\") as f:\n        f.write(img_bytes)\n\nLetâ€™s run it!\n$",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>GPU and ML</span>"
    ]
  },
  {
    "objectID": "07_web_endpoints.html",
    "href": "07_web_endpoints.html",
    "title": "7Â  Web endpoints",
    "section": "",
    "text": "7.1 basic_web.py\nModal makes it easy to turn your Python functions into serverless web services: access them via a browser or call them from any client that speaks HTTP, all without having to worry about setting up servers or managing infrastructure.\nThis tutorial shows the path with the shortest â€œtime to 200â€: modal.web_endpoint.\nOn Modal, web endpoints have all the superpowers of Modal Functions: they can be accelerated with GPUs, they can access Secrets or Volumes, and they automatically scale to handle more traffic.\nUnder the hood, we use the FastAPI library, which has high-quality documentation, linked throughout this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Web endpoints</span>"
    ]
  },
  {
    "objectID": "07_web_endpoints.html#basic_web.py",
    "href": "07_web_endpoints.html#basic_web.py",
    "title": "7Â  Web endpoints",
    "section": "",
    "text": "7.1.1 Turn a Modal Function into an endpoint with a single decorator\nModal Functions are already accessible remotely â€“ when you add the @app.function decorator to a Python function and run modal deploy, you make it possible for your other Python functions to call it.\nThatâ€™s great, but itâ€™s not much help if you want to share what youâ€™ve written with someone running code in a different language â€“ or not running code at all!\nAnd thatâ€™s where most of the power of the Internet comes from: sharing information and functionality across different computer systems.\nSo we provide the web_endpoint decorator to wrap your Modal Functions in the lingua franca of the web: HTTP.\nHereâ€™s what that looks like:\nimport modal\n\napp = modal.App(name=\"example-lifecycle-web\")\n\n\n@app.function()\n@modal.web_endpoint(\n  docs=True  # adds interactive documentation in the browser\n)\ndef hello():\n  return \"Hello world!\"\nYou can turn this function into a web endpoint by running modal serve basic_web.py.\n\n\n\n\n\n\nWarning\n\n\n\nNotice that when your app only has an endpoint, youâ€™ll use modal serve, not modal run.\n\n\n$ modal serve basic_web.py\nâœ“ Initialized. View run at https://modal.com/apps/charlotte-llm/main/ap-xxxxxxxxxx\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /Users/ryan/modal-examples/07_web_endpoints/basic_web.py\nâ”œâ”€â”€ ğŸ”¨ Created web function hello =&gt; https://charlotte-llm--example-lifecycle-web-hello-dev.modal.run\nâ”œâ”€â”€ ğŸ”¨ Created web function goodbye =&gt; https://charlotte-llm--example-lifecycle-web-goodbye-dev.modal.run\nâ”œâ”€â”€ ğŸ”¨ Created web function WebApp.web =&gt; https://charlotte-llm--example-lifecycle-web-webapp-web-dev.modal.run\nâ””â”€â”€ ğŸ”¨ Created web function greet =&gt; https://charlotte-llm--example-lifecycle-web-greet-dev.modal.run\nï¸ï¸âš¡ï¸ Serving... hit Ctrl-C to stop!\nâ””â”€â”€ Watching /Users/ryan/modal-examples/07_web_endpoints.\nâ ¦ Running app...\nIn the output, you should see a URL that ends with hello-dev.modal.run (the first one). If you navigate to this URL, you should see the \"Hello world!\" message appear in your browser.\nYou can also find interactive documentation, powered by OpenAPI and Swagger, if you add /docs to the end of the URL.\nFrom this documentation, you can interact with your endpoint, sending HTTP requests and receiving HTTP responses.\nFor more details, see the FastAPI documentation.\nBy running the endpoint with modal serve, you created a temporary endpoint that will disappear if you interrupt your terminal.\nThese temporary endpoints are great for debugging â€“ when you save a change to any of your dependent files, the endpoint will redeploy.\nTry changing the message to something else, hitting save, and then hitting refresh in your browser or re-sending the request from /docs or the command line. You should see the new message, along with logs in your terminal showing the redeploy and the request.\nWhen youâ€™re ready to deploy this endpoint permanently, run modal deploy basic_web.py. Now, your function will be available even when youâ€™ve closed your terminal or turned off your computer.\n\n\n7.1.2 Send data to a web endpoint\nThe web endpoint above was a bit silly: it always returns the same message.\nMost endpoints need an input to be useful. There are two ways to send data to a web endpoint: - in the URL as a query parameter - in the body of the request as JSON\n\n7.1.2.1 Sending data in query parameters\nBy default, your functionâ€™s arguments are treated as query parameters: they are extracted from the end of the URL, where they should be added in the form ?arg1=foo&arg2=bar.\nFrom the Python side, thereâ€™s hardly anything to do:\n@app.function()\n@modal.web_endpoint(docs=True)\ndef greet(user: str) -&gt; str:\n  return f\"Hello {user}!\"\nIf you are already running modal serve basic_web.py, this endpoint will be available at a URL, printed in your terminal, that ends with greet-dev.modal.run.\nModal provides Python type-hints to get type information in the docs and automatic validation.\nFor example, if you navigate directly to the URL for greet, you will get a detailed error message indicating that the user parameter is missing. Navigate instead to /docs to see how to invoke the endpoint properly.\nYou can read more about query parameters in the FastAPI documentation.\n\n\n7.1.2.2 Sending data in the request body\nFor larger and more complex data, it is generally preferrable to send data in the body of the HTTP request. This body is formatted as JSON, the most common data interchange format on the web.\nTo set up an endpoint that accepts JSON data, add an argument with a dict type-hint to your function.\nThis argument will be populated with the data sent in the request body.\n@app.function()\n@modal.web_endpoint(method=\"POST\", docs=True)\ndef goodbye(data: dict) -&gt; str:\n  name = data.get(\"name\") or \"world\"\n  return f\"Goodbye {name}!\"\nNote that we gave a value of \"POST\" for the method argument here.\nThis argument defines the HTTP request method that the endpoint will respond to, and it defaults to \"GET\". If you head to the URL for the goodbye endpoint in your browser, you will get a 405 Method Not Allowed error, because browsers only send GET requests by default.\nWhile this is technically a separate concern from query parameters versus request bodies and you can define an endpoint that accepts GET requests and uses data from the body, it is considered bad form.\nNavigate to /docs for more on how to invoke the endpoint properly.\nYou will need to send a POST request with a JSON body containing a name key. To get the same typing and validation benefits as with query parameters, use a Pydantic model for this argument.\nYou can read more about request bodies in the FastAPI documentation.\n\n\n\n7.1.3 Handle expensive startup with modal.Cls\nSometimes your endpoint needs to do something before it can handle its first request, like get a value from a database or set the value of a variable. If that step is expensive, like loading a large ML model, itâ€™d be a shame to have to do it every time a request comes in!\nWeb endpoints can be methods on a modal.Cls. Note that they donâ€™t need the modal.method decorator.\nThis example will only set the start_time instance variable once, on container startup.\n@app.cls()\nclass WebApp:\n  @modal.enter()\n  def startup(self):\n      from datetime import datetime, timezone\n\n      print(\"ğŸ Starting up!\")\n      self.start_time = datetime.now(timezone.utc)\n\n  @modal.web_endpoint(docs=True)\n  def web(self):\n      from datetime import datetime, timezone\n\n      current_time = datetime.now(timezone.utc)\n      return {\"start_time\": self.start_time, \"current_time\": current_time}\n\n\n7.1.4 What next?\nModalâ€™s web_endpoint decorator is opinionated and designed for relatively simple web applications â€“ one or a few independent Python functions that you want to expose to the web.\nThree additional decorators allow you to serve more complex web applications with greater control: - asgi_app to serve applications compliant with the ASGI standard, like FastAPI - wsgi_app to serve applications compliant with the WSGI standard, like Flask - web_server to serve any application that listens on a port\nSo letâ€™s examine these different decorators through more examples.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Web endpoints</span>"
    ]
  },
  {
    "objectID": "07_web_endpoints.html#fastapi",
    "href": "07_web_endpoints.html#fastapi",
    "title": "7Â  Web endpoints",
    "section": "7.2 FastAPI",
    "text": "7.2 FastAPI\nLetâ€™s start first with serving a FastAPI app.\n\n\nfastapi_app.py\n\nfrom typing import Optional\n\nfrom fastapi import FastAPI, Header\nfrom modal import App, Image, asgi_app, web_endpoint\nfrom pydantic import BaseModel\n\nweb_app = FastAPI()\napp = App(\"example-fastapi-app\")\nimage = Image.debian_slim()\n\n\nclass Item(BaseModel):\n    name: str\n\n\n@web_app.get(\"/\")\nasync def handle_root(user_agent: Optional[str] = Header(None)):\n    print(f\"GET /     - received user_agent={user_agent}\")\n    return \"Hello World\"\n\n\n@web_app.post(\"/foo\")\nasync def handle_foo(item: Item, user_agent: Optional[str] = Header(None)):\n    print(\n        f\"POST /foo - received user_agent={user_agent}, item.name={item.name}\"\n    )\n    return item\n\n\n@app.function(image=image)\n@asgi_app()\ndef fastapi_app():\n    return web_app\n\n\n@app.function()\n@web_endpoint(method=\"POST\")\ndef f(item: Item):\n    return \"Hello \" + item.name\n\n\nif __name__ == \"__main__\":\n    app.deploy(\"webapp\")\n\nLetâ€™s start first at the top.\nfrom typing import Optional\n\nfrom fastapi import FastAPI, Header\nfrom modal import App, Image, asgi_app, web_endpoint\nfrom pydantic import BaseModel\n\nweb_app = FastAPI()\napp = App(\"example-fastapi-app\")\nimage = Image.debian_slim()\nWeâ€™re using four libraries: modal, typing, pydantic and fastapi.\nWe import FastAPI from fastapi which we can initialize to create a new Fast API web app named web_app.\nWe then create our modal app (app) and our basic image (Debian Slim Docker image) without any additional dependencies.\nFor this endpoint, there are two routes defined for the FastAPI app: - GET \"/\": Returns â€œHello Worldâ€ and prints the user agent. - POST \"/foo\": Accepts an Item in the request body, prints the user agent and item name, and returns the item.\nThere are two Modal function decorators:\n\n@app.function(image=image): Specifies the function should run in the Debian Slim container.\n@asgi_app(): Indicates that this function serves an ASGI application (FastAPI in this case).\n\nThere is also a FastAPI app wrapper, fastapi_app() function, which returns the FastAPI web_app instance, allowing Modal to serve it.\nA separate Modal function f() is defined with a POST method web endpoint. It takes an Item as input and returns a greeting with the itemâ€™s name.\nAn Item class is defined using Pydanticâ€™s BaseModel, with a single name field of type str.\nLast to deploy, if __name__ == \"__main__\" deploys the app to Modal with the name \"webapp\" when the script is run directly.\nSo if we can run it like:\n$ modal serve fastapi_app.py            \nâœ“ Initialized. View run at https://modal.com/apps/charlotte-llm/main/ap-2AgUcGwaVzBvaFq3OyaiMK\nâœ“ Created objects.\nâ”œâ”€â”€ ğŸ”¨ Created mount /Users/ryan/modal-examples/07_web_endpoints/fastapi_app.py\nâ”œâ”€â”€ ğŸ”¨ Created web function fastapi_app =&gt; https://charlotte-llm--example-fastapi-app-fastapi-app-dev.modal.run\nâ””â”€â”€ ğŸ”¨ Created web function f =&gt; https://charlotte-llm--example-fastapi-app-f-dev.modal.run\nï¸ï¸âš¡ï¸ Serving... hit Ctrl-C to stop!",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Web endpoints</span>"
    ]
  },
  {
    "objectID": "08_advanced.html",
    "href": "08_advanced.html",
    "title": "8Â  Advanced",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Advanced</span>"
    ]
  },
  {
    "objectID": "09_job_queues.html",
    "href": "09_job_queues.html",
    "title": "9Â  Job queues",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Job queues</span>"
    ]
  },
  {
    "objectID": "10_integrations.html",
    "href": "10_integrations.html",
    "title": "10Â  Integrations",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Integrations</span>"
    ]
  },
  {
    "objectID": "11_notebooks.html",
    "href": "11_notebooks.html",
    "title": "11Â  Notebooks",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Notebooks</span>"
    ]
  },
  {
    "objectID": "12_datasets.html",
    "href": "12_datasets.html",
    "title": "12Â  Datasets",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "13_finetuning.html",
    "href": "13_finetuning.html",
    "title": "13Â  Finetuning",
    "section": "",
    "text": "13.1 Setup\nWeâ€™ll first need to setup our secrets.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Finetuning</span>"
    ]
  },
  {
    "objectID": "13_finetuning.html#setup",
    "href": "13_finetuning.html#setup",
    "title": "13Â  Finetuning",
    "section": "",
    "text": "Install modal in your current Python virtual environment (pip install modal)\nSet up a Modal token in your environment (python -m modal setup)\nYou need to have a secret named huggingface in your workspace. You can create a new secret with the HuggingFace template in your Modal dashboard, using the key from HuggingFace (in settings under API tokens) to populate HF_TOKEN and changing the name from my-huggingface-secret to huggingface.\nFor some LLaMA models, you need to go to the Hugging Face page (e.g.Â this page for LLaMA 3 8B_ and agree to their Terms and Conditions for access (granted instantly).\nIf you want to use Weights & Biases for logging, you need to have a secret named wandb in your workspace as well. You can also create it from a template. Training is hard enough without good logs, so we recommend you try it or look into axolotlâ€™s integration with MLFlow!",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Finetuning</span>"
    ]
  },
  {
    "objectID": "13_finetuning.html#reproduce-with-pythia-1.4b",
    "href": "13_finetuning.html#reproduce-with-pythia-1.4b",
    "title": "13Â  Finetuning",
    "section": "13.2 Reproduce with Pythia 1.4B",
    "text": "13.2 Reproduce with Pythia 1.4B\nWeâ€™re now ready to run! Letâ€™s use the Pythia 1.4B model (EleutherAI/pythia-1.4b-deduped) on the SQLQA subsample sqlqa.subsample.jsonl.\n\n\n\n\n\n\nWarning\n\n\n\nAs described in its HF model card, Pythia 1.4B and its suite of models is intended for research purposes only and not for deployment.\n\n\n$ export ALLOW_WANDB=true  # if you're using Weights & Biases\n$ modal run --detach src.train --config=config/pythia.yml --data=data/sqlqa.subsample.jsonl\n\n...\n\n[2024-06-19 16:07:13,670] [INFO] [axolotl.load_model:794] [PID:25] [RANK:0] converting modules to torch.bfloat16 for flash attention\n[2024-06-19 16:07:13,840] [INFO] [axolotl.load_lora:951] [PID:25] [RANK:0] found linear modules: ['dense_h_to_4h', 'dense', 'query_key_value', 'dense_4h_to_h']\n[2024-06-19 16:07:13,840] [DEBUG] [axolotl.load_lora:993] [PID:25] [RANK:0] Loading pretrained PEFT - LoRA\ntrainable params: 218,628,096 || all params: 1,633,275,904 || trainable%: 13.385864290568755\n[2024-06-19 16:07:17,148] [INFO] [axolotl.load_model:843] [PID:25] [RANK:0] GPU memory usage after adapters: 0.000GB ()\n[2024-06-19 16:07:17,148] [INFO] [axolotl.scripts.do_merge_lora:144] [PID:25] [RANK:0] running merge of LoRA with base model\nUnloading and merging model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 445/445 [00:03&lt;00:00, 114.95it/s]\n[2024-06-19 16:07:21,028] [INFO] [axolotl.scripts.do_merge_lora:153] [PID:25] [RANK:0] saving merged model to: lora-out/merged\nRun complete. Tag: axo-2024-06-19-16-03-18-54e2\nTo inspect outputs, run `modal volume ls example-runs-vol axo-2024-06-19-16-03-18-54e2`\nTo run sample inference, run `modal run -q src.inference --run-name axo-2024-06-19-16-03-18-54e2`\nâœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-uD7AwcuHYH0lrU9fDIw4Nz\nIâ€™ve skipped a lot of the logs, but thereâ€™s a lot of helpful information. For example, we can see that LoRA trained about 13% of the modelâ€™s 1.6BN parameters (218MM). We can also see where the LoRA model was saved out and its respective Tag.\nWe can view the volumes here:\n$ modal volume ls example-runs-vol axo-2024-06-19-16-03-18-54e2\n      Directory listing of 'axo-2024-06-19-16-03-18-54e2' in 'example-runs-vol'       \nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Filename                                  â”ƒ Type â”ƒ Created/Modified     â”ƒ Size     â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ axo-2024-06-19-16-03-18-54e2/preprocessed â”‚ dir  â”‚ 2024-06-19 12:03 EDT â”‚ 32 B     â”‚\nâ”‚ axo-2024-06-19-16-03-18-54e2/lora-out     â”‚ dir  â”‚ 2024-06-19 12:07 EDT â”‚ 136 B    â”‚\nâ”‚ axo-2024-06-19-16-03-18-54e2/logs.txt     â”‚ file â”‚ 2024-06-19 12:06 EDT â”‚ 133 B    â”‚\nâ”‚ axo-2024-06-19-16-03-18-54e2/data.jsonl   â”‚ file â”‚ 2024-06-19 12:03 EDT â”‚ 20.5 KiB â”‚\nâ”‚ axo-2024-06-19-16-03-18-54e2/config.yml   â”‚ file â”‚ 2024-06-19 12:03 EDT â”‚ 1.6 KiB  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nWe can also run inference like this:\n$ modal run -q src.inference --run-name axo-2024-06-19-16-03-18-54e2 --prompt \"[INST] Using the schema context below, generate a SQL query that answers the question.\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\n        How many students have cat allergies? [/INST]\"\nğŸ§ : Querying model axo-2024-06-19-16-03-18-54e2\nğŸ§ : Initializing vLLM engine for model at /runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged\n2024-06-19 16:34:04,753 INFO worker.py:1753 -- Started a local Ray instance.\nINFO 06-19 16:34:07 llm_engine.py:73] Initializing an LLM engine with config: model=PosixPath('/runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged'), tokenizer=PosixPath('/runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged'), tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO 06-19 16:34:21 llm_engine.py:223] # GPU blocks: 12896, # CPU blocks: 2730\n(RayWorkerVllm pid=309) INFO 06-19 16:34:26 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n(RayWorkerVllm pid=309) [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n(RayWorkerVllm pid=309) INFO 06-19 16:35:01 model_runner.py:437] Graph capturing finished in 35 secs.\n(RayWorkerVllm pid=310) INFO 06-19 16:34:26 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 06-19 16:35:01 async_llm_engine.py:379] Received request 5f8c384f2b7d41bf9553c8aae08c7fd9: prompt: '[INST] Using the schema context below, generate a SQL query that answers the question.\\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\\n        How many students have cat allergies? [/INST]', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.2, top_p=0.95, top_k=50, min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True), prompt token ids: None.\nINFO 06-19 16:35:01 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\nINFO 06-19 16:35:06 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\nINFO 06-19 16:35:11 async_llm_engine.py:111] Finished request 5f8c384f2b7d41bf9553c8aae08c7fd9.\nğŸ§ : Effective throughput of 107.84 tok/s\nğŸ‘¤: [INST] Using the schema context below, generate a SQL query that answers the question.\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\n        How many students have cat allergies? [/INST]\nğŸ¤–: (1)\n        INSERT INTO Has_allergy VALUES ('cat')\n        INSERT INTO Has_allergy VALUES ('dog')\n        INSERT INTO Has_allergy VALUES ('bird')\n        ...\n        INSERT INTO Has_allergy VALUES ('hamster')\n        INSERT INTO Has_allergy VALUES ('mouse')\n        INSERT INTO Has_\nStopping app - local entrypoint completed.\n(RayWorkerVllm pid=310) INFO 06-19 16:35:01 model_runner.py:437] Graph capturing finished in 35 secs.\n(RayWorkerVllm pid=310) [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\nSo this took about a minute, but most of that is due to overhead, which isnâ€™t bad. However, itâ€™s worth noting that our model didnâ€™t go great; this is expected, as we trained LoRA on just a subset with a very small model.\nA few more points to note. First, per the config.yml, weâ€™ve specified the instruction prompt template like:\n      format: |-\n        [INST] Using the schema context below, generate a SQL query that answers the question.\n        {input}\n        {instruction} [/INST]\nSo we passed an example that met this template.\nAnother nice output is we can view the estimated throughput like:\nINFO 06-19 16:35:06 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\nINFO 06-19 16:35:11 async_llm_engine.py:111] Finished request 5f8c384f2b7d41bf9553c8aae08c7fd9.\nğŸ§ : Effective throughput of 107.84 tok/s\nThis is one of the benefits of using vLLM and definitely something we want to keep an eye on down the road when weâ€™re tracking our model in production.\n\n13.2.1 Code review\nLetâ€™s now explore the code we just ran to better understand it.\nWhat weâ€™re most interested in are the scripts in /src/, in particular the train.py file.\nFirst, letâ€™s look at the dependencies:\n\n\ntrain.py\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nimport secrets\n\nfrom .common import (\n    app,\n    axolotl_image,\n    HOURS,\n    MINUTES,\n    VOLUME_CONFIG,\n)\n\n\n13.2.1.1 common.py\nOverall, this is very neat - we donâ€™t have many local dependencies. The one exception is the common.py file, where we are loading common configurations to ensure consistency with both training and inference.\nThere are a few things we do in the common.py:\n\nSet our APP_NAME here.\nSet fixed MINUTES and HOURS, which are passed as timeout values\nSet the Axolotl image by its hash (only used in training)\nSet Python dependencies using .pip_install()\nPass environmental variables like HUGGINGFACE_HUB_CACHE\nSet the vLLM image (only used in inference)\nInitialize the Modal App along with passing the secrets\nConfigure the volumes where weâ€™ll save our logs and model\n\n\n\ncommon.py\n\nimport os\nfrom pathlib import PurePosixPath\nfrom typing import Union\n\nimport modal\n\nAPP_NAME = \"example-axolotl\"\n\nMINUTES = 60  # seconds\nHOURS = 60 * MINUTES\n\n# Axolotl image hash corresponding to main-20240522-py3.11-cu121-2.2.2\nAXOLOTL_REGISTRY_SHA = (\n    \"8ec2116dd36ecb9fb23702278ac612f27c1d4309eca86ad0afd3a3fe4a80ad5b\"\n)\n\nALLOW_WANDB = os.environ.get(\"ALLOW_WANDB\", \"false\").lower() == \"true\"\n\naxolotl_image = (\n    modal.Image.from_registry(f\"winglian/axolotl@sha256:{AXOLOTL_REGISTRY_SHA}\")\n    .pip_install(\n        \"huggingface_hub==0.20.3\",\n        \"hf-transfer==0.1.5\",\n        \"wandb==0.16.3\",\n        \"fastapi==0.110.0\",\n        \"pydantic==2.6.3\",\n    )\n    .env(\n        dict(\n            HUGGINGFACE_HUB_CACHE=\"/pretrained\",\n            HF_HUB_ENABLE_HF_TRANSFER=\"1\",\n            TQDM_DISABLE=\"true\",\n            AXOLOTL_NCCL_TIMEOUT=\"60\",\n        )\n    )\n)\n\nvllm_image = modal.Image.from_registry(\n    \"nvidia/cuda:12.1.0-base-ubuntu22.04\", add_python=\"3.10\"\n).pip_install(\n    \"vllm==0.2.6\",\n    \"torch==2.1.2\",\n)\n\napp = modal.App(\n    APP_NAME,\n    secrets=[\n        modal.Secret.from_name(\"huggingface\"),\n        modal.Secret.from_dict({\"ALLOW_WANDB\": os.environ.get(\"ALLOW_WANDB\", \"false\")}),\n        *([modal.Secret.from_name(\"wandb\")] if ALLOW_WANDB else []),\n    ],\n)\n\n# Volumes for pre-trained models and training runs.\npretrained_volume = modal.Volume.from_name(\n    \"example-pretrained-vol\", create_if_missing=True\n)\nruns_volume = modal.Volume.from_name(\"example-runs-vol\", create_if_missing=True)\nVOLUME_CONFIG: dict[Union[str, PurePosixPath], modal.Volume] = {\n    \"/pretrained\": pretrained_volume,\n    \"/runs\": runs_volume,\n}\n\n\n\n13.2.1.2 train.py\nLetâ€™s now get back to the train.py script (after loading dependencies). Before getting to the main() function, at the top of the script we find:\n\n\ntrain.py\n\nGPU_CONFIG = os.environ.get(\"GPU_CONFIG\", \"a100:2\")\nif len(GPU_CONFIG.split(\":\")) &lt;= 1:\n    N_GPUS = int(os.environ.get(\"N_GPUS\", 2))\n    GPU_CONFIG = f\"{GPU_CONFIG}:{N_GPUS}\"\nSINGLE_GPU_CONFIG = os.environ.get(\"GPU_CONFIG\", \"a10g:1\")\n\nThis code configures GPU settings by retrieving the GPU_CONFIG environment variable, defaulting to â€œa100:2â€ if unset, and ensures it contains a specified GPU type and count format. If the format is incorrect (missing the count), it fetches a default count from N_GPUS or sets it to 2, then updates GPU_CONFIG accordingly.\nBut letâ€™s now look at our main(). This function takes five parameters:\n\nconfig: this is our config file (e.g., pythia.yml)\ndata: this is our .jsonl, which its format is specified in the config file\nmerge_lora (optional): boolean whether to run accelerate launch -m axolotl.cli.merge_lora\npreproc_only (optional): boolean on whether to run preprocessing steps only\nrun_to_resume (optional): A str of a previous run to resume\n\n\n\ntrain.py\n\n@app.local_entrypoint()\ndef main(\n    config: str,\n    data: str,\n    merge_lora: bool = True,\n    preproc_only: bool = False,\n    run_to_resume: str = None,\n):\n    # Read config and data source files and pass their contents to the remote function.\n    with open(config, \"r\") as cfg, open(data, \"r\") as dat:\n        run_name, launch_handle = launch.remote(\n            cfg.read(), dat.read(), run_to_resume, preproc_only\n        )\n\n    # Write a local reference to the location on the remote volume with the run\n    with open(\".last_run_name\", \"w\") as f:\n        f.write(run_name)\n\n    # Wait for the training run to finish.\n    merge_handle = launch_handle.get()\n    if merge_lora and not preproc_only:\n        merge_handle.get()\n\n    print(f\"Run complete. Tag: {run_name}\")\n    print(f\"To inspect outputs, run `modal volume ls example-runs-vol {run_name}`\")\n    if not preproc_only:\n        print(\n            f\"To run sample inference, run `modal run -q src.inference --run-name {run_name}`\"\n        )\n\nThen after loading the config and data, the launch function is run remotely. As shown below, the launch function downloads the base model from HF Hub, sets the run timestamp (or sets it from a previous run), saves config and data files to the volumes, then kicks off training by running train.spawn.\n\n\ntrain.py\n\n@app.function(image=axolotl_image, timeout=30 * MINUTES, volumes=VOLUME_CONFIG)\ndef launch(config_raw: dict, data_raw: str, run_to_resume: str, preproc_only: bool):\n    import yaml\n    from huggingface_hub import snapshot_download\n\n    # Ensure the base model is downloaded\n    # TODO(gongy): test if this works with a path to previous fine-tune\n    config = yaml.safe_load(config_raw)\n    model_name = config[\"base_model\"]\n\n    try:\n        snapshot_download(model_name, local_files_only=True)\n        print(f\"Volume contains {model_name}.\")\n    except FileNotFoundError:\n        print(f\"Downloading {model_name} ...\")\n        snapshot_download(model_name)\n\n        print(\"Committing /pretrained directory (no progress bar) ...\")\n        VOLUME_CONFIG[\"/pretrained\"].commit()\n\n    # Write config and data into a training subfolder.\n    time_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    run_name = (\n        f\"axo-{time_string}-{secrets.token_hex(2)}\"\n        if not run_to_resume\n        else run_to_resume\n    )\n    run_folder = f\"/runs/{run_name}\"\n    os.makedirs(run_folder, exist_ok=True)\n\n    print(f\"Preparing training run in {run_folder}.\")\n    with (\n        open(f\"{run_folder}/config.yml\", \"w\") as config_file,\n        open(f\"{run_folder}/{config['datasets'][0]['path']}\", \"w\") as data_file,\n    ):\n        config_file.write(config_raw)\n        data_file.write(data_raw)\n    VOLUME_CONFIG[\"/runs\"].commit()\n\n    if preproc_only:\n        print(\"Spawning container for data preprocessing.\")\n        launch_handle = preproc_data.spawn(run_folder)\n    else:\n        print(\"Spawning container for data preprocessing.\")\n        preproc_handle = preproc_data.spawn(run_folder)\n        with open(f\"{run_folder}/logs.txt\", \"w\") as f:\n            lbl = \"preproc\"\n            f.write(f\"{lbl}: https://modal.com/logs/call/{preproc_handle.object_id}\")\n        # wait for preprocessing to finish.\n        preproc_handle.get()\n\n        # Start training run.\n        print(\"Spawning container for training.\")\n        launch_handle = train.spawn(run_folder, config[\"output_dir\"])\n\n    with open(f\"{run_folder}/logs.txt\", \"w\") as f:\n        lbl = \"train\" if not preproc_only else \"preproc\"\n        f.write(f\"{lbl}: https://modal.com/logs/call/{launch_handle.object_id}\")\n    VOLUME_CONFIG[\"/runs\"].commit()\n\n    return run_name, launch_handle\n\nSo last, letâ€™s examine the train function:\n\n\ntrain.py\n\n\n@app.function(\n    image=axolotl_image,\n    gpu=GPU_CONFIG,\n    volumes=VOLUME_CONFIG,\n    timeout=24 * HOURS,\n    _allow_background_volume_commits=True,\n)\ndef train(run_folder: str, output_dir: str):\n    import torch\n\n    print(f\"Starting training run in {run_folder}.\")\n    print(f\"Using {torch.cuda.device_count()} {torch.cuda.get_device_name()} GPU(s).\")\n\n    ALLOW_WANDB = os.environ.get(\"ALLOW_WANDB\", \"false\").lower() == \"true\"\n    cmd = f\"accelerate launch -m axolotl.cli.train ./config.yml {'--wandb_mode disabled' if not ALLOW_WANDB else ''}\"\n    run_cmd(cmd, run_folder)\n\n    # Kick off CPU job to merge the LoRA weights into base model.\n    merge_handle = merge.spawn(run_folder, output_dir)\n    with open(f\"{run_folder}/logs.txt\", \"a\") as f:\n        f.write(f\"&lt;br&gt;merge: https://modal.com/logs/call/{merge_handle.object_id}\\n\")\n        print(f\"Beginning merge {merge_handle.object_id}.\")\n    return merge_handle\n\nFrom this, we can see that weâ€™re largely running accelerate launch -m axolotl.cli.train but using run_cmd, which run the command inside a folder with Modal Volume reloading before and commiting on success.\n\n\n13.2.1.3 config\nLetâ€™s now do a deep dive on the config file as itâ€™s a critical input. You may also find this config doc to be helpful in explaining the configuraiton options.\nFirst, letâ€™s start at the top:\n\n\npythia.yml\n\n# Lightweight example of training a small Pythia model for simple demonstrations\nbase_model: EleutherAI/pythia-1.4b-deduped\n\nload_in_8bit: false # pythia is small, so keep it in 16-bit precision\nstrict: false\n\nHere we specify the base_model, which we decided to start with Pythia model for demo purposes. We also decided not to quantize our model to 8bit since Pythia is so small; therefore, this is why we ran LoRA, not QLoRA.\n\n\npythia.yml\n\ndatasets:\n  # This will be the path used for the data when it is saved to the Volume in the cloud.\n  - path: data.jsonl\n    ds_type: json\n    type:\n      # JSONL file contains question, context, answer fields per line.\n      # This gets mapped to instruction, input, output axolotl tags.\n      field_instruction: question\n      field_input: context\n      field_output: answer\n      # Format is used by axolotl to generate the prompt.\n      format: |-\n        [INST] Using the schema context below, generate a SQL query that answers the question.\n        {input}\n        {instruction} [/INST]\n\nThe next part shows the dataset configuration. In this case, we set the path and ds_type for the data when it is saved to the cloud Volume.\nNext, we specify the mapping of the key values in our .jsonl file to the instruction, input, and output. This generally follows the alpaca standard where our prompt is expecting those three formats.\n\n\n\n\n\n\nHow to add custom prompt format\n\n\n\n\n\nPer the docs, for a dataset that is preprocessed for instruction purposes:\n\n\ndata.jsonl\n\n{\"input\": \"...\", \"output\": \"...\"}\n\nYou can use this example in your YAML config:\n\n\nconfig.yaml\n\ndatasets:\n  - path: repo\n    type:\n      system_prompt: \"\"\n      field_system: system\n      field_instruction: input\n      field_output: output\n      format: \"[INST] {instruction} [/INST]\"\n      no_input_format: \"[INST] {instruction} [/INST]\"\n\nSee the docs for other data format options\n\n\n\n\n\npythia.yml\n\n# add tokens\ntokens:\n  - \"[INST]\"\n  - \" [/INST]\"\n  - \"[SQL]\"\n  - \" [/SQL]\"\n\ndataset_prepared_path: preprocessed\nval_set_size: 0.05\noutput_dir: ./lora-out\n\n# max length of an input to train with, typically less than 2048 as most have context limit of 2048\nsequence_len: 4096\n# use efficient multi-packing with block diagonal attention and per sequence position_ids.\nsample_packing: false\neval_sample_packing: false\n# Pad inputs so each step uses constant sized buffers\n# This will reduce memory fragmentation and may prevent OOMs, by re-using memory more efficiently\npad_to_sequence_len: false\n\nThis part provides special tokens, paths, and some settings for handling padding and maximium training sequence length.\n\n\npythia.yml\n\nadapter: lora\nlora_model_dir:\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out: true # required for pythia/GPTNeoX lora\nlora_target_modules:\n  - query_key_value\nlora_modules_to_save: # required when adding new tokens to pythia\n  - embed_in\n  - embed_out\n\ngradient_accumulation_steps: 1\nmicro_batch_size: 32\nnum_epochs: 1\noptimizer: adamw_torch\nlr_scheduler: cosine\nlearning_rate: 0.0001\n\nbf16: auto\nfp16: false\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nlocal_rank:\nlogging_steps: 1\n\nwarmup_steps: 10\nweight_decay: 0.0\n\nThis last part is where weâ€™re customizing our LoRA model. For this, Iâ€™m not going to go into detail but I encourage the curious reader to read on Sebastian Raschkaâ€™s wonderful Practical Tips for Finetuning LLMs. It explains a lot of the intuition on LoRA and tips from LoRA training experiments.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Finetuning</span>"
    ]
  },
  {
    "objectID": "13_finetuning.html#warnings-and-errors",
    "href": "13_finetuning.html#warnings-and-errors",
    "title": "13Â  Finetuning",
    "section": "13.3 Warnings and Errors",
    "text": "13.3 Warnings and Errors\nI found several warnings in the logs, so letâ€™s review them.\nFirst, letâ€™s note the versions of dependencies:\n****************************************\n**** Axolotl Dependency Versions *****\n  accelerate: 0.30.1         \n        peft: 0.10.0         \ntransformers: 4.40.2         \n         trl: 0.8.5          \n       torch: 2.2.2+cu121    \nbitsandbytes: 0.43.1         \n****************************************\nThis isnâ€™t surprising as we specified this in our common.py script, but Iâ€™m noting just for documentation sake.\nThe following values were not passed to `accelerate launch` and had defaults used instead:\n        `--num_processes` was set to a value of `2`\n                More than one GPU was found, enabling multi-GPU training.\n                If this was unintended please pass in `--num_processes=1`.\n        `--num_machines` was set to a value of `1`\n        `--mixed_precision` was set to a value of `'no'`\n        `--dynamo_backend` was set to a value of `'no'`\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\nWARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\nThis can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\nIf this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\nIf you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\nFor example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;path_to_cuda_dir/lib64\nFor these warnings, theyâ€™re just that: warnings of defaults being used but not deterimental (IMHO) to training.\n[2024-06-19 16:04:15,288] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\ndf: /root/.triton/autotune: No such file or directory\nThis warning seems not to find a Triton file; this doesnâ€™t seem to be a problem but worth noting.\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n [WARNING]  sparse_attn requires a torch version &gt;= 1.5 and &lt; 2.0 but detected 2.2\n [WARNING]  using untested triton version (2.2.0), only 1.0.0 is known to be compatible\nFor this warning, this seems to mention that sparse_attn uses an older version of torch that what weâ€™re running. This seems to be a bit of a known problem so weâ€™ll ignore this for now.\n[2024-06-19 16:04:20,191] [WARNING] [axolotl.utils.config.models.input.hint_lora_8bit:973] [PID:28] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning\nThis is expected. Back in the pythia.yml, thereâ€™s this comment:\nload_in_8bit: false # pythia is small, so keep it in 16-bit precision\n[2024-06-19 16:04:26,870] [INFO] [axolotl.load_tokenizer:294] [PID:29] [RANK:1] No Chat template selected. Consider adding a chat template for easier inference.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nThis seems like a recommended best practice. Iâ€™ll try to remember this for next time.\n[2024-06-19 16:06:10,805] [WARNING] [axolotl.load_model:712] [PID:28] [RANK:0] increasing model.config.max_position_embeddings from 2048 to 4096\n/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1050: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n  warnings.warn(\nThese two warnings are a bit a little surprising as we saw in the pythia.yml, e.g., lora_fan_in_fan_out: true # required for pythia/GPTNeoX lora and sequence_len: 4096. This is worth more investigation and awareness.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Finetuning</span>"
    ]
  },
  {
    "objectID": "13_finetuning.html#train-lora-on-llama-3-8b",
    "href": "13_finetuning.html#train-lora-on-llama-3-8b",
    "title": "13Â  Finetuning",
    "section": "13.4 Train LoRA on LLaMA 3 8B",
    "text": "13.4 Train LoRA on LLaMA 3 8B\nNow that weâ€™ve explored the base code a bit better, letâ€™s see if we can rerun a LoRA training, but this time with weâ€™ll make a few changes to llama-3.yml:\n\nWeâ€™ll use NousResearch/Meta-Llama-3-8B. This is a larger model and more state-of-the-art.\nWeâ€™ll use the larger sqlqa.jsonl dataset, which is 4,000 records instead of 64 from sqlqa_subsample.jsonl\nChange the Modal app name in common.py to APP_NAME = \"sqlqa-llama-3-8b\"\nChange in the config to deepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json and use GPU_CONFIG=a100-80gb:4\n\n$ GPU_CONFIG=a100-80gb:4 modal run --detach src.train --config=config/llama-3.yml --data=data/sqlqa.jsonl\n\n...\n\n\n[2024-06-19 20:12:56,242] [INFO] [axolotl.scripts.do_merge_lora:153] [PID:27] [RANK:0] saving merged model to: lora-out/merged\nUnloading and merging model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [00:00&lt;00:00, 3664.51it/s]\nUnloading and merging model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [00:00&lt;00:00, 3580.76it/s]\nRun complete. Tag: axo-2024-06-19-19-47-41-2f6c\nTo inspect outputs, run `modal volume ls example-runs-vol axo-2024-06-19-19-47-41-2f6c`\nTo run sample inference, run `modal run -q src.inference --run-name axo-2024-06-19-19-47-41-2f6c`\nRunner terminated.\nSo weâ€™ve now trained our LoRA adapter!\nWe can view some of the logs in Modalâ€™s UI:\n\nNice! We can see all four A100â€™s are working, with the peak GPU RAM about 180GB. For four epochs, this took about 20 minutes, which isnâ€™t too bad.\nLetâ€™s now test this by running inference in CLI:\n$ modal run -q src.inference --run-name axo-2024-06-19-19-47-41-2f6c --prompt \"[INST] Using the schema context below, generate a SQL query that answers the question.\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\n        How many students have cat allergies? [/INST]\"\nğŸ§ : Querying model axo-2024-06-19-19-47-41-2f6c\nğŸ§ : Initializing vLLM engine for model at /runs/axo-2024-06-19-19-47-41-2f6c/lora-out/merged\n2024-06-19 20:18:11,543 INFO worker.py:1753 -- Started a local Ray instance.\n\n...\n\nINFO 06-19 20:19:38 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\nINFO 06-19 20:19:39 async_llm_engine.py:111] Finished request b99f9a9858e346009969a9f41764c9d2.\nğŸ§ : Effective throughput of 30.91 tok/s\nğŸ‘¤: [INST] Using the schema context below, generate a SQL query that answers the question.\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\n        How many students have cat allergies? [/INST]\nğŸ¤–: [SQL] SELECT COUNT(*) FROM Has_allergy WHERE Allergy = \"Cat\" [/SQL]\n\nStopping app - local entrypoint completed.\n(RayWorkerVllm pid=309) [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n(RayWorkerVllm pid=309) INFO 06-19 20:19:38 model_runner.py:437] Graph capturing finished in 34 secs.\nRunner terminated.\nNice! We have a much better result of what weâ€™re looking for.\nOne thing I noticed after the fact was I forgot to setup a Weights and Bias project â€“ make sure to set wandb_project: sqlqa-llama3-lora in the config file. Thatâ€™s okay - weâ€™ll make sure to do this in our next chapter when we run a custom fine tuning project.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Finetuning</span>"
    ]
  }
]