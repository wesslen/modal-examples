<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ryan Wesslen">

<title>Modal Examples - 14&nbsp; Finetuning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12_datasets.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13_finetuning.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Finetuning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modal Examples</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Index</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting Started</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_building_containers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building Containers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_scaling_out.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Scaling out</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_secrets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Secrets</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_scheduling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Scheduling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_gpu_and_ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">GPU and ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_web_endpoints.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Web endpoints</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Advanced</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_job_queues.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Job queues</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_integrations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Notebooks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_finetuning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Finetuning</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup"><span class="header-section-number">14.1</span> Setup</a></li>
  <li><a href="#reproduce-with-pythia-1.4b" id="toc-reproduce-with-pythia-1.4b" class="nav-link" data-scroll-target="#reproduce-with-pythia-1.4b"><span class="header-section-number">14.2</span> Reproduce with Pythia 1.4B</a>
  <ul class="collapse">
  <li><a href="#code-review" id="toc-code-review" class="nav-link" data-scroll-target="#code-review"><span class="header-section-number">14.2.1</span> Code review</a></li>
  </ul></li>
  <li><a href="#warnings-and-errors" id="toc-warnings-and-errors" class="nav-link" data-scroll-target="#warnings-and-errors"><span class="header-section-number">14.3</span> Warnings and Errors</a></li>
  <li><a href="#train-lora-on-llama-3-8b" id="toc-train-lora-on-llama-3-8b" class="nav-link" data-scroll-target="#train-lora-on-llama-3-8b"><span class="header-section-number">14.4</span> Train LoRA on LLaMA 3 8B</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Finetuning</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">llm-finetuning GH repo</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ryan Wesslen </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>Unlike the previous chapters, this chapter uses a separate GitHub repo: <a href="https://github.com/modal-labs/llm-finetuning">github.com/modal-labs/llm-finetuning</a>. This example provides a SQL fine tuning example where the goal is to ask questions along with a SQL schema template and the model outputs a SQL query.</p>
<p>For this, weâ€™ll do a few passes. First this chapter, weâ€™ll run the current code and explaining parts. But in the next Chapter, weâ€™ll start a new extension of this for a custom problem.</p>
<section id="setup" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">14.1</span> Setup</h2>
<p>Weâ€™ll first need to setup our secrets.</p>
<ol type="1">
<li>Install <code>modal</code> in your current Python virtual environment (<code>pip install modal</code>)</li>
<li>Set up a Modal token in your environment (<code>python -m modal setup</code>)</li>
<li>You need to have a secret named <code>huggingface</code> in your workspace. You can create a new secret with the HuggingFace template in your Modal dashboard, using the key from HuggingFace (in settings under API tokens) to populate HF_TOKEN and changing the name from <code>my-huggingface-secret</code> to <code>huggingface</code>.</li>
<li>For some LLaMA models, you need to go to the Hugging Face page (e.g.&nbsp;this page for LLaMA 3 8B_ and agree to their Terms and Conditions for access (granted instantly).</li>
<li>If you want to use Weights &amp; Biases for logging, you need to have a secret named <code>wandb</code> in your workspace as well. You can also create it from a template. Training is hard enough without good logs, so we recommend you try it or look into axolotlâ€™s integration with MLFlow!</li>
</ol>
</section>
<section id="reproduce-with-pythia-1.4b" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="reproduce-with-pythia-1.4b"><span class="header-section-number">14.2</span> Reproduce with Pythia 1.4B</h2>
<p>Weâ€™re now ready to run! Letâ€™s use the Pythia 1.4B model (<a href="https://huggingface.co/EleutherAI/pythia-1.4b-deduped"><code>EleutherAI/pythia-1.4b-deduped</code></a>) on the SQLQA subsample <code>sqlqa.subsample.jsonl</code>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>As described in its <a href="https://huggingface.co/EleutherAI/pythia-1.4b-deduped#out-of-scope-use">HF model card</a>, Pythia 1.4B and its suite of models is intended for research purposes only and <strong>not</strong> for deployment.</p>
</div>
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> export ALLOW_WANDB=true  <span class="co"># if you're using Weights &amp; Biases</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal run <span class="at">--detach</span> src.train <span class="at">--config</span><span class="op">=</span>config/pythia.yml <span class="at">--data</span><span class="op">=</span>data/sqlqa.subsample.jsonl</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:13,670] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_model:794</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> converting modules to torch.bfloat16 for flash attention</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:13,840] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_lora:951</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> found linear modules: [<span class="st">'dense_h_to_4h'</span>, <span class="st">'dense'</span>, <span class="st">'query_key_value'</span>, <span class="st">'dense_4h_to_h'</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:13,840] <span class="pp">[</span><span class="ss">DEBUG</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_lora:993</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> Loading pretrained PEFT <span class="at">-</span> LoRA</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="ex">trainable</span> params: 218,628,096 <span class="kw">||</span> <span class="ex">all</span> params: 1,633,275,904 <span class="kw">||</span> <span class="ex">trainable%:</span> 13.385864290568755</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:17,148] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_model:843</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> GPU memory usage after adapters: 0.000GB <span class="er">(</span><span class="kw">)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:17,148] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.scripts.do_merge_lora:144</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> running merge of LoRA with base model</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="ex">Unloading</span> and merging model: 100%<span class="kw">|</span><span class="ex">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span><span class="kw">|</span> <span class="ex">445/445</span> [00:03<span class="op">&lt;</span>00:00, 114.95it/s]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:21,028] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.scripts.do_merge_lora:153</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> saving merged model to: lora-out/merged</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="ex">Run</span> complete. Tag: axo-2024-06-19-16-03-18-54e2</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> inspect outputs, run <span class="kw">`</span><span class="ex">modal</span> volume ls example-runs-vol axo-2024-06-19-16-03-18-54e2<span class="kw">`</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> run sample inference, run <span class="kw">`</span><span class="ex">modal</span> run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-16-03-18-54e2<span class="kw">`</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ex">âœ“</span> App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-uD7AwcuHYH0lrU9fDIw4Nz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Iâ€™ve skipped a lot of the logs, but thereâ€™s a lot of helpful information. For example, we can see that LoRA trained about 13% of the modelâ€™s 1.6BN parameters (218MM). We can also see where the LoRA model was saved out and its respective Tag.</p>
<p>We can view the volumes here:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal volume ls example-runs-vol axo-2024-06-19-16-03-18-54e2</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>      <span class="ex">Directory</span> listing of <span class="st">'axo-2024-06-19-16-03-18-54e2'</span> in <span class="st">'example-runs-vol'</span>       </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="ex">â”ƒ</span> Filename                                  â”ƒ Type â”ƒ Created/Modified     â”ƒ Size     â”ƒ</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="ex">â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/preprocessed â”‚ dir  â”‚ 2024-06-19 12:03 EDT â”‚ 32 B     â”‚</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/lora-out     â”‚ dir  â”‚ 2024-06-19 12:07 EDT â”‚ 136 B    â”‚</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/logs.txt     â”‚ file â”‚ 2024-06-19 12:06 EDT â”‚ 133 B    â”‚</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/data.jsonl   â”‚ file â”‚ 2024-06-19 12:03 EDT â”‚ 20.5 KiB â”‚</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/config.yml   â”‚ file â”‚ 2024-06-19 12:03 EDT â”‚ 1.6 KiB  â”‚</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="ex">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can also run inference like this:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-16-03-18-54e2 <span class="at">--prompt</span> <span class="st">"[INST] Using the schema context below, generate a SQL query that answers the question.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="st">        CREATE TABLE Has_allergy (Allergy VARCHAR)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="st">        How many students have cat allergies? [/INST]"</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Querying model axo-2024-06-19-16-03-18-54e2</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Initializing vLLM engine for model at /runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">2024-06-19</span> 16:34:04,753 INFO worker.py:1753 <span class="at">--</span> Started a local Ray instance.</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:34:07 llm_engine.py:73] Initializing an LLM engine with config: model=PosixPath<span class="er">(</span><span class="st">'/runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged'</span><span class="kw">)</span><span class="ex">,</span> tokenizer=PosixPath<span class="er">(</span><span class="st">'/runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged'</span><span class="kw">)</span><span class="ex">,</span> tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0<span class="er">)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Special</span> tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:34:21 llm_engine.py:223] <span class="co"># GPU blocks: 12896, # CPU blocks: 2730</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:34:26 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set <span class="st">'enforce_eager=True'</span> or use <span class="st">'--enforce-eager'</span> in the CLI.</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">[W</span> CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. <span class="er">(</span><span class="kw">function</span><span class="fu"> operator()</span><span class="kw">)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:35:01 model_runner.py:437] Graph capturing finished in 35 secs.</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=310<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:34:26 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set <span class="st">'enforce_eager=True'</span> or use <span class="st">'--enforce-eager'</span> in the CLI.</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:01 async_llm_engine.py:379] Received request 5f8c384f2b7d41bf9553c8aae08c7fd9: prompt: <span class="st">'[INST] Using the schema context below, generate a SQL query that answers the question.\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\n        How many students have cat allergies? [/INST]'</span>, sampling params: SamplingParams<span class="er">(</span><span class="va">n</span><span class="op">=</span>1, <span class="va">best_of</span><span class="op">=</span>1, <span class="va">presence_penalty</span><span class="op">=</span>0.0, <span class="va">frequency_penalty</span><span class="op">=</span>0.0, <span class="va">repetition_penalty</span><span class="op">=</span>1.1, <span class="va">temperature</span><span class="op">=</span>0.2, <span class="va">top_p</span><span class="op">=</span>0.95, <span class="va">top_k</span><span class="op">=</span>50, <span class="va">min_p</span><span class="op">=</span>0.0, <span class="va">use_beam_search</span><span class="op">=</span>False, <span class="va">length_penalty</span><span class="op">=</span>1.0, <span class="va">early_stopping</span><span class="op">=</span>False, <span class="va">stop</span><span class="op">=</span>[], <span class="va">stop_token_ids</span><span class="op">=</span>[], <span class="va">include_stop_str_in_output</span><span class="op">=</span>False, <span class="va">ignore_eos</span><span class="op">=</span>False, <span class="va">max_tokens</span><span class="op">=</span>1024, <span class="va">logprobs</span><span class="op">=</span>None, <span class="va">prompt_logprobs</span><span class="op">=</span>None, <span class="va">skip_special_tokens</span><span class="op">=</span>True, <span class="va">spaces_between_special_tokens</span><span class="op">=</span>True<span class="kw">)</span><span class="ex">,</span> prompt token ids: None.</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:01 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:06 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:11 async_llm_engine.py:111] Finished request 5f8c384f2b7d41bf9553c8aae08c7fd9.</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Effective throughput of 107.84 tok/s</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ‘¤:</span> <span class="pp">[</span><span class="ss">INST</span><span class="pp">]</span> Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="ex">CREATE</span> TABLE Has_allergy <span class="er">(</span><span class="ex">Allergy</span> VARCHAR<span class="kw">)</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="ex">How</span> many students have cat allergies<span class="pp">?</span> <span class="pp">[</span><span class="ss">/INST</span><span class="pp">]</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ¤–:</span> <span class="er">(</span><span class="ex">1</span><span class="kw">)</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'cat'</span><span class="kw">)</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'dog'</span><span class="kw">)</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'bird'</span><span class="kw">)</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="ex">...</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'hamster'</span><span class="kw">)</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'mouse'</span><span class="kw">)</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="ex">Stopping</span> app <span class="at">-</span> local entrypoint completed.</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=310<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:35:01 model_runner.py:437] Graph capturing finished in 35 secs.</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=310<span class="kw">)</span> <span class="ex">[W</span> CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. <span class="er">(</span><span class="kw">function</span><span class="fu"> operator()</span><span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So this took about a minute, but most of that is due to overhead, which isnâ€™t bad. However, itâ€™s worth noting that our model didnâ€™t go great; this is expected, as we trained LoRA on just a subset with a very small model.</p>
<p>A few more points to note. First, per the <code>config.yml</code>, weâ€™ve specified the instruction prompt template like:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">      format</span><span class="kw">: </span><span class="ch">|-</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>        [INST] Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        {input}</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        {instruction} [/INST]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So we passed an example that met this template.</p>
<p>Another nice output is we can view the estimated throughput like:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:06 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:11 async_llm_engine.py:111] Finished request 5f8c384f2b7d41bf9553c8aae08c7fd9.</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Effective throughput of 107.84 tok/s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is one of the benefits of using vLLM and definitely something we want to keep an eye on down the road when weâ€™re tracking our model in production.</p>
<section id="code-review" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="code-review"><span class="header-section-number">14.2.1</span> Code review</h3>
<p>Letâ€™s now explore the code we just ran to better understand it.</p>
<p>What weâ€™re most interested in are the scripts in <code>/src/</code>, in particular the <code>train.py</code> file.</p>
<p>First, letâ€™s look at the dependencies:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>train.py</strong></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> secrets</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> .common <span class="im">import</span> (</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    app,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    axolotl_image,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    HOURS,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    MINUTES,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    VOLUME_CONFIG,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="common.py" class="level4" data-number="14.2.1.1">
<h4 data-number="14.2.1.1" class="anchored" data-anchor-id="common.py"><span class="header-section-number">14.2.1.1</span> common.py</h4>
<p>Overall, this is very neat - we donâ€™t have many local dependencies. The one exception is the <code>common.py</code> file, where we are loading common configurations to ensure consistency with both training and inference.</p>
<p>There are a few things we do in the <code>common.py</code>:</p>
<ol type="1">
<li>Set our <code>APP_NAME</code> here.</li>
<li>Set fixed <code>MINUTES</code> and <code>HOURS</code>, which are passed as timeout values</li>
<li>Set the <code>Axolotl</code> <a href="https://hub.docker.com/layers/winglian/axolotl-cloud/main-20240522-py3.11-cu121-2.2.2/images/sha256-e6c2d1f38083869baa71ebf2f3b2a82678acc980534adb525c78582723f2c9a3">image</a> by its hash (only used in training)</li>
<li>Set Python dependencies using <code>.pip_install()</code></li>
<li>Pass environmental variables like <code>HUGGINGFACE_HUB_CACHE</code></li>
<li>Set the <code>vLLM</code> image (only used in inference)</li>
<li>Initialize the Modal App along with passing the secrets</li>
<li>Configure the volumes where weâ€™ll save our logs and model</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>common.py</strong></pre>
</div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> PurePosixPath</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Union</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> modal</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>APP_NAME <span class="op">=</span> <span class="st">"example-axolotl"</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>MINUTES <span class="op">=</span> <span class="dv">60</span>  <span class="co"># seconds</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>HOURS <span class="op">=</span> <span class="dv">60</span> <span class="op">*</span> MINUTES</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Axolotl image hash corresponding to main-20240522-py3.11-cu121-2.2.2</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>AXOLOTL_REGISTRY_SHA <span class="op">=</span> (</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"8ec2116dd36ecb9fb23702278ac612f27c1d4309eca86ad0afd3a3fe4a80ad5b"</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>ALLOW_WANDB <span class="op">=</span> os.environ.get(<span class="st">"ALLOW_WANDB"</span>, <span class="st">"false"</span>).lower() <span class="op">==</span> <span class="st">"true"</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>axolotl_image <span class="op">=</span> (</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    modal.Image.from_registry(<span class="ss">f"winglian/axolotl@sha256:</span><span class="sc">{</span>AXOLOTL_REGISTRY_SHA<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    .pip_install(</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"huggingface_hub==0.20.3"</span>,</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hf-transfer==0.1.5"</span>,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">"wandb==0.16.3"</span>,</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">"fastapi==0.110.0"</span>,</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">"pydantic==2.6.3"</span>,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    .env(</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">dict</span>(</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>            HUGGINGFACE_HUB_CACHE<span class="op">=</span><span class="st">"/pretrained"</span>,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>            HF_HUB_ENABLE_HF_TRANSFER<span class="op">=</span><span class="st">"1"</span>,</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            TQDM_DISABLE<span class="op">=</span><span class="st">"true"</span>,</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>            AXOLOTL_NCCL_TIMEOUT<span class="op">=</span><span class="st">"60"</span>,</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>vllm_image <span class="op">=</span> modal.Image.from_registry(</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">"nvidia/cuda:12.1.0-base-ubuntu22.04"</span>, add_python<span class="op">=</span><span class="st">"3.10"</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>).pip_install(</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vllm==0.2.6"</span>,</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"torch==2.1.2"</span>,</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>app <span class="op">=</span> modal.App(</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    APP_NAME,</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    secrets<span class="op">=</span>[</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>        modal.Secret.from_name(<span class="st">"huggingface"</span>),</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>        modal.Secret.from_dict({<span class="st">"ALLOW_WANDB"</span>: os.environ.get(<span class="st">"ALLOW_WANDB"</span>, <span class="st">"false"</span>)}),</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>([modal.Secret.from_name(<span class="st">"wandb"</span>)] <span class="cf">if</span> ALLOW_WANDB <span class="cf">else</span> []),</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Volumes for pre-trained models and training runs.</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>pretrained_volume <span class="op">=</span> modal.Volume.from_name(</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"example-pretrained-vol"</span>, create_if_missing<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>runs_volume <span class="op">=</span> modal.Volume.from_name(<span class="st">"example-runs-vol"</span>, create_if_missing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>VOLUME_CONFIG: <span class="bu">dict</span>[Union[<span class="bu">str</span>, PurePosixPath], modal.Volume] <span class="op">=</span> {</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>    <span class="st">"/pretrained"</span>: pretrained_volume,</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">"/runs"</span>: runs_volume,</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="train.py" class="level4" data-number="14.2.1.2">
<h4 data-number="14.2.1.2" class="anchored" data-anchor-id="train.py"><span class="header-section-number">14.2.1.2</span> train.py</h4>
<p>Letâ€™s now get back to the <code>train.py</code> script (after loading dependencies). Before getting to the <code>main()</code> function, at the top of the script we find:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>train.py</strong></pre>
</div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>GPU_CONFIG <span class="op">=</span> os.environ.get(<span class="st">"GPU_CONFIG"</span>, <span class="st">"a100:2"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(GPU_CONFIG.split(<span class="st">":"</span>)) <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    N_GPUS <span class="op">=</span> <span class="bu">int</span>(os.environ.get(<span class="st">"N_GPUS"</span>, <span class="dv">2</span>))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    GPU_CONFIG <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>GPU_CONFIG<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>N_GPUS<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>SINGLE_GPU_CONFIG <span class="op">=</span> os.environ.get(<span class="st">"GPU_CONFIG"</span>, <span class="st">"a10g:1"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code configures GPU settings by retrieving the <code>GPU_CONFIG</code> environment variable, defaulting to â€œa100:2â€ if unset, and ensures it contains a specified GPU type and count format. If the format is incorrect (missing the count), it fetches a default count from <code>N_GPUS</code> or sets it to 2, then updates <code>GPU_CONFIG</code> accordingly.</p>
<p>But letâ€™s now look at our <code>main()</code>. This function takes five parameters:</p>
<ol type="1">
<li><code>config</code>: this is our config file (e.g., <code>pythia.yml</code>)</li>
<li><code>data</code>: this is our <code>.jsonl</code>, which its format is specified in the config file</li>
<li><code>merge_lora</code> (optional): boolean whether to run <code>accelerate launch -m axolotl.cli.merge_lora</code></li>
<li><code>preproc_only</code> (optional): boolean on whether to run preprocessing steps only</li>
<li><code>run_to_resume</code> (optional): A <code>str</code> of a previous run to resume</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>train.py</strong></pre>
</div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="at">@app.local_entrypoint</span>()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    config: <span class="bu">str</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    data: <span class="bu">str</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    merge_lora: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    preproc_only: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    run_to_resume: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read config and data source files and pass their contents to the remote function.</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(config, <span class="st">"r"</span>) <span class="im">as</span> cfg, <span class="bu">open</span>(data, <span class="st">"r"</span>) <span class="im">as</span> dat:</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        run_name, launch_handle <span class="op">=</span> launch.remote(</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            cfg.read(), dat.read(), run_to_resume, preproc_only</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write a local reference to the location on the remote volume with the run</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">".last_run_name"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        f.write(run_name)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Wait for the training run to finish.</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    merge_handle <span class="op">=</span> launch_handle.get()</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> merge_lora <span class="kw">and</span> <span class="kw">not</span> preproc_only:</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        merge_handle.get()</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Run complete. Tag: </span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"To inspect outputs, run `modal volume ls example-runs-vol </span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">`"</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> preproc_only:</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"To run sample inference, run `modal run -q src.inference --run-name </span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">`"</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then after loading the config and data, the <code>launch</code> function is run remotely. As shown below, the <code>launch</code> function downloads the base model from HF Hub, sets the run timestamp (or sets it from a previous run), saves config and data files to the volumes, then kicks off training by running <code>train.spawn</code>.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>train.py</strong></pre>
</div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="at">@app.function</span>(image<span class="op">=</span>axolotl_image, timeout<span class="op">=</span><span class="dv">30</span> <span class="op">*</span> MINUTES, volumes<span class="op">=</span>VOLUME_CONFIG)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> launch(config_raw: <span class="bu">dict</span>, data_raw: <span class="bu">str</span>, run_to_resume: <span class="bu">str</span>, preproc_only: <span class="bu">bool</span>):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> yaml</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> huggingface_hub <span class="im">import</span> snapshot_download</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure the base model is downloaded</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">(gongy): test if this works with a path to previous fine-tune</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> yaml.safe_load(config_raw)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    model_name <span class="op">=</span> config[<span class="st">"base_model"</span>]</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        snapshot_download(model_name, local_files_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Volume contains </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> ..."</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        snapshot_download(model_name)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Committing /pretrained directory (no progress bar) ..."</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        VOLUME_CONFIG[<span class="st">"/pretrained"</span>].commit()</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write config and data into a training subfolder.</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    time_string <span class="op">=</span> datetime.now().strftime(<span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">-%H-%M-%S"</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    run_name <span class="op">=</span> (</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"axo-</span><span class="sc">{</span>time_string<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>secrets<span class="sc">.</span>token_hex(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> run_to_resume</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span> run_to_resume</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    run_folder <span class="op">=</span> <span class="ss">f"/runs/</span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    os.makedirs(run_folder, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Preparing training run in </span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> (</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/config.yml"</span>, <span class="st">"w"</span>) <span class="im">as</span> config_file,</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>config[<span class="st">'datasets'</span>][<span class="dv">0</span>][<span class="st">'path'</span>]<span class="sc">}</span><span class="ss">"</span>, <span class="st">"w"</span>) <span class="im">as</span> data_file,</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        config_file.write(config_raw)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>        data_file.write(data_raw)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>    VOLUME_CONFIG[<span class="st">"/runs"</span>].commit()</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> preproc_only:</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Spawning container for data preprocessing."</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        launch_handle <span class="op">=</span> preproc_data.spawn(run_folder)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Spawning container for data preprocessing."</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        preproc_handle <span class="op">=</span> preproc_data.spawn(run_folder)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/logs.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>            lbl <span class="op">=</span> <span class="st">"preproc"</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>            f.write(<span class="ss">f"</span><span class="sc">{</span>lbl<span class="sc">}</span><span class="ss">: https://modal.com/logs/call/</span><span class="sc">{</span>preproc_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># wait for preprocessing to finish.</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        preproc_handle.get()</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Start training run.</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Spawning container for training."</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        launch_handle <span class="op">=</span> train.spawn(run_folder, config[<span class="st">"output_dir"</span>])</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/logs.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>        lbl <span class="op">=</span> <span class="st">"train"</span> <span class="cf">if</span> <span class="kw">not</span> preproc_only <span class="cf">else</span> <span class="st">"preproc"</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>        f.write(<span class="ss">f"</span><span class="sc">{</span>lbl<span class="sc">}</span><span class="ss">: https://modal.com/logs/call/</span><span class="sc">{</span>launch_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    VOLUME_CONFIG[<span class="st">"/runs"</span>].commit()</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> run_name, launch_handle</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So last, letâ€™s examine the <code>train</code> function:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>train.py</strong></pre>
</div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="at">@app.function</span>(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>axolotl_image,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    gpu<span class="op">=</span>GPU_CONFIG,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    volumes<span class="op">=</span>VOLUME_CONFIG,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    timeout<span class="op">=</span><span class="dv">24</span> <span class="op">*</span> HOURS,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    _allow_background_volume_commits<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(run_folder: <span class="bu">str</span>, output_dir: <span class="bu">str</span>):</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Starting training run in </span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>device_count()<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_name()<span class="sc">}</span><span class="ss"> GPU(s)."</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    ALLOW_WANDB <span class="op">=</span> os.environ.get(<span class="st">"ALLOW_WANDB"</span>, <span class="st">"false"</span>).lower() <span class="op">==</span> <span class="st">"true"</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    cmd <span class="op">=</span> <span class="ss">f"accelerate launch -m axolotl.cli.train ./config.yml </span><span class="sc">{</span><span class="st">'--wandb_mode disabled'</span> <span class="cf">if</span> <span class="kw">not</span> ALLOW_WANDB <span class="cf">else</span> <span class="st">''</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    run_cmd(cmd, run_folder)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Kick off CPU job to merge the LoRA weights into base model.</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    merge_handle <span class="op">=</span> merge.spawn(run_folder, output_dir)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/logs.txt"</span>, <span class="st">"a"</span>) <span class="im">as</span> f:</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        f.write(<span class="ss">f"&lt;br&gt;merge: https://modal.com/logs/call/</span><span class="sc">{</span>merge_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Beginning merge </span><span class="sc">{</span>merge_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> merge_handle</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>From this, we can see that weâ€™re largely running <code>accelerate launch -m axolotl.cli.train</code> but using <code>run_cmd</code>, which run the command inside a folder with Modal Volume reloading before and commiting on success.</p>
</section>
<section id="config" class="level4" data-number="14.2.1.3">
<h4 data-number="14.2.1.3" class="anchored" data-anchor-id="config"><span class="header-section-number">14.2.1.3</span> config</h4>
<p>Letâ€™s now do a deep dive on the config file as itâ€™s a critical input. You may also find <a href="https://openaccess-ai-collective.github.io/axolotl/docs/config.html">this config</a> doc to be helpful in explaining the configuraiton options.</p>
<p>First, letâ€™s start at the top:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pythia.yml</strong></pre>
</div>
<div class="sourceCode" id="cb12"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lightweight example of training a small Pythia model for simple demonstrations</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">base_model</span><span class="kw">:</span><span class="at"> EleutherAI/pythia-1.4b-deduped</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_8bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span><span class="co"> # pythia is small, so keep it in 16-bit precision</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">strict</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we specify the <code>base_model</code>, which we decided to start with Pythia model for demo purposes. We also decided not to quantize our model to 8bit since Pythia is so small; therefore, this is why we ran LoRA, not QLoRA.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pythia.yml</strong></pre>
</div>
<div class="sourceCode" id="cb13"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">datasets</span><span class="kw">:</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">  # This will be the path used for the data when it is saved to the Volume in the cloud.</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> data.jsonl</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ds_type</span><span class="kw">:</span><span class="at"> json</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">      # JSONL file contains question, context, answer fields per line.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">      # This gets mapped to instruction, input, output axolotl tags.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_instruction</span><span class="kw">:</span><span class="at"> question</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_input</span><span class="kw">:</span><span class="at"> context</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_output</span><span class="kw">:</span><span class="at"> answer</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">      # Format is used by axolotl to generate the prompt.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="fu">      format</span><span class="kw">: </span><span class="ch">|-</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        [INST] Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        {input}</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        {instruction} [/INST]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The next part shows the dataset configuration. In this case, we set the <code>path</code> and <code>ds_type</code> for the data when it is saved to the cloud Volume.</p>
<p>Next, we specify the mapping of the key values in our <code>.jsonl</code> file to the <code>instruction</code>, <code>input</code>, and <code>output</code>. This generally follows the <a href="https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/inst_tune.html#alpaca"><code>alpaca</code></a> standard where our prompt is expecting those three formats.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How to add custom prompt format
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Per the <a href="https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/inst_tune.html#how-to-add-custom-prompt-format">docs</a>, for a dataset that is preprocessed for instruction purposes:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>data.jsonl</strong></pre>
</div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>{<span class="st">"input"</span>: <span class="st">"..."</span>, <span class="st">"output"</span>: <span class="st">"..."</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can use this example in your YAML config:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>config.yaml</strong></pre>
</div>
<div class="sourceCode" id="cb15"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">datasets</span><span class="kw">:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> repo</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">system_prompt</span><span class="kw">:</span><span class="at"> </span><span class="st">""</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_system</span><span class="kw">:</span><span class="at"> system</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_instruction</span><span class="kw">:</span><span class="at"> input</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_output</span><span class="kw">:</span><span class="at"> output</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">format</span><span class="kw">:</span><span class="at"> </span><span class="st">"[INST] {instruction} [/INST]"</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">no_input_format</span><span class="kw">:</span><span class="at"> </span><span class="st">"[INST] {instruction} [/INST]"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>See the <a href="https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/inst_tune.html">docs</a> for other data format options</p>
</div>
</div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pythia.yml</strong></pre>
</div>
<div class="sourceCode" id="cb16"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add tokens</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tokens</span><span class="kw">:</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">"[INST]"</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">" [/INST]"</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">"[SQL]"</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">" [/SQL]"</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dataset_prepared_path</span><span class="kw">:</span><span class="at"> preprocessed</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="fu">val_set_size</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="fu">output_dir</span><span class="kw">:</span><span class="at"> ./lora-out</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co"># max length of an input to train with, typically less than 2048 as most have context limit of 2048</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="fu">sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="dv">4096</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co"># use efficient multi-packing with block diagonal attention and per sequence position_ids.</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="fu">sample_packing</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_sample_packing</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad inputs so each step uses constant sized buffers</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co"># This will reduce memory fragmentation and may prevent OOMs, by re-using memory more efficiently</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="fu">pad_to_sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This part provides special tokens, paths, and some settings for handling padding and maximium training sequence length.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pythia.yml</strong></pre>
</div>
<div class="sourceCode" id="cb17"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">adapter</span><span class="kw">:</span><span class="at"> lora</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_model_dir</span><span class="kw">:</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_r</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_alpha</span><span class="kw">:</span><span class="at"> </span><span class="dv">32</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_dropout</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_linear</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_fan_in_fan_out</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co"> # required for pythia/GPTNeoX lora</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_modules</span><span class="kw">:</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> query_key_value</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_modules_to_save</span><span class="kw">:</span><span class="co"> # required when adding new tokens to pythia</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> embed_in</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> embed_out</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_accumulation_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="fu">micro_batch_size</span><span class="kw">:</span><span class="at"> </span><span class="dv">32</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="fu">num_epochs</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="fu">optimizer</span><span class="kw">:</span><span class="at"> adamw_torch</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lr_scheduler</span><span class="kw">:</span><span class="at"> cosine</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="fu">learning_rate</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0001</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="fu">bf16</span><span class="kw">:</span><span class="at"> auto</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="fu">fp16</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="fu">tf32</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_checkpointing</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="fu">early_stopping_patience</span><span class="kw">:</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="fu">local_rank</span><span class="kw">:</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="fu">logging_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="fu">warmup_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="fu">weight_decay</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This last part is where weâ€™re customizing our LoRA model. For this, Iâ€™m not going to go into detail but I encourage the curious reader to read on Sebastian Raschkaâ€™s wonderful <a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">Practical Tips for Finetuning LLMs</a>. It explains a lot of the intuition on LoRA and tips from LoRA training experiments.</p>
</section>
</section>
</section>
<section id="warnings-and-errors" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="warnings-and-errors"><span class="header-section-number">14.3</span> Warnings and Errors</h2>
<p>I found several warnings in the logs, so letâ€™s review them.</p>
<p>First, letâ€™s note the versions of dependencies:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">****************************************</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">****</span> Axolotl Dependency Versions <span class="pp">*****</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">accelerate:</span> 0.30.1         </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="ex">peft:</span> 0.10.0         </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="ex">transformers:</span> 4.40.2         </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>         <span class="ex">trl:</span> 0.8.5          </span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>       <span class="ex">torch:</span> 2.2.2+cu121    </span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="ex">bitsandbytes:</span> 0.43.1         </span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="ex">****************************************</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This isnâ€™t surprising as we specified this in our <code>common.py</code> script, but Iâ€™m noting just for documentation sake.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> following values were not passed to <span class="kw">`</span><span class="ex">accelerate</span> launch<span class="kw">`</span> and had defaults used instead:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--num_processes</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="ex">2</span><span class="kw">`</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                <span class="ex">More</span> than one GPU was found, enabling multi-GPU training.</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                <span class="ex">If</span> this was unintended please pass in <span class="kw">`</span><span class="ex">--num_processes=1</span><span class="kw">`</span>.</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--num_machines</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="ex">1</span><span class="kw">`</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--mixed_precision</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="st">'no'</span><span class="kw">`</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--dynamo_backend</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="st">'no'</span><span class="kw">`</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> avoid this warning pass in values for each of the problematic parameters or run <span class="kw">`</span><span class="ex">accelerate</span> config<span class="kw">`</span>.</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="ex">WARNING:</span> BNB_CUDA_VERSION=121 environment variable detected<span class="kw">;</span> <span class="ex">loading</span> libbitsandbytes_cuda121.so.</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="ex">This</span> can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="ex">If</span> this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="ex">If</span> you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="ex">For</span> example by adding the following to your .bashrc: export LD_LIBRARY_PATH=<span class="va">$LD_LIBRARY_PATH</span>:<span class="op">&lt;</span>path_to_cuda_dir/lib64</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For these warnings, theyâ€™re just that: warnings of defaults being used but not deterimental (IMHO) to training.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:04:15,288] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:203:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to cuda <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="ex">df:</span> /root/.triton/autotune: No such file or directory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This warning seems not to find a Triton file; this doesnâ€™t seem to be a problem but worth noting.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  Please specify the CUTLASS repo directory as environment variable <span class="va">$CUTLASS_PATH</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  sparse_attn requires a torch version <span class="op">&gt;</span>= 1.5 and <span class="op">&lt;</span> 2.0 but detected 2.2</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  using untested triton version <span class="er">(</span><span class="ex">2.2.0</span><span class="kw">)</span><span class="ex">,</span> only 1.0.0 is known to be compatible</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For this warning, this seems to mention that <code>sparse_attn</code> uses an older version of <code>torch</code> that what weâ€™re running. This <a href="https://github.com/microsoft/DeepSpeed/issues/3117">seems</a> to be a bit of a known problem so weâ€™ll ignore this for now.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:04:20,191] <span class="pp">[</span><span class="ss">WARNING</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.utils.config.models.input.hint_lora_8bit:973</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:28</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> We recommend setting <span class="kw">`</span><span class="ex">load_in_8bit:</span> true<span class="kw">`</span> for LORA finetuning</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is expected. Back in the <code>pythia.yml</code>, thereâ€™s this comment:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_8bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span><span class="co"> # pythia is small, so keep it in 16-bit precision</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:04:26,870] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_tokenizer:294</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:29</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:1</span><span class="pp">]</span> No Chat template selected. Consider adding a chat template for easier inference.</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Special</span> tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This <a href="https://github.com/OpenAccess-AI-Collective/axolotl/blob/3f1f5e33120b75f83736d75eb3bea0a6dad5424c/src/axolotl/utils/models.py#L295">seems</a> like a recommended best practice. Iâ€™ll try to remember this for next time.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:06:10,805] <span class="pp">[</span><span class="ss">WARNING</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_model:712</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:28</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> increasing model.config.max_position_embeddings from 2048 to 4096</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1050: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.
  warnings.warn(</code></pre>
<p>These two warnings are a bit a little surprising as we saw in the <code>pythia.yml</code>, e.g., <code>lora_fan_in_fan_out: true # required for pythia/GPTNeoX lora</code> and <code>sequence_len: 4096</code>. This is worth more investigation and awareness.</p>
</section>
<section id="train-lora-on-llama-3-8b" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="train-lora-on-llama-3-8b"><span class="header-section-number">14.4</span> Train LoRA on LLaMA 3 8B</h2>
<p>Now that weâ€™ve explored the base code a bit better, letâ€™s see if we can rerun a LoRA training, but this time with weâ€™ll make a few changes to <a href="https://github.com/modal-labs/llm-finetuning/blob/c94266459911b9473ee4c4d9c4d37d7946224fd9/config/llama-3.yml"><code>llama-3.yml</code></a>:</p>
<ol type="1">
<li>Weâ€™ll use <code>NousResearch/Meta-Llama-3-8B</code>. This is a larger model and more state-of-the-art.</li>
<li>Weâ€™ll use the larger <code>sqlqa.jsonl</code> dataset, which is 4,000 records instead of 64 from <code>sqlqa_subsample.jsonl</code></li>
<li>Change the Modal app name in <code>common.py</code> to <code>APP_NAME = "sqlqa-llama-3-8b"</code></li>
<li>Change in the config to <code>deepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json</code> and use <code>GPU_CONFIG=a100-80gb:4</code></li>
</ol>
<div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> GPU_CONFIG=a100-80gb:4 modal run <span class="at">--detach</span> src.train <span class="at">--config</span><span class="op">=</span>config/llama-3.yml <span class="at">--data</span><span class="op">=</span>data/sqlqa.jsonl</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 20:12:56,242] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.scripts.do_merge_lora:153</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:27</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> saving merged model to: lora-out/merged</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Unloading</span> and merging model: 100%<span class="kw">|</span><span class="ex">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span><span class="kw">|</span> <span class="ex">684/684</span> [00:00<span class="op">&lt;</span>00:00, 3664.51it/s]</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Unloading</span> and merging model: 100%<span class="kw">|</span><span class="ex">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span><span class="kw">|</span> <span class="ex">684/684</span> [00:00<span class="op">&lt;</span>00:00, 3580.76it/s]</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Run</span> complete. Tag: axo-2024-06-19-19-47-41-2f6c</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> inspect outputs, run <span class="kw">`</span><span class="ex">modal</span> volume ls example-runs-vol axo-2024-06-19-19-47-41-2f6c<span class="kw">`</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> run sample inference, run <span class="kw">`</span><span class="ex">modal</span> run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-19-47-41-2f6c<span class="kw">`</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="ex">Runner</span> terminated.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So weâ€™ve now trained our LoRA adapter!</p>
<p>We can view some of the logs in Modalâ€™s UI:</p>
<p><img src="images/13_finetuning/lora-llama3.png" class="img-fluid"></p>
<p>Nice! We can see all four A100â€™s are working, with the peak GPU RAM about 180GB. For four epochs, this took about 20 minutes, which isnâ€™t too bad.</p>
<p>Letâ€™s now test this by running inference in CLI:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-19-47-41-2f6c <span class="at">--prompt</span> <span class="st">"[INST] Using the schema context below, generate a SQL query that answers the question.</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="st">        CREATE TABLE Has_allergy (Allergy VARCHAR)</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="st">        How many students have cat allergies? [/INST]"</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Querying model axo-2024-06-19-19-47-41-2f6c</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Initializing vLLM engine for model at /runs/axo-2024-06-19-19-47-41-2f6c/lora-out/merged</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="ex">2024-06-19</span> 20:18:11,543 INFO worker.py:1753 <span class="at">--</span> Started a local Ray instance.</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 20:19:38 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 20:19:39 async_llm_engine.py:111] Finished request b99f9a9858e346009969a9f41764c9d2.</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Effective throughput of 30.91 tok/s</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ‘¤:</span> <span class="pp">[</span><span class="ss">INST</span><span class="pp">]</span> Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        <span class="ex">CREATE</span> TABLE Has_allergy <span class="er">(</span><span class="ex">Allergy</span> VARCHAR<span class="kw">)</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        <span class="ex">How</span> many students have cat allergies<span class="pp">?</span> <span class="pp">[</span><span class="ss">/INST</span><span class="pp">]</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ¤–:</span> <span class="pp">[</span><span class="ss">SQL</span><span class="pp">]</span> SELECT COUNT<span class="er">(</span><span class="ex">*</span><span class="kw">)</span> <span class="ex">FROM</span> Has_allergy WHERE Allergy = <span class="st">"Cat"</span> <span class="pp">[</span><span class="ss">/SQL</span><span class="pp">]</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a><span class="ex">Stopping</span> app <span class="at">-</span> local entrypoint completed.</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">[W</span> CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. <span class="er">(</span><span class="kw">function</span><span class="fu"> operator()</span><span class="kw">)</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">INFO</span> 06-19 20:19:38 model_runner.py:437] Graph capturing finished in 34 secs.</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="ex">Runner</span> terminated.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nice! We have a much better result of what weâ€™re looking for.</p>
<p>One thing I noticed after the fact was I forgot to setup a Weights and Bias project â€“ make sure to set <code>wandb_project: sqlqa-llama3-lora</code> in the config file. Thatâ€™s okay - weâ€™ll make sure to do this in our next chapter when we run a custom fine tuning project.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./12_datasets.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Datasets</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb29" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Finetuning"</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "llm-finetuning GH repo"</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ryan Wesslen"</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co">    html-math-method: katex</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co">  eval: false</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>Unlike the previous chapters, this chapter uses a separate GitHub repo: <span class="co">[</span><span class="ot">github.com/modal-labs/llm-finetuning</span><span class="co">](https://github.com/modal-labs/llm-finetuning)</span>. This example provides a SQL fine tuning example where the goal is to ask questions along with a SQL schema template and the model outputs a SQL query.</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>For this, we'll do a few passes. First this chapter, we'll run the current code and explaining parts. But in the next Chapter, we'll start a new extension of this for a custom problem.</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setup</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>We'll first need to setup our secrets.</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Install <span class="in">`modal`</span> in your current Python virtual environment (<span class="in">`pip install modal`</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Set up a Modal token in your environment (<span class="in">`python -m modal setup`</span>)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>You need to have a secret named <span class="in">`huggingface`</span> in your workspace. You can create a new secret with the HuggingFace template in your Modal dashboard, using the key from HuggingFace (in settings under API tokens) to populate HF_TOKEN and changing the name from <span class="in">`my-huggingface-secret`</span> to <span class="in">`huggingface`</span>.</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>For some LLaMA models, you need to go to the Hugging Face page (e.g. this page for LLaMA 3 8B_ and agree to their Terms and Conditions for access (granted instantly).</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>If you want to use Weights &amp; Biases for logging, you need to have a secret named <span class="in">`wandb`</span> in your workspace as well. You can also create it from a template. Training is hard enough without good logs, so we recommend you try it or look into axolotl's integration with MLFlow!</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reproduce with Pythia 1.4B</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>We're now ready to run! Let's use the Pythia 1.4B model (<span class="co">[</span><span class="ot">`EleutherAI/pythia-1.4b-deduped`</span><span class="co">](https://huggingface.co/EleutherAI/pythia-1.4b-deduped)</span>) on the SQLQA subsample <span class="in">`sqlqa.subsample.jsonl`</span>.</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>As described in its <span class="co">[</span><span class="ot">HF model card</span><span class="co">](https://huggingface.co/EleutherAI/pythia-1.4b-deduped#out-of-scope-use)</span>, Pythia 1.4B and its suite of models is intended for research purposes only and **not** for deployment.</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> export ALLOW_WANDB=true  <span class="co"># if you're using Weights &amp; Biases</span></span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal run <span class="at">--detach</span> src.train <span class="at">--config</span><span class="op">=</span>config/pythia.yml <span class="at">--data</span><span class="op">=</span>data/sqlqa.subsample.jsonl</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:13,670] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_model:794</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> converting modules to torch.bfloat16 for flash attention</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:13,840] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_lora:951</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> found linear modules: [<span class="st">'dense_h_to_4h'</span>, <span class="st">'dense'</span>, <span class="st">'query_key_value'</span>, <span class="st">'dense_4h_to_h'</span>]</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:13,840] <span class="pp">[</span><span class="ss">DEBUG</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_lora:993</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> Loading pretrained PEFT <span class="at">-</span> LoRA</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a><span class="ex">trainable</span> params: 218,628,096 <span class="kw">||</span> <span class="ex">all</span> params: 1,633,275,904 <span class="kw">||</span> <span class="ex">trainable%:</span> 13.385864290568755</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:17,148] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_model:843</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> GPU memory usage after adapters: 0.000GB <span class="er">(</span><span class="kw">)</span></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:17,148] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.scripts.do_merge_lora:144</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> running merge of LoRA with base model</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a><span class="ex">Unloading</span> and merging model: 100%<span class="kw">|</span><span class="ex">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span><span class="kw">|</span> <span class="ex">445/445</span> [00:03<span class="op">&lt;</span>00:00, 114.95it/s]</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:07:21,028] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.scripts.do_merge_lora:153</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:25</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> saving merged model to: lora-out/merged</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a><span class="ex">Run</span> complete. Tag: axo-2024-06-19-16-03-18-54e2</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> inspect outputs, run <span class="kw">`</span><span class="ex">modal</span> volume ls example-runs-vol axo-2024-06-19-16-03-18-54e2<span class="kw">`</span></span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> run sample inference, run <span class="kw">`</span><span class="ex">modal</span> run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-16-03-18-54e2<span class="kw">`</span></span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a><span class="ex">âœ“</span> App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-uD7AwcuHYH0lrU9fDIw4Nz</span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a>I've skipped a lot of the logs, but there's a lot of helpful information. For example, we can see that LoRA trained about 13% of the model's 1.6BN parameters (218MM). We can also see where the LoRA model was saved out and its respective Tag.</span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a>We can view the volumes here:</span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal volume ls example-runs-vol axo-2024-06-19-16-03-18-54e2</span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a>      <span class="ex">Directory</span> listing of <span class="st">'axo-2024-06-19-16-03-18-54e2'</span> in <span class="st">'example-runs-vol'</span>       </span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a><span class="ex">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“</span></span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a><span class="ex">â”ƒ</span> Filename                                  â”ƒ Type â”ƒ Created/Modified     â”ƒ Size     â”ƒ</span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a><span class="ex">â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©</span></span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/preprocessed â”‚ dir  â”‚ 2024-06-19 12:03 EDT â”‚ 32 B     â”‚</span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/lora-out     â”‚ dir  â”‚ 2024-06-19 12:07 EDT â”‚ 136 B    â”‚</span>
<span id="cb29-71"><a href="#cb29-71" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/logs.txt     â”‚ file â”‚ 2024-06-19 12:06 EDT â”‚ 133 B    â”‚</span>
<span id="cb29-72"><a href="#cb29-72" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/data.jsonl   â”‚ file â”‚ 2024-06-19 12:03 EDT â”‚ 20.5 KiB â”‚</span>
<span id="cb29-73"><a href="#cb29-73" aria-hidden="true" tabindex="-1"></a><span class="ex">â”‚</span> axo-2024-06-19-16-03-18-54e2/config.yml   â”‚ file â”‚ 2024-06-19 12:03 EDT â”‚ 1.6 KiB  â”‚</span>
<span id="cb29-74"><a href="#cb29-74" aria-hidden="true" tabindex="-1"></a><span class="ex">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span id="cb29-75"><a href="#cb29-75" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-76"><a href="#cb29-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-77"><a href="#cb29-77" aria-hidden="true" tabindex="-1"></a>We can also run inference like this:</span>
<span id="cb29-78"><a href="#cb29-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-79"><a href="#cb29-79" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-80"><a href="#cb29-80" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-16-03-18-54e2 <span class="at">--prompt</span> <span class="st">"[INST] Using the schema context below, generate a SQL query that answers the question.</span></span>
<span id="cb29-81"><a href="#cb29-81" aria-hidden="true" tabindex="-1"></a><span class="st">        CREATE TABLE Has_allergy (Allergy VARCHAR)</span></span>
<span id="cb29-82"><a href="#cb29-82" aria-hidden="true" tabindex="-1"></a><span class="st">        How many students have cat allergies? [/INST]"</span></span>
<span id="cb29-83"><a href="#cb29-83" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Querying model axo-2024-06-19-16-03-18-54e2</span>
<span id="cb29-84"><a href="#cb29-84" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Initializing vLLM engine for model at /runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged</span>
<span id="cb29-85"><a href="#cb29-85" aria-hidden="true" tabindex="-1"></a><span class="ex">2024-06-19</span> 16:34:04,753 INFO worker.py:1753 <span class="at">--</span> Started a local Ray instance.</span>
<span id="cb29-86"><a href="#cb29-86" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:34:07 llm_engine.py:73] Initializing an LLM engine with config: model=PosixPath<span class="er">(</span><span class="st">'/runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged'</span><span class="kw">)</span><span class="ex">,</span> tokenizer=PosixPath<span class="er">(</span><span class="st">'/runs/axo-2024-06-19-16-03-18-54e2/lora-out/merged'</span><span class="kw">)</span><span class="ex">,</span> tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0<span class="er">)</span></span>
<span id="cb29-87"><a href="#cb29-87" aria-hidden="true" tabindex="-1"></a><span class="ex">Special</span> tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span>
<span id="cb29-88"><a href="#cb29-88" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:34:21 llm_engine.py:223] <span class="co"># GPU blocks: 12896, # CPU blocks: 2730</span></span>
<span id="cb29-89"><a href="#cb29-89" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:34:26 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set <span class="st">'enforce_eager=True'</span> or use <span class="st">'--enforce-eager'</span> in the CLI.</span>
<span id="cb29-90"><a href="#cb29-90" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">[W</span> CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. <span class="er">(</span><span class="kw">function</span><span class="fu"> operator()</span><span class="kw">)</span></span>
<span id="cb29-91"><a href="#cb29-91" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:35:01 model_runner.py:437] Graph capturing finished in 35 secs.</span>
<span id="cb29-92"><a href="#cb29-92" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=310<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:34:26 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set <span class="st">'enforce_eager=True'</span> or use <span class="st">'--enforce-eager'</span> in the CLI.</span>
<span id="cb29-93"><a href="#cb29-93" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:01 async_llm_engine.py:379] Received request 5f8c384f2b7d41bf9553c8aae08c7fd9: prompt: <span class="st">'[INST] Using the schema context below, generate a SQL query that answers the question.\n        CREATE TABLE Has_allergy (Allergy VARCHAR)\n        How many students have cat allergies? [/INST]'</span>, sampling params: SamplingParams<span class="er">(</span><span class="va">n</span><span class="op">=</span>1, <span class="va">best_of</span><span class="op">=</span>1, <span class="va">presence_penalty</span><span class="op">=</span>0.0, <span class="va">frequency_penalty</span><span class="op">=</span>0.0, <span class="va">repetition_penalty</span><span class="op">=</span>1.1, <span class="va">temperature</span><span class="op">=</span>0.2, <span class="va">top_p</span><span class="op">=</span>0.95, <span class="va">top_k</span><span class="op">=</span>50, <span class="va">min_p</span><span class="op">=</span>0.0, <span class="va">use_beam_search</span><span class="op">=</span>False, <span class="va">length_penalty</span><span class="op">=</span>1.0, <span class="va">early_stopping</span><span class="op">=</span>False, <span class="va">stop</span><span class="op">=</span>[], <span class="va">stop_token_ids</span><span class="op">=</span>[], <span class="va">include_stop_str_in_output</span><span class="op">=</span>False, <span class="va">ignore_eos</span><span class="op">=</span>False, <span class="va">max_tokens</span><span class="op">=</span>1024, <span class="va">logprobs</span><span class="op">=</span>None, <span class="va">prompt_logprobs</span><span class="op">=</span>None, <span class="va">skip_special_tokens</span><span class="op">=</span>True, <span class="va">spaces_between_special_tokens</span><span class="op">=</span>True<span class="kw">)</span><span class="ex">,</span> prompt token ids: None.</span>
<span id="cb29-94"><a href="#cb29-94" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:01 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%</span>
<span id="cb29-95"><a href="#cb29-95" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:06 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%</span>
<span id="cb29-96"><a href="#cb29-96" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:11 async_llm_engine.py:111] Finished request 5f8c384f2b7d41bf9553c8aae08c7fd9.</span>
<span id="cb29-97"><a href="#cb29-97" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Effective throughput of 107.84 tok/s</span>
<span id="cb29-98"><a href="#cb29-98" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ‘¤:</span> <span class="pp">[</span><span class="ss">INST</span><span class="pp">]</span> Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb29-99"><a href="#cb29-99" aria-hidden="true" tabindex="-1"></a>        <span class="ex">CREATE</span> TABLE Has_allergy <span class="er">(</span><span class="ex">Allergy</span> VARCHAR<span class="kw">)</span></span>
<span id="cb29-100"><a href="#cb29-100" aria-hidden="true" tabindex="-1"></a>        <span class="ex">How</span> many students have cat allergies<span class="pp">?</span> <span class="pp">[</span><span class="ss">/INST</span><span class="pp">]</span></span>
<span id="cb29-101"><a href="#cb29-101" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ¤–:</span> <span class="er">(</span><span class="ex">1</span><span class="kw">)</span></span>
<span id="cb29-102"><a href="#cb29-102" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'cat'</span><span class="kw">)</span></span>
<span id="cb29-103"><a href="#cb29-103" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'dog'</span><span class="kw">)</span></span>
<span id="cb29-104"><a href="#cb29-104" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'bird'</span><span class="kw">)</span></span>
<span id="cb29-105"><a href="#cb29-105" aria-hidden="true" tabindex="-1"></a>        <span class="ex">...</span></span>
<span id="cb29-106"><a href="#cb29-106" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'hamster'</span><span class="kw">)</span></span>
<span id="cb29-107"><a href="#cb29-107" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_allergy VALUES <span class="er">(</span><span class="st">'mouse'</span><span class="kw">)</span></span>
<span id="cb29-108"><a href="#cb29-108" aria-hidden="true" tabindex="-1"></a>        <span class="ex">INSERT</span> INTO Has_</span>
<span id="cb29-109"><a href="#cb29-109" aria-hidden="true" tabindex="-1"></a><span class="ex">Stopping</span> app <span class="at">-</span> local entrypoint completed.</span>
<span id="cb29-110"><a href="#cb29-110" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=310<span class="kw">)</span> <span class="ex">INFO</span> 06-19 16:35:01 model_runner.py:437] Graph capturing finished in 35 secs.</span>
<span id="cb29-111"><a href="#cb29-111" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=310<span class="kw">)</span> <span class="ex">[W</span> CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. <span class="er">(</span><span class="kw">function</span><span class="fu"> operator()</span><span class="kw">)</span></span>
<span id="cb29-112"><a href="#cb29-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-113"><a href="#cb29-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-114"><a href="#cb29-114" aria-hidden="true" tabindex="-1"></a>So this took about a minute, but most of that is due to overhead, which isn't bad. However, it's worth noting that our model didn't go great; this is expected, as we trained LoRA on just a subset with a very small model. </span>
<span id="cb29-115"><a href="#cb29-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-116"><a href="#cb29-116" aria-hidden="true" tabindex="-1"></a>A few more points to note. First, per the <span class="in">`config.yml`</span>, we've specified the instruction prompt template like:</span>
<span id="cb29-117"><a href="#cb29-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-118"><a href="#cb29-118" aria-hidden="true" tabindex="-1"></a><span class="in">```yaml</span></span>
<span id="cb29-119"><a href="#cb29-119" aria-hidden="true" tabindex="-1"></a><span class="fu">      format</span><span class="kw">: </span><span class="ch">|-</span></span>
<span id="cb29-120"><a href="#cb29-120" aria-hidden="true" tabindex="-1"></a>        [INST] Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb29-121"><a href="#cb29-121" aria-hidden="true" tabindex="-1"></a>        {input}</span>
<span id="cb29-122"><a href="#cb29-122" aria-hidden="true" tabindex="-1"></a>        {instruction} [/INST]</span>
<span id="cb29-123"><a href="#cb29-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-124"><a href="#cb29-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-125"><a href="#cb29-125" aria-hidden="true" tabindex="-1"></a>So we passed an example that met this template.</span>
<span id="cb29-126"><a href="#cb29-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-127"><a href="#cb29-127" aria-hidden="true" tabindex="-1"></a>Another nice output is we can view the estimated throughput like:</span>
<span id="cb29-128"><a href="#cb29-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-129"><a href="#cb29-129" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-130"><a href="#cb29-130" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:06 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%</span>
<span id="cb29-131"><a href="#cb29-131" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 16:35:11 async_llm_engine.py:111] Finished request 5f8c384f2b7d41bf9553c8aae08c7fd9.</span>
<span id="cb29-132"><a href="#cb29-132" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Effective throughput of 107.84 tok/s</span>
<span id="cb29-133"><a href="#cb29-133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-134"><a href="#cb29-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-135"><a href="#cb29-135" aria-hidden="true" tabindex="-1"></a>This is one of the benefits of using vLLM and definitely something we want to keep an eye on down the road when we're tracking our model in production.</span>
<span id="cb29-136"><a href="#cb29-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-137"><a href="#cb29-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### Code review</span></span>
<span id="cb29-138"><a href="#cb29-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-139"><a href="#cb29-139" aria-hidden="true" tabindex="-1"></a>Let's now explore the code we just ran to better understand it.</span>
<span id="cb29-140"><a href="#cb29-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-141"><a href="#cb29-141" aria-hidden="true" tabindex="-1"></a>What we're most interested in are the scripts in <span class="in">`/src/`</span>, in particular the <span class="in">`train.py`</span> file.</span>
<span id="cb29-142"><a href="#cb29-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-143"><a href="#cb29-143" aria-hidden="true" tabindex="-1"></a>First, let's look at the dependencies:</span>
<span id="cb29-144"><a href="#cb29-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-145"><a href="#cb29-145" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="train.py"}</span></span>
<span id="cb29-146"><a href="#cb29-146" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb29-147"><a href="#cb29-147" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb29-148"><a href="#cb29-148" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb29-149"><a href="#cb29-149" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> secrets</span>
<span id="cb29-150"><a href="#cb29-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-151"><a href="#cb29-151" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> .common <span class="im">import</span> (</span>
<span id="cb29-152"><a href="#cb29-152" aria-hidden="true" tabindex="-1"></a>    app,</span>
<span id="cb29-153"><a href="#cb29-153" aria-hidden="true" tabindex="-1"></a>    axolotl_image,</span>
<span id="cb29-154"><a href="#cb29-154" aria-hidden="true" tabindex="-1"></a>    HOURS,</span>
<span id="cb29-155"><a href="#cb29-155" aria-hidden="true" tabindex="-1"></a>    MINUTES,</span>
<span id="cb29-156"><a href="#cb29-156" aria-hidden="true" tabindex="-1"></a>    VOLUME_CONFIG,</span>
<span id="cb29-157"><a href="#cb29-157" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-158"><a href="#cb29-158" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-159"><a href="#cb29-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-160"><a href="#cb29-160" aria-hidden="true" tabindex="-1"></a><span class="fu">#### common.py</span></span>
<span id="cb29-161"><a href="#cb29-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-162"><a href="#cb29-162" aria-hidden="true" tabindex="-1"></a>Overall, this is very neat - we don't have many local dependencies. The one exception is the <span class="in">`common.py`</span> file, where we are loading common configurations to ensure consistency with both training and inference.</span>
<span id="cb29-163"><a href="#cb29-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-164"><a href="#cb29-164" aria-hidden="true" tabindex="-1"></a>There are a few things we do in the <span class="in">`common.py`</span>:</span>
<span id="cb29-165"><a href="#cb29-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-166"><a href="#cb29-166" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Set our <span class="in">`APP_NAME`</span> here.</span>
<span id="cb29-167"><a href="#cb29-167" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Set fixed <span class="in">`MINUTES`</span> and <span class="in">`HOURS`</span>, which are passed as timeout values</span>
<span id="cb29-168"><a href="#cb29-168" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Set the <span class="in">`Axolotl`</span> <span class="co">[</span><span class="ot">image</span><span class="co">](https://hub.docker.com/layers/winglian/axolotl-cloud/main-20240522-py3.11-cu121-2.2.2/images/sha256-e6c2d1f38083869baa71ebf2f3b2a82678acc980534adb525c78582723f2c9a3)</span> by its hash (only used in training)</span>
<span id="cb29-169"><a href="#cb29-169" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Set Python dependencies using <span class="in">`.pip_install()`</span></span>
<span id="cb29-170"><a href="#cb29-170" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Pass environmental variables like <span class="in">`HUGGINGFACE_HUB_CACHE`</span></span>
<span id="cb29-171"><a href="#cb29-171" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Set the <span class="in">`vLLM`</span> image (only used in inference)</span>
<span id="cb29-172"><a href="#cb29-172" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Initialize the Modal App along with passing the secrets</span>
<span id="cb29-173"><a href="#cb29-173" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Configure the volumes where we'll save our logs and model</span>
<span id="cb29-174"><a href="#cb29-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-175"><a href="#cb29-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="common.py"}</span></span>
<span id="cb29-176"><a href="#cb29-176" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb29-177"><a href="#cb29-177" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> PurePosixPath</span>
<span id="cb29-178"><a href="#cb29-178" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Union</span>
<span id="cb29-179"><a href="#cb29-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-180"><a href="#cb29-180" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> modal</span>
<span id="cb29-181"><a href="#cb29-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-182"><a href="#cb29-182" aria-hidden="true" tabindex="-1"></a>APP_NAME <span class="op">=</span> <span class="st">"example-axolotl"</span></span>
<span id="cb29-183"><a href="#cb29-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-184"><a href="#cb29-184" aria-hidden="true" tabindex="-1"></a>MINUTES <span class="op">=</span> <span class="dv">60</span>  <span class="co"># seconds</span></span>
<span id="cb29-185"><a href="#cb29-185" aria-hidden="true" tabindex="-1"></a>HOURS <span class="op">=</span> <span class="dv">60</span> <span class="op">*</span> MINUTES</span>
<span id="cb29-186"><a href="#cb29-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-187"><a href="#cb29-187" aria-hidden="true" tabindex="-1"></a><span class="co"># Axolotl image hash corresponding to main-20240522-py3.11-cu121-2.2.2</span></span>
<span id="cb29-188"><a href="#cb29-188" aria-hidden="true" tabindex="-1"></a>AXOLOTL_REGISTRY_SHA <span class="op">=</span> (</span>
<span id="cb29-189"><a href="#cb29-189" aria-hidden="true" tabindex="-1"></a>    <span class="st">"8ec2116dd36ecb9fb23702278ac612f27c1d4309eca86ad0afd3a3fe4a80ad5b"</span></span>
<span id="cb29-190"><a href="#cb29-190" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-191"><a href="#cb29-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-192"><a href="#cb29-192" aria-hidden="true" tabindex="-1"></a>ALLOW_WANDB <span class="op">=</span> os.environ.get(<span class="st">"ALLOW_WANDB"</span>, <span class="st">"false"</span>).lower() <span class="op">==</span> <span class="st">"true"</span></span>
<span id="cb29-193"><a href="#cb29-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-194"><a href="#cb29-194" aria-hidden="true" tabindex="-1"></a>axolotl_image <span class="op">=</span> (</span>
<span id="cb29-195"><a href="#cb29-195" aria-hidden="true" tabindex="-1"></a>    modal.Image.from_registry(<span class="ss">f"winglian/axolotl@sha256:</span><span class="sc">{</span>AXOLOTL_REGISTRY_SHA<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-196"><a href="#cb29-196" aria-hidden="true" tabindex="-1"></a>    .pip_install(</span>
<span id="cb29-197"><a href="#cb29-197" aria-hidden="true" tabindex="-1"></a>        <span class="st">"huggingface_hub==0.20.3"</span>,</span>
<span id="cb29-198"><a href="#cb29-198" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hf-transfer==0.1.5"</span>,</span>
<span id="cb29-199"><a href="#cb29-199" aria-hidden="true" tabindex="-1"></a>        <span class="st">"wandb==0.16.3"</span>,</span>
<span id="cb29-200"><a href="#cb29-200" aria-hidden="true" tabindex="-1"></a>        <span class="st">"fastapi==0.110.0"</span>,</span>
<span id="cb29-201"><a href="#cb29-201" aria-hidden="true" tabindex="-1"></a>        <span class="st">"pydantic==2.6.3"</span>,</span>
<span id="cb29-202"><a href="#cb29-202" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb29-203"><a href="#cb29-203" aria-hidden="true" tabindex="-1"></a>    .env(</span>
<span id="cb29-204"><a href="#cb29-204" aria-hidden="true" tabindex="-1"></a>        <span class="bu">dict</span>(</span>
<span id="cb29-205"><a href="#cb29-205" aria-hidden="true" tabindex="-1"></a>            HUGGINGFACE_HUB_CACHE<span class="op">=</span><span class="st">"/pretrained"</span>,</span>
<span id="cb29-206"><a href="#cb29-206" aria-hidden="true" tabindex="-1"></a>            HF_HUB_ENABLE_HF_TRANSFER<span class="op">=</span><span class="st">"1"</span>,</span>
<span id="cb29-207"><a href="#cb29-207" aria-hidden="true" tabindex="-1"></a>            TQDM_DISABLE<span class="op">=</span><span class="st">"true"</span>,</span>
<span id="cb29-208"><a href="#cb29-208" aria-hidden="true" tabindex="-1"></a>            AXOLOTL_NCCL_TIMEOUT<span class="op">=</span><span class="st">"60"</span>,</span>
<span id="cb29-209"><a href="#cb29-209" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-210"><a href="#cb29-210" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb29-211"><a href="#cb29-211" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-212"><a href="#cb29-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-213"><a href="#cb29-213" aria-hidden="true" tabindex="-1"></a>vllm_image <span class="op">=</span> modal.Image.from_registry(</span>
<span id="cb29-214"><a href="#cb29-214" aria-hidden="true" tabindex="-1"></a>    <span class="st">"nvidia/cuda:12.1.0-base-ubuntu22.04"</span>, add_python<span class="op">=</span><span class="st">"3.10"</span></span>
<span id="cb29-215"><a href="#cb29-215" aria-hidden="true" tabindex="-1"></a>).pip_install(</span>
<span id="cb29-216"><a href="#cb29-216" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vllm==0.2.6"</span>,</span>
<span id="cb29-217"><a href="#cb29-217" aria-hidden="true" tabindex="-1"></a>    <span class="st">"torch==2.1.2"</span>,</span>
<span id="cb29-218"><a href="#cb29-218" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-219"><a href="#cb29-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-220"><a href="#cb29-220" aria-hidden="true" tabindex="-1"></a>app <span class="op">=</span> modal.App(</span>
<span id="cb29-221"><a href="#cb29-221" aria-hidden="true" tabindex="-1"></a>    APP_NAME,</span>
<span id="cb29-222"><a href="#cb29-222" aria-hidden="true" tabindex="-1"></a>    secrets<span class="op">=</span>[</span>
<span id="cb29-223"><a href="#cb29-223" aria-hidden="true" tabindex="-1"></a>        modal.Secret.from_name(<span class="st">"huggingface"</span>),</span>
<span id="cb29-224"><a href="#cb29-224" aria-hidden="true" tabindex="-1"></a>        modal.Secret.from_dict({<span class="st">"ALLOW_WANDB"</span>: os.environ.get(<span class="st">"ALLOW_WANDB"</span>, <span class="st">"false"</span>)}),</span>
<span id="cb29-225"><a href="#cb29-225" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>([modal.Secret.from_name(<span class="st">"wandb"</span>)] <span class="cf">if</span> ALLOW_WANDB <span class="cf">else</span> []),</span>
<span id="cb29-226"><a href="#cb29-226" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb29-227"><a href="#cb29-227" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-228"><a href="#cb29-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-229"><a href="#cb29-229" aria-hidden="true" tabindex="-1"></a><span class="co"># Volumes for pre-trained models and training runs.</span></span>
<span id="cb29-230"><a href="#cb29-230" aria-hidden="true" tabindex="-1"></a>pretrained_volume <span class="op">=</span> modal.Volume.from_name(</span>
<span id="cb29-231"><a href="#cb29-231" aria-hidden="true" tabindex="-1"></a>    <span class="st">"example-pretrained-vol"</span>, create_if_missing<span class="op">=</span><span class="va">True</span></span>
<span id="cb29-232"><a href="#cb29-232" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-233"><a href="#cb29-233" aria-hidden="true" tabindex="-1"></a>runs_volume <span class="op">=</span> modal.Volume.from_name(<span class="st">"example-runs-vol"</span>, create_if_missing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-234"><a href="#cb29-234" aria-hidden="true" tabindex="-1"></a>VOLUME_CONFIG: <span class="bu">dict</span>[Union[<span class="bu">str</span>, PurePosixPath], modal.Volume] <span class="op">=</span> {</span>
<span id="cb29-235"><a href="#cb29-235" aria-hidden="true" tabindex="-1"></a>    <span class="st">"/pretrained"</span>: pretrained_volume,</span>
<span id="cb29-236"><a href="#cb29-236" aria-hidden="true" tabindex="-1"></a>    <span class="st">"/runs"</span>: runs_volume,</span>
<span id="cb29-237"><a href="#cb29-237" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-238"><a href="#cb29-238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-239"><a href="#cb29-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-240"><a href="#cb29-240" aria-hidden="true" tabindex="-1"></a><span class="fu">#### train.py</span></span>
<span id="cb29-241"><a href="#cb29-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-242"><a href="#cb29-242" aria-hidden="true" tabindex="-1"></a>Let's now get back to the <span class="in">`train.py`</span> script (after loading dependencies). Before getting to the <span class="in">`main()`</span> function, at the top of the script we find:</span>
<span id="cb29-243"><a href="#cb29-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-244"><a href="#cb29-244" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="train.py"}</span></span>
<span id="cb29-245"><a href="#cb29-245" aria-hidden="true" tabindex="-1"></a>GPU_CONFIG <span class="op">=</span> os.environ.get(<span class="st">"GPU_CONFIG"</span>, <span class="st">"a100:2"</span>)</span>
<span id="cb29-246"><a href="#cb29-246" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(GPU_CONFIG.split(<span class="st">":"</span>)) <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
<span id="cb29-247"><a href="#cb29-247" aria-hidden="true" tabindex="-1"></a>    N_GPUS <span class="op">=</span> <span class="bu">int</span>(os.environ.get(<span class="st">"N_GPUS"</span>, <span class="dv">2</span>))</span>
<span id="cb29-248"><a href="#cb29-248" aria-hidden="true" tabindex="-1"></a>    GPU_CONFIG <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>GPU_CONFIG<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>N_GPUS<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb29-249"><a href="#cb29-249" aria-hidden="true" tabindex="-1"></a>SINGLE_GPU_CONFIG <span class="op">=</span> os.environ.get(<span class="st">"GPU_CONFIG"</span>, <span class="st">"a10g:1"</span>)</span>
<span id="cb29-250"><a href="#cb29-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-251"><a href="#cb29-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-252"><a href="#cb29-252" aria-hidden="true" tabindex="-1"></a>This code configures GPU settings by retrieving the <span class="in">`GPU_CONFIG`</span> environment variable, defaulting to "a100:2" if unset, and ensures it contains a specified GPU type and count format. If the format is incorrect (missing the count), it fetches a default count from <span class="in">`N_GPUS`</span> or sets it to 2, then updates <span class="in">`GPU_CONFIG`</span> accordingly.</span>
<span id="cb29-253"><a href="#cb29-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-254"><a href="#cb29-254" aria-hidden="true" tabindex="-1"></a>But let's now look at our <span class="in">`main()`</span>. This function takes five parameters:</span>
<span id="cb29-255"><a href="#cb29-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-256"><a href="#cb29-256" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="in">`config`</span>: this is our config file (e.g., <span class="in">`pythia.yml`</span>)</span>
<span id="cb29-257"><a href="#cb29-257" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="in">`data`</span>: this is our <span class="in">`.jsonl`</span>, which its format is specified in the config file</span>
<span id="cb29-258"><a href="#cb29-258" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="in">`merge_lora`</span> (optional): boolean whether to run <span class="in">`accelerate launch -m axolotl.cli.merge_lora`</span></span>
<span id="cb29-259"><a href="#cb29-259" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span><span class="in">`preproc_only`</span> (optional): boolean on whether to run preprocessing steps only</span>
<span id="cb29-260"><a href="#cb29-260" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span><span class="in">`run_to_resume`</span> (optional): A <span class="in">`str`</span> of a previous run to resume</span>
<span id="cb29-261"><a href="#cb29-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-262"><a href="#cb29-262" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="train.py"}</span></span>
<span id="cb29-263"><a href="#cb29-263" aria-hidden="true" tabindex="-1"></a><span class="at">@app.local_entrypoint</span>()</span>
<span id="cb29-264"><a href="#cb29-264" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(</span>
<span id="cb29-265"><a href="#cb29-265" aria-hidden="true" tabindex="-1"></a>    config: <span class="bu">str</span>,</span>
<span id="cb29-266"><a href="#cb29-266" aria-hidden="true" tabindex="-1"></a>    data: <span class="bu">str</span>,</span>
<span id="cb29-267"><a href="#cb29-267" aria-hidden="true" tabindex="-1"></a>    merge_lora: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb29-268"><a href="#cb29-268" aria-hidden="true" tabindex="-1"></a>    preproc_only: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb29-269"><a href="#cb29-269" aria-hidden="true" tabindex="-1"></a>    run_to_resume: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb29-270"><a href="#cb29-270" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb29-271"><a href="#cb29-271" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read config and data source files and pass their contents to the remote function.</span></span>
<span id="cb29-272"><a href="#cb29-272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(config, <span class="st">"r"</span>) <span class="im">as</span> cfg, <span class="bu">open</span>(data, <span class="st">"r"</span>) <span class="im">as</span> dat:</span>
<span id="cb29-273"><a href="#cb29-273" aria-hidden="true" tabindex="-1"></a>        run_name, launch_handle <span class="op">=</span> launch.remote(</span>
<span id="cb29-274"><a href="#cb29-274" aria-hidden="true" tabindex="-1"></a>            cfg.read(), dat.read(), run_to_resume, preproc_only</span>
<span id="cb29-275"><a href="#cb29-275" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-276"><a href="#cb29-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-277"><a href="#cb29-277" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write a local reference to the location on the remote volume with the run</span></span>
<span id="cb29-278"><a href="#cb29-278" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">".last_run_name"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb29-279"><a href="#cb29-279" aria-hidden="true" tabindex="-1"></a>        f.write(run_name)</span>
<span id="cb29-280"><a href="#cb29-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-281"><a href="#cb29-281" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Wait for the training run to finish.</span></span>
<span id="cb29-282"><a href="#cb29-282" aria-hidden="true" tabindex="-1"></a>    merge_handle <span class="op">=</span> launch_handle.get()</span>
<span id="cb29-283"><a href="#cb29-283" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> merge_lora <span class="kw">and</span> <span class="kw">not</span> preproc_only:</span>
<span id="cb29-284"><a href="#cb29-284" aria-hidden="true" tabindex="-1"></a>        merge_handle.get()</span>
<span id="cb29-285"><a href="#cb29-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-286"><a href="#cb29-286" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Run complete. Tag: </span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-287"><a href="#cb29-287" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"To inspect outputs, run `modal volume ls example-runs-vol </span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">`"</span>)</span>
<span id="cb29-288"><a href="#cb29-288" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> preproc_only:</span>
<span id="cb29-289"><a href="#cb29-289" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb29-290"><a href="#cb29-290" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"To run sample inference, run `modal run -q src.inference --run-name </span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">`"</span></span>
<span id="cb29-291"><a href="#cb29-291" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-292"><a href="#cb29-292" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-293"><a href="#cb29-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-294"><a href="#cb29-294" aria-hidden="true" tabindex="-1"></a>Then after loading the config and data, the <span class="in">`launch`</span> function is run remotely. As shown below, the <span class="in">`launch`</span> function downloads the base model from HF Hub, sets the run timestamp (or sets it from a previous run), saves config and data files to the volumes, then kicks off training by running <span class="in">`train.spawn`</span>.</span>
<span id="cb29-295"><a href="#cb29-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-296"><a href="#cb29-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="train.py"}</span></span>
<span id="cb29-297"><a href="#cb29-297" aria-hidden="true" tabindex="-1"></a><span class="at">@app.function</span>(image<span class="op">=</span>axolotl_image, timeout<span class="op">=</span><span class="dv">30</span> <span class="op">*</span> MINUTES, volumes<span class="op">=</span>VOLUME_CONFIG)</span>
<span id="cb29-298"><a href="#cb29-298" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> launch(config_raw: <span class="bu">dict</span>, data_raw: <span class="bu">str</span>, run_to_resume: <span class="bu">str</span>, preproc_only: <span class="bu">bool</span>):</span>
<span id="cb29-299"><a href="#cb29-299" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> yaml</span>
<span id="cb29-300"><a href="#cb29-300" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> huggingface_hub <span class="im">import</span> snapshot_download</span>
<span id="cb29-301"><a href="#cb29-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-302"><a href="#cb29-302" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure the base model is downloaded</span></span>
<span id="cb29-303"><a href="#cb29-303" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">(gongy): test if this works with a path to previous fine-tune</span></span>
<span id="cb29-304"><a href="#cb29-304" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> yaml.safe_load(config_raw)</span>
<span id="cb29-305"><a href="#cb29-305" aria-hidden="true" tabindex="-1"></a>    model_name <span class="op">=</span> config[<span class="st">"base_model"</span>]</span>
<span id="cb29-306"><a href="#cb29-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-307"><a href="#cb29-307" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb29-308"><a href="#cb29-308" aria-hidden="true" tabindex="-1"></a>        snapshot_download(model_name, local_files_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-309"><a href="#cb29-309" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Volume contains </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb29-310"><a href="#cb29-310" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb29-311"><a href="#cb29-311" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> ..."</span>)</span>
<span id="cb29-312"><a href="#cb29-312" aria-hidden="true" tabindex="-1"></a>        snapshot_download(model_name)</span>
<span id="cb29-313"><a href="#cb29-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-314"><a href="#cb29-314" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Committing /pretrained directory (no progress bar) ..."</span>)</span>
<span id="cb29-315"><a href="#cb29-315" aria-hidden="true" tabindex="-1"></a>        VOLUME_CONFIG[<span class="st">"/pretrained"</span>].commit()</span>
<span id="cb29-316"><a href="#cb29-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-317"><a href="#cb29-317" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write config and data into a training subfolder.</span></span>
<span id="cb29-318"><a href="#cb29-318" aria-hidden="true" tabindex="-1"></a>    time_string <span class="op">=</span> datetime.now().strftime(<span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">-%H-%M-%S"</span>)</span>
<span id="cb29-319"><a href="#cb29-319" aria-hidden="true" tabindex="-1"></a>    run_name <span class="op">=</span> (</span>
<span id="cb29-320"><a href="#cb29-320" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"axo-</span><span class="sc">{</span>time_string<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>secrets<span class="sc">.</span>token_hex(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb29-321"><a href="#cb29-321" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> run_to_resume</span>
<span id="cb29-322"><a href="#cb29-322" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span> run_to_resume</span>
<span id="cb29-323"><a href="#cb29-323" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb29-324"><a href="#cb29-324" aria-hidden="true" tabindex="-1"></a>    run_folder <span class="op">=</span> <span class="ss">f"/runs/</span><span class="sc">{</span>run_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb29-325"><a href="#cb29-325" aria-hidden="true" tabindex="-1"></a>    os.makedirs(run_folder, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-326"><a href="#cb29-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-327"><a href="#cb29-327" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Preparing training run in </span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb29-328"><a href="#cb29-328" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> (</span>
<span id="cb29-329"><a href="#cb29-329" aria-hidden="true" tabindex="-1"></a>        <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/config.yml"</span>, <span class="st">"w"</span>) <span class="im">as</span> config_file,</span>
<span id="cb29-330"><a href="#cb29-330" aria-hidden="true" tabindex="-1"></a>        <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>config[<span class="st">'datasets'</span>][<span class="dv">0</span>][<span class="st">'path'</span>]<span class="sc">}</span><span class="ss">"</span>, <span class="st">"w"</span>) <span class="im">as</span> data_file,</span>
<span id="cb29-331"><a href="#cb29-331" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb29-332"><a href="#cb29-332" aria-hidden="true" tabindex="-1"></a>        config_file.write(config_raw)</span>
<span id="cb29-333"><a href="#cb29-333" aria-hidden="true" tabindex="-1"></a>        data_file.write(data_raw)</span>
<span id="cb29-334"><a href="#cb29-334" aria-hidden="true" tabindex="-1"></a>    VOLUME_CONFIG[<span class="st">"/runs"</span>].commit()</span>
<span id="cb29-335"><a href="#cb29-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-336"><a href="#cb29-336" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> preproc_only:</span>
<span id="cb29-337"><a href="#cb29-337" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Spawning container for data preprocessing."</span>)</span>
<span id="cb29-338"><a href="#cb29-338" aria-hidden="true" tabindex="-1"></a>        launch_handle <span class="op">=</span> preproc_data.spawn(run_folder)</span>
<span id="cb29-339"><a href="#cb29-339" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb29-340"><a href="#cb29-340" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Spawning container for data preprocessing."</span>)</span>
<span id="cb29-341"><a href="#cb29-341" aria-hidden="true" tabindex="-1"></a>        preproc_handle <span class="op">=</span> preproc_data.spawn(run_folder)</span>
<span id="cb29-342"><a href="#cb29-342" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/logs.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb29-343"><a href="#cb29-343" aria-hidden="true" tabindex="-1"></a>            lbl <span class="op">=</span> <span class="st">"preproc"</span></span>
<span id="cb29-344"><a href="#cb29-344" aria-hidden="true" tabindex="-1"></a>            f.write(<span class="ss">f"</span><span class="sc">{</span>lbl<span class="sc">}</span><span class="ss">: https://modal.com/logs/call/</span><span class="sc">{</span>preproc_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-345"><a href="#cb29-345" aria-hidden="true" tabindex="-1"></a>        <span class="co"># wait for preprocessing to finish.</span></span>
<span id="cb29-346"><a href="#cb29-346" aria-hidden="true" tabindex="-1"></a>        preproc_handle.get()</span>
<span id="cb29-347"><a href="#cb29-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-348"><a href="#cb29-348" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Start training run.</span></span>
<span id="cb29-349"><a href="#cb29-349" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Spawning container for training."</span>)</span>
<span id="cb29-350"><a href="#cb29-350" aria-hidden="true" tabindex="-1"></a>        launch_handle <span class="op">=</span> train.spawn(run_folder, config[<span class="st">"output_dir"</span>])</span>
<span id="cb29-351"><a href="#cb29-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-352"><a href="#cb29-352" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/logs.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb29-353"><a href="#cb29-353" aria-hidden="true" tabindex="-1"></a>        lbl <span class="op">=</span> <span class="st">"train"</span> <span class="cf">if</span> <span class="kw">not</span> preproc_only <span class="cf">else</span> <span class="st">"preproc"</span></span>
<span id="cb29-354"><a href="#cb29-354" aria-hidden="true" tabindex="-1"></a>        f.write(<span class="ss">f"</span><span class="sc">{</span>lbl<span class="sc">}</span><span class="ss">: https://modal.com/logs/call/</span><span class="sc">{</span>launch_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-355"><a href="#cb29-355" aria-hidden="true" tabindex="-1"></a>    VOLUME_CONFIG[<span class="st">"/runs"</span>].commit()</span>
<span id="cb29-356"><a href="#cb29-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-357"><a href="#cb29-357" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> run_name, launch_handle</span>
<span id="cb29-358"><a href="#cb29-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-359"><a href="#cb29-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-360"><a href="#cb29-360" aria-hidden="true" tabindex="-1"></a>So last, let's examine the <span class="in">`train`</span> function:</span>
<span id="cb29-361"><a href="#cb29-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-362"><a href="#cb29-362" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="train.py"}</span></span>
<span id="cb29-363"><a href="#cb29-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-364"><a href="#cb29-364" aria-hidden="true" tabindex="-1"></a><span class="at">@app.function</span>(</span>
<span id="cb29-365"><a href="#cb29-365" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>axolotl_image,</span>
<span id="cb29-366"><a href="#cb29-366" aria-hidden="true" tabindex="-1"></a>    gpu<span class="op">=</span>GPU_CONFIG,</span>
<span id="cb29-367"><a href="#cb29-367" aria-hidden="true" tabindex="-1"></a>    volumes<span class="op">=</span>VOLUME_CONFIG,</span>
<span id="cb29-368"><a href="#cb29-368" aria-hidden="true" tabindex="-1"></a>    timeout<span class="op">=</span><span class="dv">24</span> <span class="op">*</span> HOURS,</span>
<span id="cb29-369"><a href="#cb29-369" aria-hidden="true" tabindex="-1"></a>    _allow_background_volume_commits<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-370"><a href="#cb29-370" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-371"><a href="#cb29-371" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(run_folder: <span class="bu">str</span>, output_dir: <span class="bu">str</span>):</span>
<span id="cb29-372"><a href="#cb29-372" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb29-373"><a href="#cb29-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-374"><a href="#cb29-374" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Starting training run in </span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb29-375"><a href="#cb29-375" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>device_count()<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_name()<span class="sc">}</span><span class="ss"> GPU(s)."</span>)</span>
<span id="cb29-376"><a href="#cb29-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-377"><a href="#cb29-377" aria-hidden="true" tabindex="-1"></a>    ALLOW_WANDB <span class="op">=</span> os.environ.get(<span class="st">"ALLOW_WANDB"</span>, <span class="st">"false"</span>).lower() <span class="op">==</span> <span class="st">"true"</span></span>
<span id="cb29-378"><a href="#cb29-378" aria-hidden="true" tabindex="-1"></a>    cmd <span class="op">=</span> <span class="ss">f"accelerate launch -m axolotl.cli.train ./config.yml </span><span class="sc">{</span><span class="st">'--wandb_mode disabled'</span> <span class="cf">if</span> <span class="kw">not</span> ALLOW_WANDB <span class="cf">else</span> <span class="st">''</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb29-379"><a href="#cb29-379" aria-hidden="true" tabindex="-1"></a>    run_cmd(cmd, run_folder)</span>
<span id="cb29-380"><a href="#cb29-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-381"><a href="#cb29-381" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Kick off CPU job to merge the LoRA weights into base model.</span></span>
<span id="cb29-382"><a href="#cb29-382" aria-hidden="true" tabindex="-1"></a>    merge_handle <span class="op">=</span> merge.spawn(run_folder, output_dir)</span>
<span id="cb29-383"><a href="#cb29-383" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>run_folder<span class="sc">}</span><span class="ss">/logs.txt"</span>, <span class="st">"a"</span>) <span class="im">as</span> f:</span>
<span id="cb29-384"><a href="#cb29-384" aria-hidden="true" tabindex="-1"></a>        f.write(<span class="ss">f"&lt;br&gt;merge: https://modal.com/logs/call/</span><span class="sc">{</span>merge_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb29-385"><a href="#cb29-385" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Beginning merge </span><span class="sc">{</span>merge_handle<span class="sc">.</span>object_id<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb29-386"><a href="#cb29-386" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> merge_handle</span>
<span id="cb29-387"><a href="#cb29-387" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-388"><a href="#cb29-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-389"><a href="#cb29-389" aria-hidden="true" tabindex="-1"></a>From this, we can see that we're largely running <span class="in">`accelerate launch -m axolotl.cli.train`</span> but using <span class="in">`run_cmd`</span>, which run the command inside a folder with Modal Volume reloading before and commiting on success.</span>
<span id="cb29-390"><a href="#cb29-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-391"><a href="#cb29-391" aria-hidden="true" tabindex="-1"></a><span class="fu">#### config</span></span>
<span id="cb29-392"><a href="#cb29-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-393"><a href="#cb29-393" aria-hidden="true" tabindex="-1"></a>Let's now do a deep dive on the config file as it's a critical input. You may also find <span class="co">[</span><span class="ot">this config</span><span class="co">](https://openaccess-ai-collective.github.io/axolotl/docs/config.html)</span> doc to be helpful in explaining the configuraiton options.</span>
<span id="cb29-394"><a href="#cb29-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-395"><a href="#cb29-395" aria-hidden="true" tabindex="-1"></a>First, let's start at the top:</span>
<span id="cb29-396"><a href="#cb29-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-397"><a href="#cb29-397" aria-hidden="true" tabindex="-1"></a><span class="in">```{.yaml filename="pythia.yml"}</span></span>
<span id="cb29-398"><a href="#cb29-398" aria-hidden="true" tabindex="-1"></a><span class="co"># Lightweight example of training a small Pythia model for simple demonstrations</span></span>
<span id="cb29-399"><a href="#cb29-399" aria-hidden="true" tabindex="-1"></a><span class="fu">base_model</span><span class="kw">:</span><span class="at"> EleutherAI/pythia-1.4b-deduped</span></span>
<span id="cb29-400"><a href="#cb29-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-401"><a href="#cb29-401" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_8bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span><span class="co"> # pythia is small, so keep it in 16-bit precision</span></span>
<span id="cb29-402"><a href="#cb29-402" aria-hidden="true" tabindex="-1"></a><span class="fu">strict</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb29-403"><a href="#cb29-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-404"><a href="#cb29-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-405"><a href="#cb29-405" aria-hidden="true" tabindex="-1"></a>Here we specify the <span class="in">`base_model`</span>, which we decided to start with Pythia model for demo purposes. We also decided not to quantize our model to 8bit since Pythia is so small; therefore, this is why we ran LoRA, not QLoRA.</span>
<span id="cb29-406"><a href="#cb29-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-407"><a href="#cb29-407" aria-hidden="true" tabindex="-1"></a><span class="in">```{.yaml filename="pythia.yml"}</span></span>
<span id="cb29-408"><a href="#cb29-408" aria-hidden="true" tabindex="-1"></a><span class="fu">datasets</span><span class="kw">:</span></span>
<span id="cb29-409"><a href="#cb29-409" aria-hidden="true" tabindex="-1"></a><span class="co">  # This will be the path used for the data when it is saved to the Volume in the cloud.</span></span>
<span id="cb29-410"><a href="#cb29-410" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> data.jsonl</span></span>
<span id="cb29-411"><a href="#cb29-411" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ds_type</span><span class="kw">:</span><span class="at"> json</span></span>
<span id="cb29-412"><a href="#cb29-412" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span></span>
<span id="cb29-413"><a href="#cb29-413" aria-hidden="true" tabindex="-1"></a><span class="co">      # JSONL file contains question, context, answer fields per line.</span></span>
<span id="cb29-414"><a href="#cb29-414" aria-hidden="true" tabindex="-1"></a><span class="co">      # This gets mapped to instruction, input, output axolotl tags.</span></span>
<span id="cb29-415"><a href="#cb29-415" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_instruction</span><span class="kw">:</span><span class="at"> question</span></span>
<span id="cb29-416"><a href="#cb29-416" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_input</span><span class="kw">:</span><span class="at"> context</span></span>
<span id="cb29-417"><a href="#cb29-417" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_output</span><span class="kw">:</span><span class="at"> answer</span></span>
<span id="cb29-418"><a href="#cb29-418" aria-hidden="true" tabindex="-1"></a><span class="co">      # Format is used by axolotl to generate the prompt.</span></span>
<span id="cb29-419"><a href="#cb29-419" aria-hidden="true" tabindex="-1"></a><span class="fu">      format</span><span class="kw">: </span><span class="ch">|-</span></span>
<span id="cb29-420"><a href="#cb29-420" aria-hidden="true" tabindex="-1"></a>        [INST] Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb29-421"><a href="#cb29-421" aria-hidden="true" tabindex="-1"></a>        {input}</span>
<span id="cb29-422"><a href="#cb29-422" aria-hidden="true" tabindex="-1"></a>        {instruction} [/INST]</span>
<span id="cb29-423"><a href="#cb29-423" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-424"><a href="#cb29-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-425"><a href="#cb29-425" aria-hidden="true" tabindex="-1"></a>The next part shows the dataset configuration. In this case, we set the <span class="in">`path`</span> and <span class="in">`ds_type`</span> for the data when it is saved to the cloud Volume.</span>
<span id="cb29-426"><a href="#cb29-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-427"><a href="#cb29-427" aria-hidden="true" tabindex="-1"></a>Next, we specify the mapping of the key values in our <span class="in">`.jsonl`</span> file to the <span class="in">`instruction`</span>, <span class="in">`input`</span>, and <span class="in">`output`</span>. This generally follows the <span class="co">[</span><span class="ot">`alpaca`</span><span class="co">](https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/inst_tune.html#alpaca)</span> standard where our prompt is expecting those three formats. </span>
<span id="cb29-428"><a href="#cb29-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-429"><a href="#cb29-429" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb29-430"><a href="#cb29-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-431"><a href="#cb29-431" aria-hidden="true" tabindex="-1"></a><span class="fu">## How to add custom prompt format</span></span>
<span id="cb29-432"><a href="#cb29-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-433"><a href="#cb29-433" aria-hidden="true" tabindex="-1"></a>Per the <span class="co">[</span><span class="ot">docs</span><span class="co">](https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/inst_tune.html#how-to-add-custom-prompt-format)</span>, for a dataset that is preprocessed for instruction purposes:</span>
<span id="cb29-434"><a href="#cb29-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-435"><a href="#cb29-435" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python filename="data.jsonl"}</span></span>
<span id="cb29-436"><a href="#cb29-436" aria-hidden="true" tabindex="-1"></a>{<span class="st">"input"</span>: <span class="st">"..."</span>, <span class="st">"output"</span>: <span class="st">"..."</span>}</span>
<span id="cb29-437"><a href="#cb29-437" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-438"><a href="#cb29-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-439"><a href="#cb29-439" aria-hidden="true" tabindex="-1"></a>You can use this example in your YAML config:</span>
<span id="cb29-440"><a href="#cb29-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-441"><a href="#cb29-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{.yaml filename="config.yaml"}</span></span>
<span id="cb29-442"><a href="#cb29-442" aria-hidden="true" tabindex="-1"></a><span class="fu">datasets</span><span class="kw">:</span></span>
<span id="cb29-443"><a href="#cb29-443" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> repo</span></span>
<span id="cb29-444"><a href="#cb29-444" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span></span>
<span id="cb29-445"><a href="#cb29-445" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">system_prompt</span><span class="kw">:</span><span class="at"> </span><span class="st">""</span></span>
<span id="cb29-446"><a href="#cb29-446" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_system</span><span class="kw">:</span><span class="at"> system</span></span>
<span id="cb29-447"><a href="#cb29-447" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_instruction</span><span class="kw">:</span><span class="at"> input</span></span>
<span id="cb29-448"><a href="#cb29-448" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">field_output</span><span class="kw">:</span><span class="at"> output</span></span>
<span id="cb29-449"><a href="#cb29-449" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">format</span><span class="kw">:</span><span class="at"> </span><span class="st">"[INST] {instruction} [/INST]"</span></span>
<span id="cb29-450"><a href="#cb29-450" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">no_input_format</span><span class="kw">:</span><span class="at"> </span><span class="st">"[INST] {instruction} [/INST]"</span></span>
<span id="cb29-451"><a href="#cb29-451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-452"><a href="#cb29-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-453"><a href="#cb29-453" aria-hidden="true" tabindex="-1"></a>See the <span class="co">[</span><span class="ot">docs</span><span class="co">](https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/inst_tune.html)</span> for other data format options</span>
<span id="cb29-454"><a href="#cb29-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-455"><a href="#cb29-455" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb29-456"><a href="#cb29-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-457"><a href="#cb29-457" aria-hidden="true" tabindex="-1"></a><span class="in">```{.yaml filename="pythia.yml"}</span></span>
<span id="cb29-458"><a href="#cb29-458" aria-hidden="true" tabindex="-1"></a><span class="co"># add tokens</span></span>
<span id="cb29-459"><a href="#cb29-459" aria-hidden="true" tabindex="-1"></a><span class="fu">tokens</span><span class="kw">:</span></span>
<span id="cb29-460"><a href="#cb29-460" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">"[INST]"</span></span>
<span id="cb29-461"><a href="#cb29-461" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">" [/INST]"</span></span>
<span id="cb29-462"><a href="#cb29-462" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">"[SQL]"</span></span>
<span id="cb29-463"><a href="#cb29-463" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">" [/SQL]"</span></span>
<span id="cb29-464"><a href="#cb29-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-465"><a href="#cb29-465" aria-hidden="true" tabindex="-1"></a><span class="fu">dataset_prepared_path</span><span class="kw">:</span><span class="at"> preprocessed</span></span>
<span id="cb29-466"><a href="#cb29-466" aria-hidden="true" tabindex="-1"></a><span class="fu">val_set_size</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb29-467"><a href="#cb29-467" aria-hidden="true" tabindex="-1"></a><span class="fu">output_dir</span><span class="kw">:</span><span class="at"> ./lora-out</span></span>
<span id="cb29-468"><a href="#cb29-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-469"><a href="#cb29-469" aria-hidden="true" tabindex="-1"></a><span class="co"># max length of an input to train with, typically less than 2048 as most have context limit of 2048</span></span>
<span id="cb29-470"><a href="#cb29-470" aria-hidden="true" tabindex="-1"></a><span class="fu">sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="dv">4096</span></span>
<span id="cb29-471"><a href="#cb29-471" aria-hidden="true" tabindex="-1"></a><span class="co"># use efficient multi-packing with block diagonal attention and per sequence position_ids.</span></span>
<span id="cb29-472"><a href="#cb29-472" aria-hidden="true" tabindex="-1"></a><span class="fu">sample_packing</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb29-473"><a href="#cb29-473" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_sample_packing</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb29-474"><a href="#cb29-474" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad inputs so each step uses constant sized buffers</span></span>
<span id="cb29-475"><a href="#cb29-475" aria-hidden="true" tabindex="-1"></a><span class="co"># This will reduce memory fragmentation and may prevent OOMs, by re-using memory more efficiently</span></span>
<span id="cb29-476"><a href="#cb29-476" aria-hidden="true" tabindex="-1"></a><span class="fu">pad_to_sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb29-477"><a href="#cb29-477" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-478"><a href="#cb29-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-479"><a href="#cb29-479" aria-hidden="true" tabindex="-1"></a>This part provides special tokens, paths, and some settings for handling padding and maximium training sequence length.</span>
<span id="cb29-480"><a href="#cb29-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-481"><a href="#cb29-481" aria-hidden="true" tabindex="-1"></a><span class="in">```{.yaml filename="pythia.yml"}</span></span>
<span id="cb29-482"><a href="#cb29-482" aria-hidden="true" tabindex="-1"></a><span class="fu">adapter</span><span class="kw">:</span><span class="at"> lora</span></span>
<span id="cb29-483"><a href="#cb29-483" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_model_dir</span><span class="kw">:</span></span>
<span id="cb29-484"><a href="#cb29-484" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_r</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb29-485"><a href="#cb29-485" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_alpha</span><span class="kw">:</span><span class="at"> </span><span class="dv">32</span></span>
<span id="cb29-486"><a href="#cb29-486" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_dropout</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb29-487"><a href="#cb29-487" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_linear</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb29-488"><a href="#cb29-488" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_fan_in_fan_out</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co"> # required for pythia/GPTNeoX lora</span></span>
<span id="cb29-489"><a href="#cb29-489" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_modules</span><span class="kw">:</span></span>
<span id="cb29-490"><a href="#cb29-490" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span>query_key_value</span>
<span id="cb29-491"><a href="#cb29-491" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_modules_to_save</span><span class="kw">:</span><span class="co"> # required when adding new tokens to pythia</span></span>
<span id="cb29-492"><a href="#cb29-492" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span>embed_in</span>
<span id="cb29-493"><a href="#cb29-493" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span>embed_out</span>
<span id="cb29-494"><a href="#cb29-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-495"><a href="#cb29-495" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_accumulation_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb29-496"><a href="#cb29-496" aria-hidden="true" tabindex="-1"></a><span class="fu">micro_batch_size</span><span class="kw">:</span><span class="at"> </span><span class="dv">32</span></span>
<span id="cb29-497"><a href="#cb29-497" aria-hidden="true" tabindex="-1"></a><span class="fu">num_epochs</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb29-498"><a href="#cb29-498" aria-hidden="true" tabindex="-1"></a><span class="fu">optimizer</span><span class="kw">:</span><span class="at"> adamw_torch</span></span>
<span id="cb29-499"><a href="#cb29-499" aria-hidden="true" tabindex="-1"></a><span class="fu">lr_scheduler</span><span class="kw">:</span><span class="at"> cosine</span></span>
<span id="cb29-500"><a href="#cb29-500" aria-hidden="true" tabindex="-1"></a><span class="fu">learning_rate</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0001</span></span>
<span id="cb29-501"><a href="#cb29-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-502"><a href="#cb29-502" aria-hidden="true" tabindex="-1"></a><span class="fu">bf16</span><span class="kw">:</span><span class="at"> auto</span></span>
<span id="cb29-503"><a href="#cb29-503" aria-hidden="true" tabindex="-1"></a><span class="fu">fp16</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb29-504"><a href="#cb29-504" aria-hidden="true" tabindex="-1"></a><span class="fu">tf32</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb29-505"><a href="#cb29-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-506"><a href="#cb29-506" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_checkpointing</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb29-507"><a href="#cb29-507" aria-hidden="true" tabindex="-1"></a><span class="fu">early_stopping_patience</span><span class="kw">:</span></span>
<span id="cb29-508"><a href="#cb29-508" aria-hidden="true" tabindex="-1"></a><span class="fu">local_rank</span><span class="kw">:</span></span>
<span id="cb29-509"><a href="#cb29-509" aria-hidden="true" tabindex="-1"></a><span class="fu">logging_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb29-510"><a href="#cb29-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-511"><a href="#cb29-511" aria-hidden="true" tabindex="-1"></a><span class="fu">warmup_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb29-512"><a href="#cb29-512" aria-hidden="true" tabindex="-1"></a><span class="fu">weight_decay</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0</span></span>
<span id="cb29-513"><a href="#cb29-513" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-514"><a href="#cb29-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-515"><a href="#cb29-515" aria-hidden="true" tabindex="-1"></a>This last part is where we're customizing our LoRA model. For this, I'm not going to go into detail but I encourage the curious reader to read on Sebastian Raschka's wonderful <span class="co">[</span><span class="ot">Practical Tips for Finetuning LLMs</span><span class="co">](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)</span>. It explains a lot of the intuition on LoRA and tips from LoRA training experiments.</span>
<span id="cb29-516"><a href="#cb29-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-517"><a href="#cb29-517" aria-hidden="true" tabindex="-1"></a><span class="fu">## Warnings and Errors</span></span>
<span id="cb29-518"><a href="#cb29-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-519"><a href="#cb29-519" aria-hidden="true" tabindex="-1"></a>I found several warnings in the logs, so let's review them.</span>
<span id="cb29-520"><a href="#cb29-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-521"><a href="#cb29-521" aria-hidden="true" tabindex="-1"></a>First, let's note the versions of dependencies:</span>
<span id="cb29-522"><a href="#cb29-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-523"><a href="#cb29-523" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-524"><a href="#cb29-524" aria-hidden="true" tabindex="-1"></a><span class="ex">****************************************</span></span>
<span id="cb29-525"><a href="#cb29-525" aria-hidden="true" tabindex="-1"></a><span class="ex">****</span> Axolotl Dependency Versions <span class="pp">*****</span></span>
<span id="cb29-526"><a href="#cb29-526" aria-hidden="true" tabindex="-1"></a>  <span class="ex">accelerate:</span> 0.30.1         </span>
<span id="cb29-527"><a href="#cb29-527" aria-hidden="true" tabindex="-1"></a>        <span class="ex">peft:</span> 0.10.0         </span>
<span id="cb29-528"><a href="#cb29-528" aria-hidden="true" tabindex="-1"></a><span class="ex">transformers:</span> 4.40.2         </span>
<span id="cb29-529"><a href="#cb29-529" aria-hidden="true" tabindex="-1"></a>         <span class="ex">trl:</span> 0.8.5          </span>
<span id="cb29-530"><a href="#cb29-530" aria-hidden="true" tabindex="-1"></a>       <span class="ex">torch:</span> 2.2.2+cu121    </span>
<span id="cb29-531"><a href="#cb29-531" aria-hidden="true" tabindex="-1"></a><span class="ex">bitsandbytes:</span> 0.43.1         </span>
<span id="cb29-532"><a href="#cb29-532" aria-hidden="true" tabindex="-1"></a><span class="ex">****************************************</span></span>
<span id="cb29-533"><a href="#cb29-533" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-534"><a href="#cb29-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-535"><a href="#cb29-535" aria-hidden="true" tabindex="-1"></a>This isn't surprising as we specified this in our <span class="in">`common.py`</span> script, but I'm noting just for documentation sake.</span>
<span id="cb29-536"><a href="#cb29-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-537"><a href="#cb29-537" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-538"><a href="#cb29-538" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> following values were not passed to <span class="kw">`</span><span class="ex">accelerate</span> launch<span class="kw">`</span> and had defaults used instead:</span>
<span id="cb29-539"><a href="#cb29-539" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--num_processes</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="ex">2</span><span class="kw">`</span></span>
<span id="cb29-540"><a href="#cb29-540" aria-hidden="true" tabindex="-1"></a>                <span class="ex">More</span> than one GPU was found, enabling multi-GPU training.</span>
<span id="cb29-541"><a href="#cb29-541" aria-hidden="true" tabindex="-1"></a>                <span class="ex">If</span> this was unintended please pass in <span class="kw">`</span><span class="ex">--num_processes=1</span><span class="kw">`</span>.</span>
<span id="cb29-542"><a href="#cb29-542" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--num_machines</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="ex">1</span><span class="kw">`</span></span>
<span id="cb29-543"><a href="#cb29-543" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--mixed_precision</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="st">'no'</span><span class="kw">`</span></span>
<span id="cb29-544"><a href="#cb29-544" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--dynamo_backend</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="st">'no'</span><span class="kw">`</span></span>
<span id="cb29-545"><a href="#cb29-545" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> avoid this warning pass in values for each of the problematic parameters or run <span class="kw">`</span><span class="ex">accelerate</span> config<span class="kw">`</span>.</span>
<span id="cb29-546"><a href="#cb29-546" aria-hidden="true" tabindex="-1"></a><span class="ex">WARNING:</span> BNB_CUDA_VERSION=121 environment variable detected<span class="kw">;</span> <span class="ex">loading</span> libbitsandbytes_cuda121.so.</span>
<span id="cb29-547"><a href="#cb29-547" aria-hidden="true" tabindex="-1"></a><span class="ex">This</span> can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.</span>
<span id="cb29-548"><a href="#cb29-548" aria-hidden="true" tabindex="-1"></a><span class="ex">If</span> this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=</span>
<span id="cb29-549"><a href="#cb29-549" aria-hidden="true" tabindex="-1"></a><span class="ex">If</span> you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH</span>
<span id="cb29-550"><a href="#cb29-550" aria-hidden="true" tabindex="-1"></a><span class="ex">For</span> example by adding the following to your .bashrc: export LD_LIBRARY_PATH=<span class="va">$LD_LIBRARY_PATH</span>:<span class="op">&lt;</span>path_to_cuda_dir/lib64</span>
<span id="cb29-551"><a href="#cb29-551" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-552"><a href="#cb29-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-553"><a href="#cb29-553" aria-hidden="true" tabindex="-1"></a>For these warnings, they're just that: warnings of defaults being used but not deterimental (IMHO) to training.</span>
<span id="cb29-554"><a href="#cb29-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-555"><a href="#cb29-555" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-556"><a href="#cb29-556" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:04:15,288] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:203:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to cuda <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb29-557"><a href="#cb29-557" aria-hidden="true" tabindex="-1"></a><span class="ex">df:</span> /root/.triton/autotune: No such file or directory</span>
<span id="cb29-558"><a href="#cb29-558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-559"><a href="#cb29-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-560"><a href="#cb29-560" aria-hidden="true" tabindex="-1"></a>This warning seems not to find a Triton file; this doesn't seem to be a problem but worth noting.</span>
<span id="cb29-561"><a href="#cb29-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-562"><a href="#cb29-562" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-563"><a href="#cb29-563" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  Please specify the CUTLASS repo directory as environment variable <span class="va">$CUTLASS_PATH</span></span>
<span id="cb29-564"><a href="#cb29-564" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  sparse_attn requires a torch version <span class="op">&gt;</span>= 1.5 and <span class="op">&lt;</span> 2.0 but detected 2.2</span>
<span id="cb29-565"><a href="#cb29-565" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  using untested triton version <span class="er">(</span><span class="ex">2.2.0</span><span class="kw">)</span><span class="ex">,</span> only 1.0.0 is known to be compatible</span>
<span id="cb29-566"><a href="#cb29-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-567"><a href="#cb29-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-568"><a href="#cb29-568" aria-hidden="true" tabindex="-1"></a>For this warning, this seems to mention that <span class="in">`sparse_attn`</span> uses an older version of <span class="in">`torch`</span> that what we're running. This <span class="co">[</span><span class="ot">seems</span><span class="co">](https://github.com/microsoft/DeepSpeed/issues/3117)</span> to be a bit of a known problem so we'll ignore this for now.</span>
<span id="cb29-569"><a href="#cb29-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-570"><a href="#cb29-570" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-571"><a href="#cb29-571" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:04:20,191] <span class="pp">[</span><span class="ss">WARNING</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.utils.config.models.input.hint_lora_8bit:973</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:28</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> We recommend setting <span class="kw">`</span><span class="ex">load_in_8bit:</span> true<span class="kw">`</span> for LORA finetuning</span>
<span id="cb29-572"><a href="#cb29-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-573"><a href="#cb29-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-574"><a href="#cb29-574" aria-hidden="true" tabindex="-1"></a>This is expected. Back in the <span class="in">`pythia.yml`</span>, there's this comment:</span>
<span id="cb29-575"><a href="#cb29-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-576"><a href="#cb29-576" aria-hidden="true" tabindex="-1"></a><span class="in">```yaml</span></span>
<span id="cb29-577"><a href="#cb29-577" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_8bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span><span class="co"> # pythia is small, so keep it in 16-bit precision</span></span>
<span id="cb29-578"><a href="#cb29-578" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-579"><a href="#cb29-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-580"><a href="#cb29-580" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-581"><a href="#cb29-581" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:04:26,870] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_tokenizer:294</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:29</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:1</span><span class="pp">]</span> No Chat template selected. Consider adding a chat template for easier inference.</span>
<span id="cb29-582"><a href="#cb29-582" aria-hidden="true" tabindex="-1"></a><span class="ex">Special</span> tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span>
<span id="cb29-583"><a href="#cb29-583" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-584"><a href="#cb29-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-585"><a href="#cb29-585" aria-hidden="true" tabindex="-1"></a>This <span class="co">[</span><span class="ot">seems</span><span class="co">](https://github.com/OpenAccess-AI-Collective/axolotl/blob/3f1f5e33120b75f83736d75eb3bea0a6dad5424c/src/axolotl/utils/models.py#L295)</span> like a recommended best practice. I'll try to remember this for next time.</span>
<span id="cb29-586"><a href="#cb29-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-587"><a href="#cb29-587" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-588"><a href="#cb29-588" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 16:06:10,805] <span class="pp">[</span><span class="ss">WARNING</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.load_model:712</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:28</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> increasing model.config.max_position_embeddings from 2048 to 4096</span>
<span id="cb29-589"><a href="#cb29-589" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-590"><a href="#cb29-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-591"><a href="#cb29-591" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-592"><a href="#cb29-592" aria-hidden="true" tabindex="-1"></a><span class="in">/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1050: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.</span></span>
<span id="cb29-593"><a href="#cb29-593" aria-hidden="true" tabindex="-1"></a><span class="in">  warnings.warn(</span></span>
<span id="cb29-594"><a href="#cb29-594" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-595"><a href="#cb29-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-596"><a href="#cb29-596" aria-hidden="true" tabindex="-1"></a>These two warnings are a bit a little surprising as we saw in the <span class="in">`pythia.yml`</span>, e.g., <span class="in">`lora_fan_in_fan_out: true # required for pythia/GPTNeoX lora`</span> and <span class="in">`sequence_len: 4096`</span>. This is worth more investigation and awareness.</span>
<span id="cb29-597"><a href="#cb29-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-598"><a href="#cb29-598" aria-hidden="true" tabindex="-1"></a><span class="fu">## Train LoRA on LLaMA 3 8B</span></span>
<span id="cb29-599"><a href="#cb29-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-600"><a href="#cb29-600" aria-hidden="true" tabindex="-1"></a>Now that we've explored the base code a bit better, let's see if we can rerun a LoRA training, but this time with we'll make a few changes to <span class="co">[</span><span class="ot">`llama-3.yml`</span><span class="co">](https://github.com/modal-labs/llm-finetuning/blob/c94266459911b9473ee4c4d9c4d37d7946224fd9/config/llama-3.yml)</span>:</span>
<span id="cb29-601"><a href="#cb29-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-602"><a href="#cb29-602" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We'll use <span class="in">`NousResearch/Meta-Llama-3-8B`</span>. This is a larger model and more state-of-the-art.</span>
<span id="cb29-603"><a href="#cb29-603" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We'll use the larger <span class="in">`sqlqa.jsonl`</span> dataset, which is 4,000 records instead of 64 from <span class="in">`sqlqa_subsample.jsonl`</span></span>
<span id="cb29-604"><a href="#cb29-604" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Change the Modal app name in <span class="in">`common.py`</span> to <span class="in">`APP_NAME = "sqlqa-llama-3-8b"`</span></span>
<span id="cb29-605"><a href="#cb29-605" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Change in the config to <span class="in">`deepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json`</span> and use <span class="in">`GPU_CONFIG=a100-80gb:4`</span></span>
<span id="cb29-606"><a href="#cb29-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-607"><a href="#cb29-607" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-608"><a href="#cb29-608" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> GPU_CONFIG=a100-80gb:4 modal run <span class="at">--detach</span> src.train <span class="at">--config</span><span class="op">=</span>config/llama-3.yml <span class="at">--data</span><span class="op">=</span>data/sqlqa.jsonl</span>
<span id="cb29-609"><a href="#cb29-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-610"><a href="#cb29-610" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb29-611"><a href="#cb29-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-612"><a href="#cb29-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-613"><a href="#cb29-613" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-06-19</span> 20:12:56,242] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.scripts.do_merge_lora:153</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:27</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> saving merged model to: lora-out/merged</span>
<span id="cb29-614"><a href="#cb29-614" aria-hidden="true" tabindex="-1"></a><span class="ex">Unloading</span> and merging model: 100%<span class="kw">|</span><span class="ex">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span><span class="kw">|</span> <span class="ex">684/684</span> [00:00<span class="op">&lt;</span>00:00, 3664.51it/s]</span>
<span id="cb29-615"><a href="#cb29-615" aria-hidden="true" tabindex="-1"></a><span class="ex">Unloading</span> and merging model: 100%<span class="kw">|</span><span class="ex">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span><span class="kw">|</span> <span class="ex">684/684</span> [00:00<span class="op">&lt;</span>00:00, 3580.76it/s]</span>
<span id="cb29-616"><a href="#cb29-616" aria-hidden="true" tabindex="-1"></a><span class="ex">Run</span> complete. Tag: axo-2024-06-19-19-47-41-2f6c</span>
<span id="cb29-617"><a href="#cb29-617" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> inspect outputs, run <span class="kw">`</span><span class="ex">modal</span> volume ls example-runs-vol axo-2024-06-19-19-47-41-2f6c<span class="kw">`</span></span>
<span id="cb29-618"><a href="#cb29-618" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> run sample inference, run <span class="kw">`</span><span class="ex">modal</span> run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-19-47-41-2f6c<span class="kw">`</span></span>
<span id="cb29-619"><a href="#cb29-619" aria-hidden="true" tabindex="-1"></a><span class="ex">Runner</span> terminated.</span>
<span id="cb29-620"><a href="#cb29-620" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-621"><a href="#cb29-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-622"><a href="#cb29-622" aria-hidden="true" tabindex="-1"></a>So we've now trained our LoRA adapter!</span>
<span id="cb29-623"><a href="#cb29-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-624"><a href="#cb29-624" aria-hidden="true" tabindex="-1"></a>We can view some of the logs in Modal's UI:</span>
<span id="cb29-625"><a href="#cb29-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-626"><a href="#cb29-626" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/13_finetuning/lora-llama3.png)</span></span>
<span id="cb29-627"><a href="#cb29-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-628"><a href="#cb29-628" aria-hidden="true" tabindex="-1"></a>Nice! We can see all four A100's are working, with the peak GPU RAM about 180GB. For four epochs, this took about 20 minutes, which isn't too bad.</span>
<span id="cb29-629"><a href="#cb29-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-630"><a href="#cb29-630" aria-hidden="true" tabindex="-1"></a>Let's now test this by running inference in CLI:</span>
<span id="cb29-631"><a href="#cb29-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-632"><a href="#cb29-632" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb29-633"><a href="#cb29-633" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> modal run <span class="at">-q</span> src.inference <span class="at">--run-name</span> axo-2024-06-19-19-47-41-2f6c <span class="at">--prompt</span> <span class="st">"[INST] Using the schema context below, generate a SQL query that answers the question.</span></span>
<span id="cb29-634"><a href="#cb29-634" aria-hidden="true" tabindex="-1"></a><span class="st">        CREATE TABLE Has_allergy (Allergy VARCHAR)</span></span>
<span id="cb29-635"><a href="#cb29-635" aria-hidden="true" tabindex="-1"></a><span class="st">        How many students have cat allergies? [/INST]"</span></span>
<span id="cb29-636"><a href="#cb29-636" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Querying model axo-2024-06-19-19-47-41-2f6c</span>
<span id="cb29-637"><a href="#cb29-637" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Initializing vLLM engine for model at /runs/axo-2024-06-19-19-47-41-2f6c/lora-out/merged</span>
<span id="cb29-638"><a href="#cb29-638" aria-hidden="true" tabindex="-1"></a><span class="ex">2024-06-19</span> 20:18:11,543 INFO worker.py:1753 <span class="at">--</span> Started a local Ray instance.</span>
<span id="cb29-639"><a href="#cb29-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-640"><a href="#cb29-640" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb29-641"><a href="#cb29-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-642"><a href="#cb29-642" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 20:19:38 llm_engine.py:653] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%</span>
<span id="cb29-643"><a href="#cb29-643" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO</span> 06-19 20:19:39 async_llm_engine.py:111] Finished request b99f9a9858e346009969a9f41764c9d2.</span>
<span id="cb29-644"><a href="#cb29-644" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ§ :</span> Effective throughput of 30.91 tok/s</span>
<span id="cb29-645"><a href="#cb29-645" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ‘¤:</span> <span class="pp">[</span><span class="ss">INST</span><span class="pp">]</span> Using the schema context below, generate a SQL query that answers the question.</span>
<span id="cb29-646"><a href="#cb29-646" aria-hidden="true" tabindex="-1"></a>        <span class="ex">CREATE</span> TABLE Has_allergy <span class="er">(</span><span class="ex">Allergy</span> VARCHAR<span class="kw">)</span></span>
<span id="cb29-647"><a href="#cb29-647" aria-hidden="true" tabindex="-1"></a>        <span class="ex">How</span> many students have cat allergies<span class="pp">?</span> <span class="pp">[</span><span class="ss">/INST</span><span class="pp">]</span></span>
<span id="cb29-648"><a href="#cb29-648" aria-hidden="true" tabindex="-1"></a><span class="ex">ğŸ¤–:</span> <span class="pp">[</span><span class="ss">SQL</span><span class="pp">]</span> SELECT COUNT<span class="er">(</span><span class="ex">*</span><span class="kw">)</span> <span class="ex">FROM</span> Has_allergy WHERE Allergy = <span class="st">"Cat"</span> <span class="pp">[</span><span class="ss">/SQL</span><span class="pp">]</span></span>
<span id="cb29-649"><a href="#cb29-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-650"><a href="#cb29-650" aria-hidden="true" tabindex="-1"></a><span class="ex">Stopping</span> app <span class="at">-</span> local entrypoint completed.</span>
<span id="cb29-651"><a href="#cb29-651" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">[W</span> CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. <span class="er">(</span><span class="kw">function</span><span class="fu"> operator()</span><span class="kw">)</span></span>
<span id="cb29-652"><a href="#cb29-652" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">RayWorkerVllm</span> pid=309<span class="kw">)</span> <span class="ex">INFO</span> 06-19 20:19:38 model_runner.py:437] Graph capturing finished in 34 secs.</span>
<span id="cb29-653"><a href="#cb29-653" aria-hidden="true" tabindex="-1"></a><span class="ex">Runner</span> terminated.</span>
<span id="cb29-654"><a href="#cb29-654" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-655"><a href="#cb29-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-656"><a href="#cb29-656" aria-hidden="true" tabindex="-1"></a>Nice! We have a much better result of what we're looking for. </span>
<span id="cb29-657"><a href="#cb29-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-658"><a href="#cb29-658" aria-hidden="true" tabindex="-1"></a>One thing I noticed after the fact was I forgot to setup a Weights and Bias project -- make sure to set <span class="in">`wandb_project: sqlqa-llama3-lora`</span> in the config file. That's okay - we'll make sure to do this in our next chapter when we run a custom fine tuning project.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>