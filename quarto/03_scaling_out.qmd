---
title: "Scaling out"
subtitle: "03_scaling_out"
author: "Ryan Wesslen"
toc: true
format:
  html:
    html-math-method: katex
    code-tools: true
    self-contained: true
execute:
  echo: true
  eval: false
---

Let's now begin to scale out examples.

# basic_grid_search.py

This example showcases a simple grid search in one dimension, where we try different parameters for a model and pick the one with the best results on a holdout set.

## Defining the image

First, let's build a custom image and install `scikit-learn` in it.

```python
import modal

app = modal.App(
    "example-basic-grid-search",
    image=modal.Image.debian_slim().pip_install("scikit-learn~=1.2.2"),
)
```

## The Modal function

Next, define the function. Note that we use the custom image with `scikit-learn` in it. We also take the hyperparameter `k`, which is how many nearest neighbors we use.

```python
@app.function()
def fit_knn(k):
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier

    X, y = load_digits(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

    clf = KNeighborsClassifier(k)
    clf.fit(X_train, y_train)
    score = float(clf.score(X_test, y_test))
    print("k = %3d, score = %.4f" % (k, score))
    return score, k
```

## Parallel search

To do a hyperparameter search, let's map over this function with different values for `k`, and then select for the best score on the holdout set:

```python
@app.local_entrypoint()
def main():
    # Do a basic hyperparameter search
    best_score, best_k = max(fit_knn.map(range(1, 100)))
    print("Best k = %3d, score = %.4f" % (best_k, best_score))
```

Notice the `map()` function, which is a parallel map over a set of inputs.

It takes one iterator argument per argument in the function being mapped over.

Putting all of this together, we can run it:

```bash
$ modal run basic_grid_search.py 
âœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx
Building image im-F71KgZSeUtGyLnboXtpDAA

=> Step 0: FROM base

=> Step 1: RUN python -m pip install 'scikit-learn~=1.2.2' 
Looking in indexes: http://pypi-mirror.modal.local:5555/simple
Collecting scikit-learn~=1.2.2
  Downloading http://pypi-mirror.modal.local:5555/simple/scikit-learn/scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9.6/9.6 MB 204.2 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/site-packages (from scikit-learn~=1.2.2) (1.25.0)
Collecting scipy>=1.3.2 (from scikit-learn~=1.2.2)
  Downloading http://pypi-mirror.modal.local:5555/simple/scipy/scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 38.6/38.6 MB 230.3 MB/s eta 0:00:00
Collecting joblib>=1.1.1 (from scikit-learn~=1.2.2)
  Downloading http://pypi-mirror.modal.local:5555/simple/joblib/joblib-1.4.2-py3-none-any.whl (301 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 301.8/301.8 kB 237.9 MB/s eta 0:00:00
Collecting threadpoolctl>=2.0.0 (from scikit-learn~=1.2.2)
  Downloading http://pypi-mirror.modal.local:5555/simple/threadpoolctl/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)
Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn
Successfully installed joblib-1.4.2 scikit-learn-1.2.2 scipy-1.13.1 threadpoolctl-3.5.0

[notice] A new release of pip is available: 23.1.2 -> 24.0
[notice] To update, run: pip install --upgrade pip
Creating image snapshot...
Finished snapshot; took 2.85s

Built image im-F71KgZSeUtGyLnboXtpDAA in 15.78s
âœ“ Created objects.
â”œâ”€â”€ ðŸ”¨ Created mount /modal-examples/03_scaling_out/basic_grid_search.py
â””â”€â”€ ðŸ”¨ Created function fit_knn.
k =   1, score = 0.9822
k =  16, score = 0.9778
k =  17, score = 0.9800

...

k =  93, score = 0.9156
k =  28, score = 0.9711
k =  22, score = 0.9800
Best k =   6, score = 0.9956
Stopping app - local entrypoint completed.
âœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx
```

# fetch_stock_prices.py

TBD

# Practice problem

Let's test out what we've learned by creating a new script.

For this, we'll use another `scikit-learn` tutorial ([Gradient Boosting Regularization](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html)) but loop through a parameter (the sample size, `n`) and for each saving a `matplotlib` image from `fetch_stock_prices.py`.

This tutorial is inspired by a recent [:probabl. video](https://youtu.be/oZm4nGN8YMg) by Vincent Warmerdam that explored this tutorial more in detail.

## Initialize the App

Let's first name the app and create the initial image.

```python
import io
import os

import modal

app = modal.App(
    "example-boosting-regularization",
    image=modal.Image.debian_slim()
    .pip_install("scikit-learn~=1.2.2")
    .pip_install("matplotlib~=3.9.0"),
)
```

We needed to install `matplotlib` since we're calling it in our function.

## Define function 

For our function, we'll use:

```python
@app.function()
def fit_boosting(n):
    import matplotlib.pyplot as plt
    import numpy as np

    from sklearn import datasets, ensemble
    from sklearn.metrics import log_loss
    from sklearn.model_selection import train_test_split

    X, y = datasets.make_hastie_10_2(n_samples=n, random_state=1)

    # map labels from {-1, 1} to {0, 1}
    labels, y = np.unique(y, return_inverse=True)

    # note change from 0.8 to 0.2 test dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    original_params = {
        "n_estimators": 500,
        "max_leaf_nodes": 4,
        "max_depth": None,
        "random_state": 2,
        "min_samples_split": 5,
    }

    plt.figure()

    for label, color, setting in [
        ("No shrinkage", "orange", {"learning_rate": 1.0, "subsample": 1.0}),
        ("learning_rate=0.2", "turquoise", {"learning_rate": 0.2, "subsample": 1.0}),
        ("subsample=0.5", "blue", {"learning_rate": 1.0, "subsample": 0.5}),
        (
            "learning_rate=0.2, subsample=0.5",
            "gray",
            {"learning_rate": 0.2, "subsample": 0.5},
        ),
        (
            "learning_rate=0.2, max_features=2",
            "magenta",
            {"learning_rate": 0.2, "max_features": 2},
        ),
    ]:
        params = dict(original_params)
        params.update(setting)

        clf = ensemble.GradientBoostingClassifier(**params)
        clf.fit(X_train, y_train)

        # compute test set deviance
        test_deviance = np.zeros((params["n_estimators"],), dtype=np.float64)

        for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):
            test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])

        plt.plot(
            (np.arange(test_deviance.shape[0]) + 1)[::5],
            test_deviance[::5],
            "-",
            color=color,
            label=label,
        )

    plt.legend(loc="upper right")
    plt.xlabel("Boosting Iterations")
    plt.ylabel("Test Set Deviance")

        # Dump the chart to .png and return the bytes
    with io.BytesIO() as buf:
        plt.savefig(buf, format="png", dpi=300)
        return buf.getvalue()
```

This is primarily the `scikit-learn` demo but a few modifications like:

- we modified the `test_size` from `0.8` to `0.2`
- we parameterized the sample size `n`, which we'll loop through
- we'll return the chart, similarly from `fetch_stock_prices.py`
- increased the number of boosting iterations from `400` to `500`

Last, we'll define the `local_entrypoint` as:

```python
OUTPUT_DIR = "/tmp/modal"


@app.local_entrypoint()
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    for n in [1000,5000,10000,20000,50000]:
        plot = fit_boosting.remote(n)
        filename = os.path.join(OUTPUT_DIR, f"boosting_{n}.png")
        print(f"saving data to {filename}")
        with open(filename, "wb") as f:
            f.write(plot)
```

This will end with us saving each of the images into a folder `/tmp/modal`.

So let's now run this:

```bash
$ modal run boosting_regularization.py
âœ“ Initialized. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx
âœ“ Created objects.
â”œâ”€â”€ ðŸ”¨ Created mount /modal-examples/03_scaling_out/boosting_regularization.py
â””â”€â”€ ðŸ”¨ Created function fit_boosting.
saving data to /tmp/modal/boosting_1000.png
saving data to /tmp/modal/boosting_5000.png
saving data to /tmp/modal/boosting_10000.png
saving data to /tmp/modal/boosting_20000.png
saving data to /tmp/modal/boosting_50000.png
Stopping app - local entrypoint completed.
âœ“ App completed. View run at https://modal.com/charlotte-llm/main/apps/ap-xxxxxxxxxx
```

We can view a few of the images. For example, this is `n = 5000`:

![](images/03_scaling_out/boosting_5000.png)

This is particularly interesting due to the `subsample = 0.5`, which generally follows `No shrinkage` but then jumps up. It's not clear why but a curious case.

Alternatively, let's look at `n = 10000`:

![](images/03_scaling_out/boosting_10000.png)

Now we see a result consistent with Vincent's video as all curves smooth out, none shrinkage learns quickly and then levels out very quickly. Even after 500 iterations no shrinkage has a lower deviance, which indicates a better out-of-sample fit.

Let's last look at `n = 50000`:

![](images/03_scaling_out/boosting_50000.png)

Very similar curves again, but this time the gains of no shrinkage is even magnified more as up to `500` iterations there's a larger gap between no shrinkage and shrinkage.

What's nice about Modal is we can also view the remote logs such that:

![](images/03_scaling_out/boosting_usage.png)

Not surprising, our last (`n = 50000`) execution took the longest, taking about 4 minutes and 23 seconds. This is helpful for us to keep in mind and use these logs more as we begin to run more computationally intensive examples moving forward.  
